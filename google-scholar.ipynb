{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cfea8a",
   "metadata": {},
   "source": [
    "### Import and define functions to crawl pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b394384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import undetected_chromedriver as uc\n",
    "import re, bs4, random, time\n",
    "import pandas as pd\n",
    "\n",
    "def get_rendered_html(url: str, timeout: int = 10) -> str:\n",
    "    driver = uc.Chrome(headless=True, use_subprocess=False)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, timeout)\n",
    "        html = driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return html\n",
    "\n",
    "def escape_bold(text: str) -> str:\n",
    "    return re.sub(r'</?b>', '', text)\n",
    "\n",
    "def build_url(page):\n",
    "    # return f\"https://scholar.google.com/scholar?start={(page-1)*10}&q=mamba%7C%22state+space%22+recommend%7Crecommender%7Crecommendation&hl=en&as_sdt=0,5\"\n",
    "    return f\"https://scholar.google.com/scholar?start={(page-1)*20}&q=(mamba+OR+%22state+space%22+OR+%22state-space%22+OR+%22state+spaces%22+OR+%22state-spaces%22)+AND+(recommend+OR+recommender+OR+recommendation)&hl=en&as_sdt=0,5&as_ylo=2024&num=20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45382e89",
   "metadata": {},
   "source": [
    "### Fuctions to parse paper info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e71a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_info(paper):\n",
    "    title = paper.find('h3', class_=\"gs_rt\").get_text(strip=True)\n",
    "    authors = paper.find('div', class_=\"gs_a\").get_text(strip=True)\n",
    "    link = paper.find('h3', class_=\"gs_rt\").find('a')['href'] if paper.find('h3', class_=\"gs_rt\").find('a') else None\n",
    "    abstract = paper.find('div', class_=\"gs_rs\").get_text(strip=True) if paper.find('div', class_=\"gs_rs\") else None\n",
    "    citations = paper.find('div', class_=\"gs_fl\").find_all('a')[2].get_text(strip=True) if len(paper.find('div', class_=\"gs_fl\").find_all('a')) > 2 else None\n",
    "    return (\n",
    "        title,\n",
    "        authors,\n",
    "        link,\n",
    "        abstract,\n",
    "        citations\n",
    "    )\n",
    "    \n",
    "def crawl_page(idx):\n",
    "    random_delay = random.uniform(0, 1) * 10\n",
    "    print(f\"Waiting for {random_delay} seconds before crawling page {idx}... \")\n",
    "    time.sleep(random_delay)\n",
    "    return get_rendered_html(build_url(idx), timeout=10)\n",
    "\n",
    "def parse_page(html_raw: str) -> list:\n",
    "    soup = bs4.BeautifulSoup(escape_bold(html_raw), 'html.parser')\n",
    "    papers = soup.find_all('div', id=\"gs_res_ccl_mid\")[0].find_all('div', class_=\"gs_r gs_or gs_scl\")\n",
    "    print(f\"done. found {len(papers)} papers.\")\n",
    "    return [get_paper_info(paper) for paper in papers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484e158",
   "metadata": {},
   "source": [
    "### Crawl pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f9ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 5.826511522821813 seconds before crawling page 1... "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m [crawl_page(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)]\n",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)]\n",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m pagedata \u001b[38;5;241m=\u001b[39m get_rendered_html(build_url(idx), timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     20\u001b[0m soup \u001b[38;5;241m=\u001b[39m bs4\u001b[38;5;241m.\u001b[39mBeautifulSoup(escape_bold(pagedata), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs_res_ccl_mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs_r gs_or gs_scl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone. found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(papers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [get_paper_info(paper) \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m papers]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data = [crawl_page(idx) for idx in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6e5e8",
   "metadata": {},
   "source": [
    "### Optional : manually visit pages and download html, then load into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db412dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n"
     ]
    }
   ],
   "source": [
    "def load_page(idx:int) -> str:\n",
    "    with open(f'crawl-mamba-rec/page{idx}.html', 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "htmls = [load_page(i) for i in range(1, 21)]\n",
    "papers = []\n",
    "for html in htmls:\n",
    "    papers += parse_page(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d99668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(papers, columns=['Title', 'Authors', 'Link', 'Abstract', 'Citations']).to_csv('scholar_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32563b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
