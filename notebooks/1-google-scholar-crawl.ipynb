{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e707d8a",
   "metadata": {},
   "source": [
    "# Search for papers\n",
    "\n",
    "Here I record the steps I took to fetch publications from Google Scholar search results. The process consists of largely two steps:\n",
    "\n",
    "1. Obtain HTML files of search result pages\n",
    "2. Parse the HTML file and form a tabular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c9f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import undetected_chromedriver as uc\n",
    "import re, bs4, random, time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484e158",
   "metadata": {},
   "source": [
    "### 1-A. Crawl pages to get HTMLs\n",
    "\n",
    "> `WARNING` : It is recommended to visit Google Scholar search pages and download htmls manually, as stated in `1-B`. Automated methods will likely fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f9ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rendered_html(url: str, timeout: int = 10) -> str:\n",
    "    '''\n",
    "    visit a page, wait until it is fully loaded, and return the HTML source\n",
    "    '''\n",
    "    driver = uc.Chrome(headless=True, use_subprocess=False)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, timeout)\n",
    "        html = driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return html\n",
    "\n",
    "def escape_bold(text: str) -> str:\n",
    "    '''\n",
    "    remove <b></b> tags from the text\n",
    "    '''\n",
    "    return re.sub(r'</?b>', '', text)\n",
    "\n",
    "def build_url(page):\n",
    "    '''\n",
    "    returns url for a google scholar search page, given a page number\n",
    "    search term is `(mamba OR \"state space\" OR \"state-space\" OR \"state-spaces\") AND (recomend OR recommender OR recommendation)`\n",
    "    '''\n",
    "    return f\"https://scholar.google.com/scholar?start={(page-1)*20}&q=(mamba+OR+%22state+space%22+OR+%22state-space%22+OR+%22state+spaces%22+OR+%22state-spaces%22)+AND+(recommend+OR+recommender+OR+recommendation)&hl=en&as_sdt=0,5&as_ylo=2024&num=20\"\n",
    "\n",
    "def crawl_page(idx):\n",
    "    '''\n",
    "    crawl a page and return its HTML source\n",
    "    '''\n",
    "    random_delay = random.uniform(0, 1) * 10\n",
    "    print(f\"Waiting for {random_delay} seconds before crawling page {idx}... \")\n",
    "    time.sleep(random_delay)\n",
    "    return get_rendered_html(build_url(idx), timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71062221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line will likely fail. Use the manual method instead.\n",
    "htmls = [crawl_page(idx) for idx in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6e5e8",
   "metadata": {},
   "source": [
    "### 1-B. Manually load HTMLs into memory\n",
    "\n",
    "This is a much more reliable way to fetch search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db412dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(idx:int, dirname:str) -> str:\n",
    "    '''\n",
    "    Loads a single HTML file stored in `dirname` with names `page{idx}.html`\n",
    "    '''\n",
    "    with open(f'{dirname}/page{idx}.html', 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "pages_crawl = [load_page(i, 'google-scholar-crawl') for i in range(1, 21)]\n",
    "pages_citations = [load_page(i, 'google-scholar-citations') for i in range(1, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3e7b2",
   "metadata": {},
   "source": [
    "### 2. Parse HTML to tabular data and save as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94440309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "Tabularized data saved to \"processed-data/scholar-data-crawl.csv\".\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "done. found 20 papers.\n",
      "Tabularized data saved to \"processed-data/scholar-data-citations.csv\".\n"
     ]
    }
   ],
   "source": [
    "def get_paper_info(paper):\n",
    "    title = paper.find('h3', class_=\"gs_rt\").get_text(strip=True)\n",
    "    authors = paper.find('div', class_=\"gs_a\").get_text(strip=True)\n",
    "    link = paper.find('h3', class_=\"gs_rt\").find('a')['href'] if paper.find('h3', class_=\"gs_rt\").find('a') else None\n",
    "    abstract = paper.find('div', class_=\"gs_rs\").get_text(strip=True) if paper.find('div', class_=\"gs_rs\") else None\n",
    "    citations = paper.find('div', class_=\"gs_fl\").find_all('a')[2].get_text(strip=True) if len(paper.find('div', class_=\"gs_fl\").find_all('a')) > 2 else None\n",
    "    return (\n",
    "        title,\n",
    "        authors,\n",
    "        link,\n",
    "        abstract,\n",
    "        citations\n",
    "    )\n",
    "    \n",
    "def parse_page(html_raw: str) -> list:\n",
    "    soup = bs4.BeautifulSoup(escape_bold(html_raw), 'html.parser')\n",
    "    papers = soup.find_all('div', id=\"gs_res_ccl_mid\")[0].find_all('div', class_=\"gs_r gs_or gs_scl\")\n",
    "    print(f\"done. found {len(papers)} papers.\")\n",
    "    return [get_paper_info(paper) for paper in papers]\n",
    "\n",
    "##############################################################################\n",
    "# Search A\n",
    "##############################################################################\n",
    "\n",
    "papers_crawl = []\n",
    "for html in pages_crawl:\n",
    "    papers_crawl += parse_page(html)\n",
    "\n",
    "df_crawl = pd.DataFrame(papers_crawl, columns=['Title', 'Authors', 'Link', 'Abstract', 'Citations'])\n",
    "df_crawl.to_csv('processed-data/scholar-data-crawl.csv', index=False)\n",
    "print('Tabularized data saved to \"processed-data/scholar-data-crawl.csv\".')\n",
    "\n",
    "##############################################################################\n",
    "# Search B\n",
    "##############################################################################\n",
    "\n",
    "papers_citations = []\n",
    "for html in pages_citations:\n",
    "    papers_citations += parse_page(html)\n",
    "    \n",
    "df_citations = pd.DataFrame(papers_citations, columns=['Title', 'Authors', 'Link', 'Abstract', 'Citations'])\n",
    "df_citations.to_csv('processed-data/scholar-data-citations.csv', index=False)\n",
    "print('Tabularized data saved to \"processed-data/scholar-data-citations.csv\".')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
