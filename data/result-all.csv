published,Title,Survey 대상인가,요약,특이사항,Code,arxiv_abstract,Authors,Link,Citations
2024년 3월 7일 오전 2:00 (GMT+8),Mamba4rec: Towards efficient sequential recommendation with selective state space models,Concrete-RecSys,"Mamba Block + FFN으로 Mamba Layer 구성, 이것을 쌓아서 백본으로 쓴다. Embedding Layer → Mamba Layer (x N) → Prediction Layer가 끝. 

Ablation Study로 PE, FFN, LN을 더하고 제거해본 결과, PE는 없는게 낫고, FFN/LN은 있는게 좋다고 한다. Layer 숫자는 Hyperparameter인듯.

MovieLens, Amazon Beauty, Amazon Video Games 데이터셋에 실험.","Texas A&M 논문,
현재 최다 Citations (60)

(RelKD@KDD 2024 Best Paper Award) - Source https://github.com/chengkai-liu/Mamba4Rec",https://github.com/chengkai-liu/Mamba4Rec,"Sequential recommendation aims to estimate the dynamic user preferences and
sequential dependencies among historical user behaviors. Although
Transformer-based models have proven to be effective for sequential
recommendation, they suffer from the inference inefficiency problem stemming
from the quadratic computational complexity of attention operators, especially
for long behavior sequences. Inspired by the recent success of state space
models (SSMs), we propose Mamba4Rec, which is the first work to explore the
potential of selective SSMs for efficient sequential recommendation. Built upon
the basic Mamba block which is a selective SSM with an efficient hardware-aware
parallel algorithm, we design a series of sequential modeling techniques to
further promote model performance while maintaining inference efficiency.
Through experiments on public datasets, we demonstrate how Mamba4Rec
effectively tackles the effectiveness-efficiency dilemma, outperforming both
RNN- and attention-based baselines in terms of both effectiveness and
efficiency. The code is available at https://github.com/chengkai-liu/Mamba4Rec.","C Liu,J Lin,J Wang,H Liu,J Caverlee- arXiv preprint arXiv:2403.03900, 2024 - arxiv.org",https://arxiv.org/abs/2403.03900,
2024년 7월 27일 오후 8:07 (GMT+8),Matrrec: Uniting mamba and transformer for sequential recommendation,Concrete-RecSys,,,https://github.com/unintelligentmumu/matrrec,"Sequential recommendation systems aim to provide personalized recommendations
by analyzing dynamic preferences and dependencies within user behavior
sequences. Recently, Transformer models can effectively capture user
preferences. However, their quadratic computational complexity limits
recommendation performance on long interaction sequence data. Inspired by the
State Space Model (SSM)representative model, Mamba, which efficiently captures
user preferences in long interaction sequences with linear complexity, we find
that Mamba's recommendation effectiveness is limited in short interaction
sequences, with failing to recall items of actual interest to users and
exacerbating the data sparsity cold start problem. To address this issue, we
innovatively propose a new model, MaTrRec, which combines the strengths of
Mamba and Transformer. This model fully leverages Mamba's advantages in
handling long-term dependencies and Transformer's global attention advantages
in short-term dependencies, thereby enhances predictive capabilities on both
long and short interaction sequence datasets while balancing model efficiency.
Notably, our model significantly improves the data sparsity cold start problem,
with an improvement of up to 33% on the highly sparse Amazon Musical
Instruments dataset. We conducted extensive experimental evaluations on five
widely used public datasets. The experimental results show that our model
outperforms the current state-of-the-art sequential recommendation models on
all five datasets. The code is available at
https://github.com/Unintelligentmumu/MaTrRec.","S Zhang, R Zhang,Z Yang- arXiv preprint arXiv:2407.19239, 2024 - arxiv.org",https://arxiv.org/abs/2407.19239,
2024년 3월 25일 오전 10:31 (GMT+8),Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation,Concrete-RecSys,,,https://github.com/nancheng58/RecMamba,"Sequential Recommenders have been widely applied in various online services,
aiming to model users' dynamic interests from their sequential interactions.
With users increasingly engaging with online platforms, vast amounts of
lifelong user behavioral sequences have been generated. However, existing
sequential recommender models often struggle to handle such lifelong sequences.
The primary challenges stem from computational complexity and the ability to
capture long-range dependencies within the sequence. Recently, a state space
model featuring a selective mechanism (i.e., Mamba) has emerged. In this work,
we investigate the performance of Mamba for lifelong sequential recommendation
(i.e., length>=2k). More specifically, we leverage the Mamba block to model
lifelong user sequences selectively. We conduct extensive experiments to
evaluate the performance of representative sequential recommendation models in
the setting of lifelong sequences. Experiments on two real-world datasets
demonstrate the superiority of Mamba. We found that RecMamba achieves
performance comparable to the representative model while significantly reducing
training duration by approximately 70% and memory costs by 80%. Codes and data
are available at \url{https://github.com/nancheng58/RecMamba}.","J Yang, Y Li, J Zhao,H Wang,M Ma, J Ma… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2403.16371,
2024년 9월 2일 오후 7:58 (GMT+8),Ssd4rec: a structured state space duality model for efficient sequential recommendation,Concrete-RecSys,,,,"Sequential recommendation methods are crucial in modern recommender systems
for their remarkable capability to understand a user's changing interests based
on past interactions. However, a significant challenge faced by current methods
(e.g., RNN- or Transformer-based models) is to effectively and efficiently
capture users' preferences by modeling long behavior sequences, which impedes
their various applications like short video platforms where user interactions
are numerous. Recently, an emerging architecture named Mamba, built on state
space models (SSM) with efficient hardware-aware designs, has showcased the
tremendous potential for sequence modeling, presenting a compelling avenue for
addressing the challenge effectively. Inspired by this, we propose a novel
generic and efficient sequential recommendation backbone, SSD4Rec, which
explores the seamless adaptation of Mamba for sequential recommendations.
Specifically, SSD4Rec marks the variable- and long-length item sequences with
sequence registers and processes the item representations with bidirectional
Structured State Space Duality (SSD) blocks. This not only allows for
hardware-aware matrix multiplication but also empowers outstanding capabilities
in variable-length and long-range sequence modeling. Extensive evaluations on
four benchmark datasets demonstrate that the proposed model achieves
state-of-the-art performance while maintaining near-linear scalability with
user sequence length. Our code is publicly available at
https://github.com/ZhangYifeng1995/SSD4Rec.","H Qu, Y Zhang,L Ning,W Fan,Q Li- arXiv preprint arXiv:2409.01192, 2024 - arxiv.org",https://arxiv.org/abs/2409.01192,
2024년 6월 4일 오후 5:07 (GMT+8),EchoMamba4Rec: Harmonizing Bidirectional State Space Models with Spectral Filtering for Advanced Sequential Recommendation,Concrete-RecSys,,,,"Predicting user preferences and sequential dependencies based on historical
behavior is the core goal of sequential recommendation. Although
attention-based models have shown effectiveness in this field, they often
struggle with inference inefficiency due to the quadratic computational
complexity inherent in attention mechanisms, especially with long-range
behavior sequences. Drawing inspiration from the recent advancements of state
space models (SSMs) in control theory, which provide a robust framework for
modeling and controlling dynamic systems, we introduce EchoMamba4Rec. Control
theory emphasizes the use of SSMs for managing long-range dependencies and
maintaining inferential efficiency through structured state matrices.
EchoMamba4Rec leverages these control relationships in sequential
recommendation and integrates bi-directional processing with frequency-domain
filtering to capture complex patterns and dependencies in user interaction data
more effectively. Our model benefits from the ability of state space models
(SSMs) to learn and perform parallel computations, significantly enhancing
computational efficiency and scalability. It features a bi-directional Mamba
module that incorporates both forward and reverse Mamba components, leveraging
information from both past and future interactions. Additionally, a filter
layer operates in the frequency domain using learnable Fast Fourier Transform
(FFT) and learnable filters, followed by an inverse FFT to refine item
embeddings and reduce noise. We also integrate Gate Linear Units (GLU) to
dynamically control information flow, enhancing the model's expressiveness and
training stability. Experimental results demonstrate that EchoMamba
significantly outperforms existing models, providing more accurate and
personalized recommendations.","Y Wang,X He,S Zhu- arXiv preprint arXiv:2406.02638, 2024 - arxiv.org",https://arxiv.org/abs/2406.02638,
2024년 9월 24일 오후 11:26 (GMT+8),TiM4Rec: An Efficient Sequential Recommendation Model Based on Time-Aware Structured State Space Duality Model,Concrete-RecSys,,,,"The Sequential Recommendation modeling paradigm is shifting from Transformer
to Mamba architecture, which comprises two generations: Mamba1, based on the
State Space Model (SSM), and Mamba2, based on State Space Duality (SSD).
Although SSD offers superior computational efficiency compared to SSM, it
suffers performance degradation in sequential recommendation tasks, especially
in low-dimensional scenarios that are critical for these tasks. Considering
that time-aware enhancement methods are commonly employed to mitigate
performance loss, our analysis reveals that the performance decline of SSD can
similarly be fundamentally compensated by leveraging mechanisms in time-aware
methods. Thus, we propose integrating time-awareness into the SSD framework to
address these performance issues. However, integrating current time-aware
methods, modeled after TiSASRec, into SSD faces the following challenges: 1)
the complexity of integrating these transformer-based mechanisms with the SSD
architecture, and 2) the computational inefficiency caused by the need for
dimensionality expansion of time-difference modeling. To overcome these
challenges, we introduce a novel Time-aware Structured Masked Matrix that
efficiently incorporates time-aware capabilities into SSD. Building on this, we
propose Time-Aware Mamba for Recommendation (TiM4Rec), which mitigates
performance degradation in low-dimensional SSD contexts while preserving
computational efficiency. This marks the inaugural application of a time-aware
enhancement method specifically tailored for the Mamba architecture within the
domain of sequential recommendation. Extensive experiments conducted on three
real-world datasets demonstrate the superiority of our approach. The code for
our model is accessible at https://github.com/AlwaysFHao/TiM4Rec.","H Fan, M Zhu, Y Hu, H Feng, Z He, H Liu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.16182,
2024년 8월 21일 오후 5:12 (GMT+8),Bidirectional gated mamba for sequential recommendation,Concrete-RecSys,,,,"In various domains, Sequential Recommender Systems (SRS) have become
essential due to their superior capability to discern intricate user
preferences. Typically, SRS utilize transformer-based architectures to forecast
the subsequent item within a sequence. Nevertheless, the quadratic
computational complexity inherent in these models often leads to
inefficiencies, hindering the achievement of real-time recommendations. Mamba,
a recent advancement, has exhibited exceptional performance in time series
prediction, significantly enhancing both efficiency and accuracy. However,
integrating Mamba directly into SRS poses several challenges. Its inherently
unidirectional nature may constrain the model's capacity to capture the full
context of user-item interactions, while its instability in state estimation
can compromise its ability to detect short-term patterns within interaction
sequences.
  To overcome these issues, we introduce a new framework named Selective Gated
Mamba (SIGMA) for Sequential Recommendation. This framework leverages a
Partially Flipped Mamba (PF-Mamba) to construct a bidirectional architecture
specifically tailored to improve contextual modeling. Additionally, an
input-sensitive Dense Selective Gate (DS Gate) is employed to optimize
directional weights and enhance the processing of sequential information in
PF-Mamba. For short sequence modeling, we have also developed a Feature Extract
GRU (FE-GRU) to efficiently capture short-term dependencies. Empirical results
indicate that SIGMA outperforms current models on five real-world datasets. Our
implementation code is available at https://github.com/ziwliu-cityu/SIMGA to
ease reproducibility.","Z Liu,Q Liu,Y Wang, W Wang,P Jia,M Wang… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.11451,
,M3Rec: Selective State Space Models with Mixture-of-Modality Experts for Multi-Modal Sequential Recommendation,Concrete-RecSys,,,,,"X Guo, T Zhang, Y Xue,C Wang… - ICASSP 2025-2025 …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10887582/,Cited by 2
2024년 7월 18일 오전 11:46 (GMT+8),Mlsa4rec: Mamba combined with low-rank decomposed self-attention for sequential recommendation,Concrete-RecSys,,,,"In applications such as e-commerce, online education, and streaming services,
sequential recommendation systems play a critical role. Despite the excellent
performance of self-attention-based sequential recommendation models in
capturing dependencies between items in user interaction history, their
quadratic complexity and lack of structural bias limit their applicability.
Recently, some works have replaced the self-attention module in sequential
recommenders with Mamba, which has linear complexity and structural bias.
However, these works have not noted the complementarity between the two
approaches. To address this issue, this paper proposes a new hybrid
recommendation framework, Mamba combined with Low-Rank decomposed
Self-Attention for Sequential Recommendation (MLSA4Rec), whose complexity is
linear with respect to the length of the user's historical interaction
sequence. Specifically, MLSA4Rec designs an efficient Mamba-LSA interaction
module. This module introduces a low-rank decomposed self-attention (LSA)
module with linear complexity and injects structural bias into it through
Mamba. The LSA module analyzes user preferences from a different perspective
and dynamically guides Mamba to focus on important information in user
historical interactions through a gated information transmission mechanism.
Finally, MLSA4Rec combines user preference information refined by the Mamba and
LSA modules to accurately predict the user's next possible interaction. To our
knowledge, this is the first study to combine Mamba and self-attention in
sequential recommendation systems. Experimental results show that MLSA4Rec
outperforms existing self-attention and Mamba-based sequential recommendation
models in recommendation accuracy on three real-world datasets, demonstrating
the great potential of Mamba and self-attention working together.","J Su, Z Huang - arXiv preprint arXiv:2407.13135, 2024 - arxiv.org",https://arxiv.org/abs/2407.13135,
2024년 8월 11일 오전 2:09 (GMT+8),Exploring Applications of State Space Models and Advanced Training Techniques in Sequential Recommendations: A Comparative Study on Efficiency and …,Survey-RecSys,,맘바 추천 Survey 논문,,"Recommender systems aim to estimate the dynamically changing user preferences
and sequential dependencies between historical user behaviour and metadata.
Although transformer-based models have proven to be effective in sequential
recommendations, their state growth is proportional to the length of the
sequence that is being processed, which makes them expensive in terms of memory
and inference costs. Our research focused on three promising directions in
sequential recommendations: enhancing speed through the use of State Space
Models (SSM), as they can achieve SOTA results in the sequential
recommendations domain with lower latency, memory, and inference costs, as
proposed by arXiv:2403.03900 improving the quality of recommendations with
Large Language Models (LLMs) via Monolithic Preference Optimization without
Reference Model (ORPO); and implementing adaptive batch- and step-size
algorithms to reduce costs and accelerate training processes.","M Obozov, M Baderko, S Kulibaba,N Kutuzov… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.05606,
2024년 4월 25일 오전 2:10 (GMT+8),"Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges",Survey-RecSys-Related,,Mamba + Time Series Analysis (Survey),,"Sequence modeling is a crucial area across various domains, including Natural
Language Processing (NLP), speech recognition, time series forecasting, music
generation, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short
Term Memory Networks (LSTMs) have historically dominated sequence modeling
tasks like Machine Translation, Named Entity Recognition (NER), etc. However,
the advancement of transformers has led to a shift in this paradigm, given
their superior performance. Yet, transformers suffer from $O(N^2)$ attention
complexity and challenges in handling inductive bias. Several variations have
been proposed to address these issues which use spectral networks or
convolutions and have performed well on a range of tasks. However, they still
have difficulty in dealing with long sequences. State Space Models(SSMs) have
emerged as promising alternatives for sequence modeling paradigms in this
context, especially with the advent of S4 and its variants, such as S4nd,
Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear
Recurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the
foundational SSMs based on three paradigms namely, Gating architectures,
Structural architectures, and Recurrent architectures. This survey also
highlights diverse applications of SSMs across domains such as vision, video,
audio, speech, language (especially long sequence modeling), medical (including
genomics), chemical (like drug design), recommendation systems, and time series
analysis, including tabular data. Moreover, we consolidate the performance of
SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile,
ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast,
COIN, LVU, and various time series datasets. The project page for Mamba-360
work is available on this webpage.\url{https://github.com/badripatro/mamba360}.","BN Patro,VS Agneeswaran- arXiv preprint arXiv:2404.16112, 2024 - arxiv.org",https://arxiv.org/abs/2404.16112,
2024년 4월 15일 오후 3:24 (GMT+8),State space model for new-generation network alternative to transformers: A survey,Survey-Non-RecSys,,맘바 Survey 논문,,"In the post-deep learning era, the Transformer architecture has demonstrated
its powerful performance across pre-trained big models and various downstream
tasks. However, the enormous computational demands of this architecture have
deterred many researchers. To further reduce the complexity of attention
models, numerous efforts have been made to design more efficient methods. Among
them, the State Space Model (SSM), as a possible replacement for the
self-attention based Transformer model, has drawn more and more attention in
recent years. In this paper, we give the first comprehensive review of these
works and also provide experimental comparisons and analysis to better
demonstrate the features and advantages of SSM. Specifically, we first give a
detailed description of principles to help the readers quickly capture the key
ideas of SSM. After that, we dive into the reviews of existing SSMs and their
various applications, including natural language processing, computer vision,
graph, multi-modal and multi-media, point cloud/event stream, time series data,
and other domains. In addition, we give statistical comparisons and analysis of
these models and hope it helps the readers to understand the effectiveness of
different structures on various tasks. Then, we propose possible research
points in this direction to better promote the development of the theoretical
model and application of SSM. More related works will be continuously updated
on the following GitHub:
https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.","X Wang, S Wang,Y Ding,Y Li, W Wu, Y Rong… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2404.09516,
2024년 8월 13일 오후 11:21 (GMT+8),Dyg-mamba: Continuous state space modeling on dynamic graphs,Survey-RecSys-Related,,그래프 시퀀스모델링  맘바 Survey 논문,,"Dynamic graph learning aims to uncover evolutionary laws in real-world
systems, enabling accurate social recommendation (link prediction) or early
detection of cancer cells (classification). Inspired by the success of state
space models, e.g., Mamba, for efficiently capturing long-term dependencies in
language modeling, we propose DyG-Mamba, a new continuous state space model
(SSM) for dynamic graph learning. Specifically, we first found that using
inputs as control signals for SSM is not suitable for continuous-time dynamic
network data with irregular sampling intervals, resulting in models being
insensitive to time information and lacking generalization properties. Drawing
inspiration from the Ebbinghaus forgetting curve, which suggests that memory of
past events is strongly correlated with time intervals rather than specific
details of the events themselves, we directly utilize irregular time spans as
control signals for SSM to achieve significant robustness and generalization.
Through exhaustive experiments on 12 datasets for dynamic link prediction and
dynamic node classification tasks, we found that DyG-Mamba achieves
state-of-the-art performance on most of the datasets, while also demonstrating
significantly improved computation and memory efficiency.","D Li,S Tan,Y Zhang,M Jin,S Pan, M Okumura… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.06966,
2024년 12월 24일 오후 6:17 (GMT+8),Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning,Survey-RecSys-Related,,그래프 맘바 Survey 논문,,"Graph Mamba, a powerful graph embedding technique, has emerged as a
cornerstone in various domains, including bioinformatics, social networks, and
recommendation systems. This survey represents the first comprehensive study
devoted to Graph Mamba, to address the critical gaps in understanding its
applications, challenges, and future potential. We start by offering a detailed
explanation of the original Graph Mamba architecture, highlighting its key
components and underlying mechanisms. Subsequently, we explore the most recent
modifications and enhancements proposed to improve its performance and
applicability. To demonstrate the versatility of Graph Mamba, we examine its
applications across diverse domains. A comparative analysis of Graph Mamba and
its variants is conducted to shed light on their unique characteristics and
potential use cases. Furthermore, we identify potential areas where Graph Mamba
can be applied in the future, highlighting its potential to revolutionize data
analysis in these fields. Finally, we address the current limitations and open
research questions associated with Graph Mamba. By acknowledging these
challenges, we aim to stimulate further research and development in this
promising area. This survey serves as a valuable resource for both newcomers
and experienced researchers seeking to understand and leverage the power of
Graph Mamba.","SB Atitallah,CB Rabah,M Driss,W Boulila… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2412.18322,
,Graph mamba: Towards learning on graphs with state space models,Concrete-RecSys-Related,,그래프 맘바 Concrete Method,,,"A Behrouz,F Hashemi- Proceedings of the 30th ACM SIGKDD …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3637528.3672044?casa_token=SiUkd6j_wmYAAAAA:yLPulzKjyehKcmr7BMgVnHKq43p87A0shKYi_EHSFo08hbNEL3tfFLIwIZ53iboXMGXqo65EtYhR4A,
,SIGMA: Selective Gated Mamba for Sequential Recommendation,Concrete-RecSys,,,,,"Z Liu,Q Liu, Y Wang, W Wang,P Jia, M Wang… - Proceedings of the …, 2025 - ojs.aaai.org",https://ojs.aaai.org/index.php/AAAI/article/view/33336,
2024년 2월 2일 오전 1:21 (GMT+8),Graph-mamba: Towards long-range graph sequence modeling with selective state spaces,Concrete-RecSys-Related,,그래프 맘바 Concrete Method,,"Attention mechanisms have been widely used to capture long-range dependencies
among nodes in Graph Transformers. Bottlenecked by the quadratic computational
cost, attention mechanisms fail to scale in large graphs. Recent improvements
in computational efficiency are mainly achieved by attention sparsification
with random or heuristic-based graph subsampling, which falls short in
data-dependent context reasoning. State space models (SSMs), such as Mamba,
have gained prominence for their effectiveness and efficiency in modeling
long-range dependencies in sequential data. However, adapting SSMs to
non-sequential graph data presents a notable challenge. In this work, we
introduce Graph-Mamba, the first attempt to enhance long-range context modeling
in graph networks by integrating a Mamba block with the input-dependent node
selection mechanism. Specifically, we formulate graph-centric node
prioritization and permutation strategies to enhance context-aware reasoning,
leading to a substantial improvement in predictive performance. Extensive
experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms
state-of-the-art methods in long-range graph prediction tasks, with a fraction
of the computational cost in both FLOPs and GPU memory consumption. The code
and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.","C Wang, O Tsepa,J Ma,B Wang- arXiv preprint arXiv:2402.00789, 2024 - arxiv.org",https://arxiv.org/abs/2402.00789,
,A Local context enhanced Consistency-aware Mamba-based Sequential Recommendation model,Concrete-RecSys,,,,,"Z Zhang,B Yang, Y Lu - Information Processing & Management, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0306457325000184,Related articles
2024년 9월 11일 오후 10:26 (GMT+8),Mamba for Scalable and Efficient Personalized Recommendations,Concrete-RecSys,,,,"In this effort, we propose using the Mamba for handling tabular data in
personalized recommendation systems. We present the \textit{FT-Mamba} (Feature
Tokenizer\,$+$\,Mamba), a novel hybrid model that replaces Transformer layers
with Mamba layers within the FT-Transformer architecture, for handling tabular
data in personalized recommendation systems. The \textit{Mamba model} offers an
efficient alternative to Transformers, reducing computational complexity from
quadratic to linear by enhancing the capabilities of State Space Models (SSMs).
FT-Mamba is designed to improve the scalability and efficiency of
recommendation systems while maintaining performance. We evaluate FT-Mamba in
comparison to a traditional Transformer-based model within a Two-Tower
architecture on three datasets: Spotify music recommendation, H\&M fashion
recommendation, and vaccine messaging recommendation. Each model is trained on
160,000 user-action pairs, and performance is measured using precision (P),
recall (R), Mean Reciprocal Rank (MRR), and Hit Ratio (HR) at several
truncation values. Our results demonstrate that FT-Mamba outperforms the
Transformer-based model in terms of computational efficiency while maintaining
or exceeding performance across key recommendation metrics. By leveraging Mamba
layers, FT-Mamba provides a scalable and effective solution for large-scale
personalized recommendation systems, showcasing the potential of the Mamba
architecture to enhance both efficiency and accuracy.","A Starnes,C Webster- arXiv preprint arXiv:2409.17165, 2024 - arxiv.org",https://arxiv.org/abs/2409.17165,
2024년 3월 19일 오후 12:02 (GMT+8),Stg-mamba: Spatial-temporal graph learning via selective state space model,Concrete-RecSys-Related,,그래프 맘바 Concrete Method,,"Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,
and non-stationary, leading to the continuous challenge of spatial-temporal
graph learning. In the past few years, various GNN-based methods have been
proposed to solely focus on mimicking the relationships among node individuals
of the STG network, ignoring the significance of modeling the intrinsic
features that exist in STG system over time. In contrast, modern Selective
State Space Models (SSSMs) present a new approach which treat STG Network as a
system, and meticulously explore the STG system's dynamic state evolution
across temporal dimension. In this work, we introduce Spatial-Temporal Graph
Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective
state space models for STG learning by treating STG Network as a system, and
employing the Spatial-Temporal Selective State Space Module (ST-S3M) to
precisely focus on the selected STG latent features. Furthermore, to strengthen
GNN's ability of modeling STG data under the setting of selective state space
models, we propose Kalman Filtering Graph Neural Networks (KFGN) for
dynamically integrate and upgrade the STG embeddings from different temporal
granularities through a learnable Kalman Filtering statistical theory-based
approach. Extensive empirical studies are conducted on three benchmark STG
forecasting datasets, demonstrating the performance superiority and
computational efficiency of STG-Mamba. It not only surpasses existing
state-of-the-art methods in terms of STG forecasting performance, but also
effectively alleviate the computational bottleneck of large-scale graph
networks in reducing the computational cost of FLOPs and test inference time.
The implementation code is available at:
\url{https://github.com/LincanLi98/STG-Mamba}.","L Li,H Wang, W Zhang,A Coster- arXiv preprint arXiv:2403.12418, 2024 - arxiv.org",https://arxiv.org/abs/2403.12418,
2024년 8월 2일 오후 5:18 (GMT+8),A survey of mamba,Survey-Non-RecSys,,맘바 Survey 논문,,"As one of the most representative DL techniques, Transformer architecture has
empowered numerous advanced models, especially the large language models (LLMs)
that comprise billions of parameters, becoming a cornerstone in deep learning.
Despite the impressive achievements, Transformers still face inherent
limitations, particularly the time-consuming inference resulting from the
quadratic computation complexity of attention calculation. Recently, a novel
architecture named Mamba, drawing inspiration from classical state space models
(SSMs), has emerged as a promising alternative for building foundation models,
delivering comparable modeling abilities to Transformers while preserving
near-linear scalability concerning sequence length. This has sparked an
increasing number of studies actively exploring Mamba's potential to achieve
impressive performance across diverse domains. Given such rapid evolution,
there is a critical need for a systematic review that consolidates existing
Mamba-empowered models, offering a comprehensive understanding of this emerging
model architecture. In this survey, we therefore conduct an in-depth
investigation of recent Mamba-associated studies, covering three main aspects:
the advancements of Mamba-based models, the techniques of adapting Mamba to
diverse data, and the applications where Mamba can excel. Specifically, we
first review the foundational knowledge of various representative deep learning
models and the details of Mamba-1&2 as preliminaries. Then, to showcase the
significance of Mamba for AI, we comprehensively review the related studies
focusing on Mamba models' architecture design, data adaptability, and
applications. Finally, we present a discussion of current limitations and
explore various promising research directions to provide deeper insights for
future investigations.","H Qu,L Ning, R An,W Fan,T Derr,H Liu,X Xu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.01129,
2024년 3월 19일 오후 12:02 (GMT+8),Stg-mamba: Spatial-temporal graph learning via selective state space model,Concrete-RecSys-Related,,그래프 맘바 Concrete Method,,"Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,
and non-stationary, leading to the continuous challenge of spatial-temporal
graph learning. In the past few years, various GNN-based methods have been
proposed to solely focus on mimicking the relationships among node individuals
of the STG network, ignoring the significance of modeling the intrinsic
features that exist in STG system over time. In contrast, modern Selective
State Space Models (SSSMs) present a new approach which treat STG Network as a
system, and meticulously explore the STG system's dynamic state evolution
across temporal dimension. In this work, we introduce Spatial-Temporal Graph
Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective
state space models for STG learning by treating STG Network as a system, and
employing the Spatial-Temporal Selective State Space Module (ST-S3M) to
precisely focus on the selected STG latent features. Furthermore, to strengthen
GNN's ability of modeling STG data under the setting of selective state space
models, we propose Kalman Filtering Graph Neural Networks (KFGN) for
dynamically integrate and upgrade the STG embeddings from different temporal
granularities through a learnable Kalman Filtering statistical theory-based
approach. Extensive empirical studies are conducted on three benchmark STG
forecasting datasets, demonstrating the performance superiority and
computational efficiency of STG-Mamba. It not only surpasses existing
state-of-the-art methods in terms of STG forecasting performance, but also
effectively alleviate the computational bottleneck of large-scale graph
networks in reducing the computational cost of FLOPs and test inference time.
The implementation code is available at:
\url{https://github.com/LincanLi98/STG-Mamba}.","L Li,H Wang, W Zhang,A Coster- arXiv preprint arXiv:2403.12418, 2024 - arxiv.org",https://arxiv.org/abs/2403.12418,
,DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with Selective State Space Models,Concrete-RecSys-Related,,그래프 맘바 Concrete Method,,,"H Yuan,Q Sun, Z Wang,X Fu,C Ji, Y Wang… - Proceedings of the …, 2025 - ojs.aaai.org",https://ojs.aaai.org/index.php/AAAI/article/view/34382,
2025년 3월 14일 오후 5:20 (GMT+8),Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models,Survey-Non-RecSys,,맘바 Survey 논문,,"State Space Models (SSMs) have emerged as a promising alternative to the
popular transformer-based models and have been increasingly gaining attention.
Compared to transformers, SSMs excel at tasks with sequential data or longer
contexts, demonstrating comparable performances with significant efficiency
gains. In this survey, we provide a coherent and systematic overview for SSMs,
including their theoretical motivations, mathematical formulations, comparison
with existing model classes, and various applications. We divide the SSM series
into three main sections, providing a detailed introduction to the original
SSM, the structured SSM represented by S4, and the selective SSM typified by
Mamba. We put an emphasis on technicality, and highlight the various key
techniques introduced to address the effectiveness and efficiency of SSMs. We
hope this manuscript serves as an introduction for researchers to explore the
theoretical foundations of SSMs.","X Lv,Y Sun,K Zhang,S Qu,X Zhu,Y Fan, Y Wu… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2503.11224,
2024년 6월 9일 오후 11:03 (GMT+8),What Can We Learn from State Space Models for Machine Learning on Graphs?,Survey-RecSys-Related,,그래프 맘바 Survey 논문,,"Machine learning on graphs has recently found extensive applications across
domains. However, the commonly used Message Passing Neural Networks (MPNNs)
suffer from limited expressive power and struggle to capture long-range
dependencies. Graph transformers offer a strong alternative due to their global
attention mechanism, but they come with great computational overheads,
especially for large graphs. In recent years, State Space Models (SSMs) have
emerged as a compelling approach to replace full attention in transformers to
model sequential data. It blends the strengths of RNNs and CNNs, offering a)
efficient computation, b) the ability to capture long-range dependencies, and
c) good generalization across sequences of various lengths. However, extending
SSMs to graph-structured data presents unique challenges due to the lack of
canonical node ordering in graphs. In this work, we propose Graph State Space
Convolution (GSSC) as a principled extension of SSMs to graph-structured data.
By leveraging global permutation-equivariant set aggregation and factorizable
graph kernels that rely on relative node distances as the convolution kernels,
GSSC preserves all three advantages of SSMs. We demonstrate the provably
stronger expressiveness of GSSC than MPNNs in counting graph substructures and
show its effectiveness across 11 real-world, widely used benchmark datasets.
GSSC achieves the best results on 6 out of 11 datasets with all significant
improvements compared to the state-of-the-art baselines and second-best results
on the other 5 datasets. Our findings highlight the potential of GSSC as a
powerful and scalable model for graph machine learning. Our code is available
at https://github.com/Graph-COM/GSSC.","Y Huang,S Miao,P Li- arXiv preprint arXiv:2406.05815, 2024 - arxiv.org",https://arxiv.org/abs/2406.05815,
2024년 4월 27일 오후 6:22 (GMT+8),Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion,Concrete-Non-RecSys,,맘바 기반 감성분석 Concrete Method,,"Multi-modal Emotion Recognition in Conversation (MERC) has received
considerable attention in various fields, e.g., human-computer interaction and
recommendation systems. Most existing works perform feature disentanglement and
fusion to extract emotional contextual information from multi-modal features
and emotion classification. After revisiting the characteristic of MERC, we
argue that long-range contextual semantic information should be extracted in
the feature disentanglement stage and the inter-modal semantic information
consistency should be maximized in the feature fusion stage. Inspired by recent
State Space Models (SSMs), Mamba can efficiently model long-distance
dependencies. Therefore, in this work, we fully consider the above insights to
further improve the performance of MERC. Specifically, on the one hand, in the
feature disentanglement stage, we propose a Broad Mamba, which does not rely on
a self-attention mechanism for sequence modeling, but uses state space models
to compress emotional representation, and utilizes broad learning systems to
explore the potential data distribution in broad space. Different from previous
SSMs, we design a bidirectional SSM convolution to extract global context
information. On the other hand, we design a multi-modal fusion strategy based
on probability guidance to maximize the consistency of information between
modalities. Experimental results show that the proposed method can overcome the
computational and memory limitations of Transformer when modeling long-distance
contexts, and has great potential to become a next-generation general
architecture in MERC.","Y Shou, T Meng, F Zhang,N Yin,K Li- arXiv preprint arXiv:2404.17858, 2024 - arxiv.org",https://arxiv.org/abs/2404.17858,
,Fine-grained global modeling learning for personalized federated sequential recommender,Concrete-RecSys,,연합 맘바 추천 Concrete Method,,,"Y Di,H Shi,R Ma, Y Liu, W Wang - ICASSP 2025-2025 IEEE …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10889033/,Cited by 1
,Selective State Space Model for Monaural Speech Enhancement,Concrete-Non-RecSys,,맘바 기반 Speech Enhancement,,,"M Chen,Q Zhang, M Wang,X Zhang… - IEEE Transactions …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10839254/,
2024년 5월 23일 오전 2:41 (GMT+8),Hetegraph-mamba: Heterogeneous graph learning via selective state space model,Concrete-RecSys-Related,,그래프 맘바 Concrete Method,,"We propose a heterogeneous graph mamba network (HGMN) as the first
exploration in leveraging the selective state space models (SSSMs) for
heterogeneous graph learning. Compared with the literature, our HGMN overcomes
two major challenges: (i) capturing long-range dependencies among heterogeneous
nodes and (ii) adapting SSSMs to heterogeneous graph data. Our key contribution
is a general graph architecture that can solve heterogeneous nodes in
real-world scenarios, followed an efficient flow. Methodologically, we
introduce a two-level efficient tokenization approach that first captures
long-range dependencies within identical node types, and subsequently across
all node types. Empirically, we conduct comparisons between our framework and
19 state-of-the-art methods on the heterogeneous benchmarks. The extensive
comparisons demonstrate that our framework outperforms other methods in both
the accuracy and efficiency dimensions.","Z Pan,Y Jeong, X Liu,H Liu- arXiv preprint arXiv:2405.13915, 2024 - arxiv.org",https://arxiv.org/abs/2405.13915,
,State space models based efficient long documents classification,Concrete-Non-RecSys,,맘바 기반 Classification,,,"B Song, Y Xu, P Liang, Y Wu - Journal of Intelligent Learning Systems and …, 2024 - scirp.org",https://www.scirp.org/journal/paperinformation?paperid=133869,
2024년 9월 12일 오전 10:50 (GMT+8),CollaMamba: Efficient Collaborative Perception with Cross-Agent Spatial-Temporal State Space Model,Concrete-Non-RecSys,,맘바 + 비전,,"By sharing complementary perceptual information, multi-agent collaborative
perception fosters a deeper understanding of the environment. Recent studies on
collaborative perception mostly utilize CNNs or Transformers to learn feature
representation and fusion in the spatial dimension, which struggle to handle
long-range spatial-temporal features under limited computing and communication
resources. Holistically modeling the dependencies over extensive spatial areas
and extended temporal frames is crucial to enhancing feature quality. To this
end, we propose a resource efficient cross-agent spatial-temporal collaborative
state space model (SSM), named CollaMamba. Initially, we construct a
foundational backbone network based on spatial SSM. This backbone adeptly
captures positional causal dependencies from both single-agent and cross-agent
views, yielding compact and comprehensive intermediate features while
maintaining linear complexity. Furthermore, we devise a history-aware feature
boosting module based on temporal SSM, extracting contextual cues from extended
historical frames to refine vague features while preserving low overhead.
Extensive experiments across several datasets demonstrate that CollaMamba
outperforms state-of-the-art methods, achieving higher model accuracy while
reducing computational and communication overhead by up to 71.9% and 1/64,
respectively. This work pioneers the exploration of the Mamba's potential in
collaborative perception. The source code will be made available.","Y Li,Q Yuan,G Luo, X Fu,X Zhu, Y Yang… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.07714,
2024년 6월 6일 오전 12:29 (GMT+8),Computation-efficient era: A comprehensive survey of state space models in medical image analysis,Survey-Non-RecSys,,맘바 + 비전 Survey 논문,,"Sequence modeling plays a vital role across various domains, with recurrent
neural networks being historically the predominant method of performing these
tasks. However, the emergence of transformers has altered this paradigm due to
their superior performance. Built upon these advances, transformers have
conjoined CNNs as two leading foundational models for learning visual
representations. However, transformers are hindered by the $\mathcal{O}(N^2)$
complexity of their attention mechanisms, while CNNs lack global receptive
fields and dynamic weight allocation. State Space Models (SSMs), specifically
the \textit{\textbf{Mamba}} model with selection mechanisms and hardware-aware
architecture, have garnered immense interest lately in sequential modeling and
visual representation learning, challenging the dominance of transformers by
providing infinite context lengths and offering substantial efficiency
maintaining linear complexity in the input sequence. Capitalizing on the
advances in computer vision, medical imaging has heralded a new epoch with
Mamba models. Intending to help researchers navigate the surge, this survey
seeks to offer an encyclopedic review of Mamba models in medical imaging.
Specifically, we start with a comprehensive theoretical review forming the
basis of SSMs, including Mamba architecture and its alternatives for sequence
modeling paradigms in this context. Next, we offer a structured classification
of Mamba models in the medical field and introduce a diverse categorization
scheme based on their application, imaging modalities, and targeted organs.
Finally, we summarize key challenges, discuss different future research
directions of the SSMs in the medical domain, and propose several directions to
fulfill the demands of this field. In addition, we have compiled the studies
discussed in this paper along with their open-source implementations on our
GitHub repository.","M Heidari,SG Kolahi, S Karimijafarbigloo… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2406.03430,
,GeoMamba: Towards Efficient Geography-aware Sequential POI Recommendation,Concrete-RecSys,,맘바 + POI 추천,,,"J Chen, H Wang, J Shang - IEEE Access, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10719990/,
2024년 10월 10일 오전 1:54 (GMT+8),Stuffed Mamba: Oversized States Lead to the Inability to Forget,Concrete-Non-RecSys,,맘바 보완 시도,,"Recent advancements in recurrent architectures, such as Mamba and RWKV, have
showcased strong language capabilities. Unlike transformer-based models, these
architectures encode all contextual information into a fixed-size state,
leading to great inference efficiency. However, this approach can cause
information interference, where different token data conflicts, resulting in
performance degradation and incoherent outputs beyond a certain context length.
To prevent this, most RNNs incorporate mechanisms designed to ""forget"" earlier
tokens. In this paper, we reveal that Mamba-based models struggle to
effectively forget earlier tokens even with built-in forgetting mechanisms. We
demonstrate that this issue stems from training on contexts that are too short
for the state size, enabling the model to perform well without needing to learn
how to forget. Then, we show that the minimum training length required for the
model to learn forgetting scales linearly with the state size, and the maximum
context length for accurate retrieval of a 5-digit passkey scales exponentially
with the state size, indicating that the model retains some information beyond
the point where forgetting begins. These findings highlight a critical
limitation in current RNN architectures and provide valuable insights for
improving long-context modeling. Our work suggests that future RNN designs must
account for the interplay between state size, training length, and forgetting
mechanisms to achieve robust performance in long-context tasks.","Y Chen,X Zhang,S Hu,X Han,Z Liu,M Sun- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2410.07145,
,Efficient Self-Supervised Video Hashing with Selective State Spaces,Concrete-Non-RecSys,,맘바 + 비전,,,"J Wang, N Lian,J Li,Y Wang,Y Feng,B Chen… - Proceedings of the …, 2025 - ojs.aaai.org",https://ojs.aaai.org/index.php/AAAI/article/view/32835,
2024년 7월 15일 오전 6:23 (GMT+8),Mambaforgcn: Enhancing long-range dependency with state space model and kolmogorov-arnold networks for aspect-based sentiment analysis,Concrete-Non-RecSys,,맘바 + 감성분석,,"Aspect-based Sentiment Analysis (ABSA) evaluates sentiments toward specific
aspects of entities within the text. However, attention mechanisms and neural
network models struggle with syntactic constraints. The quadratic complexity of
attention mechanisms also limits their adoption for capturing long-range
dependencies between aspect and opinion words in ABSA. This complexity can lead
to the misinterpretation of irrelevant contextual words, restricting their
effectiveness to short-range dependencies. To address the above problem, we
present a novel approach to enhance long-range dependencies between aspect and
opinion words in ABSA (MambaForGCN). This approach incorporates syntax-based
Graph Convolutional Network (SynGCN) and MambaFormer (Mamba-Transformer)
modules to encode input with dependency relations and semantic information. The
Multihead Attention (MHA) and Selective State Space model (Mamba) blocks in the
MambaFormer module serve as channels to enhance the model with short and
long-range dependencies between aspect and opinion words. We also introduce the
Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptive feature
representation system that integrates SynGCN and MambaFormer and captures
non-linear, complex dependencies. Experimental results on three benchmark
datasets demonstrate MambaForGCN's effectiveness, outperforming
state-of-the-art (SOTA) baseline models.","A Lawan, J Pu,H Yunusa,A Umar,M Lawan- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2407.10347,
2024년 6월 14일 오후 4:43 (GMT+8),SHMamba: Structured Hyperbolic State Space Model for Audio-Visual Question Answering,Concrete-Non-RecSys,,맘바 + 멀티모달 QA,,"The Audio-Visual Question Answering (AVQA) task holds significant potential
for applications. Compared to traditional unimodal approaches, the multi-modal
input of AVQA makes feature extraction and fusion processes more challenging.
Euclidean space is difficult to effectively represent multi-dimensional
relationships of data. Especially when extracting and processing data with a
tree structure or hierarchical structure, Euclidean space is not suitable as an
embedding space. Additionally, the self-attention mechanism in Transformers is
effective in capturing the dynamic relationships between elements in a
sequence. However, the self-attention mechanism's limitations in window
modeling and quadratic computational complexity reduce its effectiveness in
modeling long sequences. To address these limitations, we propose SHMamba:
Structured Hyperbolic State Space Model to integrate the advantages of
hyperbolic geometry and state space models. Specifically, SHMamba leverages the
intrinsic properties of hyperbolic space to represent hierarchical structures
and complex relationships in audio-visual data. Meanwhile, the state space
model captures dynamic changes over time by globally modeling the entire
sequence. Furthermore, we introduce an adaptive curvature hyperbolic alignment
module and a cross fusion block to enhance the understanding of hierarchical
structures and the dynamic exchange of cross-modal information, respectively.
Extensive experiments demonstrate that SHMamba outperforms previous methods
with fewer parameters and computational costs. Our learnable parameters are
reduced by 78.12\%, while the average performance improves by 2.53\%.
Experiments show that our method demonstrates superiority among all current
major methods and is more suitable for practical application scenarios.","Z Yang,W Li, G Cheng - arXiv preprint arXiv:2406.09833, 2024 - arxiv.org",https://arxiv.org/abs/2406.09833,
2024년 9월 18일 오후 10:49 (GMT+8),Topological deep learning with state-space models: A mamba approach for simplicial complexes,Concrete-RecSys-Related,,Mamba + Topological Deep Learning (GNN enhanced),,"Graph Neural Networks based on the message-passing (MP) mechanism are a
dominant approach for handling graph-structured data. However, they are
inherently limited to modeling only pairwise interactions, making it difficult
to explicitly capture the complexity of systems with $n$-body relations. To
address this, topological deep learning has emerged as a promising field for
studying and modeling higher-order interactions using various topological
domains, such as simplicial and cellular complexes. While these new domains
provide powerful representations, they introduce new challenges, such as
effectively modeling the interactions among higher-order structures through
higher-order MP. Meanwhile, structured state-space sequence models have proven
to be effective for sequence modeling and have recently been adapted for graph
data by encoding the neighborhood of a node as a sequence, thereby avoiding the
MP mechanism. In this work, we propose a novel architecture designed to operate
with simplicial complexes, utilizing the Mamba state-space model as its
backbone. Our approach generates sequences for the nodes based on the
neighboring cells, enabling direct communication between all higher-order
structures, regardless of their rank. We extensively validate our model,
demonstrating that it achieves competitive performance compared to
state-of-the-art models developed for simplicial complexes.","M Montagna,S Scardapane,L Telyatnikov- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.12033,
2025년 1월 26일 오후 5:09 (GMT+8),Mamba-Based Graph Convolutional Networks: Tackling Over-smoothing with Selective State Space,Concrete-RecSys-Related,,Mamba + GNN,,"Graph Neural Networks (GNNs) have shown great success in various graph-based
learning tasks. However, it often faces the issue of over-smoothing as the
model depth increases, which causes all node representations to converge to a
single value and become indistinguishable. This issue stems from the inherent
limitations of GNNs, which struggle to distinguish the importance of
information from different neighborhoods. In this paper, we introduce MbaGCN, a
novel graph convolutional architecture that draws inspiration from the Mamba
paradigm-originally designed for sequence modeling. MbaGCN presents a new
backbone for GNNs, consisting of three key components: the Message Aggregation
Layer, the Selective State Space Transition Layer, and the Node State
Prediction Layer. These components work in tandem to adaptively aggregate
neighborhood information, providing greater flexibility and scalability for
deep GNN models. While MbaGCN may not consistently outperform all existing
methods on each dataset, it provides a foundational framework that demonstrates
the effective integration of the Mamba paradigm into graph representation
learning. Through extensive experiments on benchmark datasets, we demonstrate
that MbaGCN paves the way for future advancements in graph neural network
research.","X He,Y Wang,W Fan,X Shen, X Juan, R Miao… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2501.15461,
2025년 4월 2일 오후 4:42 (GMT+8),Test-Time Alignment for Tracking User Interest Shifts in Sequential Recommendation,Concrete-RecSys,,Mamba + RecSys,,"Sequential recommendation is essential in modern recommender systems, aiming
to predict the next item a user may interact with based on their historical
behaviors. However, real-world scenarios are often dynamic and subject to
shifts in user interests. Conventional sequential recommendation models are
typically trained on static historical data, limiting their ability to adapt to
such shifts and resulting in significant performance degradation during
testing. Recently, Test-Time Training (TTT) has emerged as a promising
paradigm, enabling pre-trained models to dynamically adapt to test data by
leveraging unlabeled examples during testing. However, applying TTT to
effectively track and address user interest shifts in recommender systems
remains an open and challenging problem. Key challenges include how to capture
temporal information effectively and explicitly identifying shifts in user
interests during the testing phase. To address these issues, we propose
T$^2$ARec, a novel model leveraging state space model for TTT by introducing
two Test-Time Alignment modules tailored for sequential recommendation,
effectively capturing the distribution shifts in user interest patterns over
time. Specifically, T$^2$ARec aligns absolute time intervals with
model-adaptive learning intervals to capture temporal dynamics and introduce an
interest state alignment mechanism to effectively and explicitly identify the
user interest shifts with theoretical guarantees. These two alignment modules
enable efficient and incremental updates to model parameters in a
self-supervised manner during testing, enhancing predictions for online
recommendation. Extensive evaluations on three benchmark datasets demonstrate
that T$^2$ARec achieves state-of-the-art performance and robustly mitigates the
challenges posed by user interest shifts.","C Zhang,X Zhang,T Shi,J Xu,JR Wen- arXiv preprint arXiv:2504.01489, 2025 - arxiv.org",https://arxiv.org/abs/2504.01489,
2024년 9월 25일 오후 7:22 (GMT+8),Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability,Concrete-Non-RecSys,,Mamba + RL + LLM Based Recommendation,,"Optimal decision-making under partial observability requires reasoning about
the uncertainty of the environment's hidden state. However, most reinforcement
learning architectures handle partial observability with sequence models that
have no internal mechanism to incorporate uncertainty in their hidden state
representation, such as recurrent neural networks, deterministic state-space
models and transformers. Inspired by advances in probabilistic world models for
reinforcement learning, we propose a standalone Kalman filter layer that
performs closed-form Gaussian inference in linear state-space models and train
it end-to-end within a model-free architecture to maximize returns. Similar to
efficient linear recurrent layers, the Kalman filter layer processes sequential
data using a parallel scan, which scales logarithmically with the sequence
length. By design, Kalman filter layers are a drop-in replacement for other
recurrent layers in standard model-free architectures, but importantly they
include an explicit mechanism for probabilistic filtering of the latent state
representation. Experiments in a wide variety of tasks with partial
observability show that Kalman filter layers excel in problems where
uncertainty reasoning is key for decision-making, outperforming other stateful
models.","CE Luis,AG Bottero,J Vinogradska… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.16824,
,Data-Driven Strategies for Complex System Forecasts: The Role of Textual Big Data and State-Space Transformers in Decision Support,Concrete-Non-RecSys,,Mamba Enhanced,,,"H Huo, W Guo, R Yang, X Liu, J Xue, Q Peng, Y Deng… - Systems, 2024 - mdpi.com",https://www.mdpi.com/2079-8954/12/5/171,
,Breaking determinism: Fuzzy modeling of sequential recommendation using discrete state space diffusion model,Non-Mamba,,,,,"W Xie,H Wang,L Zhang, R Zhou… - Advances in Neural …, 2024 - proceedings.neurips.cc",https://proceedings.neurips.cc/paper_files/paper/2024/hash/286d67ff96f99c614f75dbcfb72a3e5f-Abstract-Conference.html,
,Samba: Semantic segmentation of remotely sensed images with state space model,Concrete-Non-RecSys,,Mamba + VIsion,,,"Q Zhu,Y Cai,Y Fang, Y Yang,C Chen,L Fan… - Heliyon, 2024 - cell.com",https://www.cell.com/heliyon/fulltext/S2405-8440(24)14526-4,
,SAMamba: Integrating State Space Model for Enhanced Multi-modal Survival Analysis,Concrete-Non-RecSys,,Mamba + Survival Analysis,,,"W Zhang, T Chen, W Xu, X Li - 2024 IEEE International …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10822595/,
2024년 3월 27일 오후 2:07 (GMT+8),RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers,Concrete-RecSys-Related,,Mamba + Dense Passage Retrieval,,"Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate reproducibility
(https://github.com/zhichaoxu-shufe/RankMamba).","Z Xu- arXiv preprint arXiv:2403.18276, 2024 - arxiv.org",https://arxiv.org/abs/2403.18276,
2024년 6월 6일 오후 9:55 (GMT+8),Glint-ru: Gated lightweight intelligent recurrent units for sequential recommender systems,Non-Mamba,,Attention based sequential recommendation (Mamba is only used for comparison),,"Transformer-based models have gained significant traction in sequential
recommender systems (SRSs) for their ability to capture user-item interactions
effectively. However, these models often suffer from high computational costs
and slow inference. Meanwhile, existing efficient SRS approaches struggle to
embed high-quality semantic and positional information into latent
representations. To tackle these challenges, this paper introduces GLINT-RU, a
lightweight and efficient SRS leveraging a single-layer dense selective Gated
Recurrent Units (GRU) module to accelerate inference. By incorporating a dense
selective gate, GLINT-RU adaptively captures temporal dependencies and
fine-grained positional information, generating high-quality latent
representations. Additionally, a parallel mixing block infuses fine-grained
positional features into user-item interactions, enhancing both recommendation
quality and efficiency. Extensive experiments on three datasets demonstrate
that GLINT-RU achieves superior prediction accuracy and inference speed,
outperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These
results establish GLINT-RU as a powerful and efficient solution for SRSs.","S Zhang,M Wang, W Wang,J Gao,X Zhao… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2406.10244,
,Is mamba compatible with trajectory optimization in offline reinforcement learning?,Concrete-Non-RecSys,,Mamba + RL,,,"Y Dai,O Ma, L Zhang, X Liang,S Hu… - Advances in …, 2024 - proceedings.neurips.cc",https://proceedings.neurips.cc/paper_files/paper/2024/hash/5c186016d0844767209dc36e9e61441b-Abstract-Conference.html,
2024년 11월 14일 오전 2:19 (GMT+8),Multimodal Instruction Tuning with Hybrid State Space Models,Concrete-Non-RecSys,,Mamba + Vision,,"Handling lengthy context is crucial for enhancing the recognition and
understanding capabilities of multimodal large language models (MLLMs) in
applications such as processing high-resolution images or high frame rate
videos. The rise in image resolution and frame rate substantially increases
computational demands due to the increased number of input tokens. This
challenge is further exacerbated by the quadratic complexity with respect to
sequence length of the self-attention mechanism. Most prior works either
pre-train models with long contexts, overlooking the efficiency problem, or
attempt to reduce the context length via downsampling (e.g., identify the key
image patches or frames) to decrease the context length, which may result in
information loss. To circumvent this issue while keeping the remarkable
effectiveness of MLLMs, we propose a novel approach using a hybrid
transformer-MAMBA model to efficiently handle long contexts in multimodal
applications. Our multimodal model can effectively process long context input
exceeding 100k tokens, outperforming existing models across various benchmarks.
Remarkably, our model enhances inference efficiency for high-resolution images
and high-frame-rate videos by about 4 times compared to current models, with
efficiency gains increasing as image resolution or video frames rise.
Furthermore, our model is the first to be trained on low-resolution images or
low-frame-rate videos while being capable of inference on high-resolution
images and high-frame-rate videos, offering flexibility for inference in
diverse scenarios.","J Zhou,H Li,S Zhang,N Xie, R Wang, X Nie… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2411.08840,Cited by 1
2024년 8월 22일 오전 10:14 (GMT+8),Simplified mamba with disentangled dependency encoding for long-term time series forecasting,Concrete-Non-RecSys,,Mamba Enhanced,,"In recent years, advancements in deep learning have spurred the development
of numerous models for Long-term Time Series Forecasting (LTSF). However, most
existing approaches struggle to fully capture the complex and structured
dependencies inherent in time series data. In this work, we identify and
formally define three critical dependencies that are fundamental to forecasting
accuracy: order dependency and semantic dependency along the temporal
dimension, as well as cross-variate dependency across the feature dimension.
These dependencies are often treated in isolation, and improper handling can
introduce noise and degrade forecasting performance. To bridge this gap, we
investigate the potential of State Space Models (SSMs) for LTSF and emphasize
their inherent advantages in capturing these essential dependencies.
Additionally, we empirically observe that excessive nonlinearity in
conventional SSMs introduce redundancy when applied to semantically sparse time
series data. Motivated by this insight, we propose SDE (Simplified and
Disentangled Dependency Encoding), a novel framework designed to enhance the
capability of SSMs for LTSF. Specifically, we first eliminate unnecessary
nonlinearities in vanilla SSMs, thereby improving the suitability for time
series forecasting. Building on this foundation, we introduce a disentangled
encoding strategy, which empowers SSMs to efficiently model cross-variate
dependencies while mitigating interference between the temporal and feature
dimensions. Furthermore, we provide rigorous theoretical justifications to
substantiate our design choices. Extensive experiments on nine real-world
benchmark datasets demonstrate that SDE-enhanced SSMs consistently outperform
state-of-the-art time series forecasting models.Our code is available at
https://github.com/YukinoAsuna/SAMBA.","Z Weng,J Han,W Jiang,H Liu- arXiv preprint arXiv:2408.12068, 2024 - arxiv.org",https://arxiv.org/abs/2408.12068,
2024년 4월 23일 오후 1:43 (GMT+8),SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting,Concrete-RecSys-Related,,Mamba + Time Series Analysis,,"Despite significant progress in time series forecasting, existing forecasters
often overlook the heterogeneity between long-range and short-range time
series, leading to performance degradation in practical applications. In this
work, we highlight the need of distinct objectives tailored to different
ranges. We point out that time series can be decomposed into global patterns
and local variations, which should be addressed separately in long- and
short-range time series. To meet the objectives, we propose a multi-scale
hybrid Mamba-Transformer experts model State Space Transformer (SST). SST
leverages Mamba as an expert to extract global patterns in coarse-grained
long-range time series, and Local Window Transformer (LWT), the other expert to
focus on capturing local variations in fine-grained short-range time series.
With an input-dependent mechanism, State Space Model (SSM)-based Mamba is able
to selectively retain long-term patterns and filter out fluctuations, while LWT
employs a local window to enhance locality-awareness capability, thus
effectively capturing local variations. To adaptively integrate the global
patterns and local variations, a long-short router dynamically adjusts
contributions of the two experts. SST achieves superior performance with
scaling linearly $O(L)$ on time series length $L$. The comprehensive
experiments demonstrate the SST can achieve SOTA results in long-short range
time series forecasting while maintaining low memory footprint and
computational cost. The code of SST is available at
https://github.com/XiongxiaoXu/SST.","X Xu,C Chen,Y Liang,B Huang,G Bai,L Zhao… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2404.14757,
2025년 2월 11일 오전 8:59 (GMT+8),A Survey on Mamba Architecture for Vision Applications,Survey-Non-RecSys,,Mamba + Vision (Survey),,"Transformers have become foundational for visual tasks such as object
detection, semantic segmentation, and video understanding, but their quadratic
complexity in attention mechanisms presents scalability challenges. To address
these limitations, the Mamba architecture utilizes state-space models (SSMs)
for linear scalability, efficient processing, and improved contextual
awareness. This paper investigates Mamba architecture for visual domain
applications and its recent advancements, including Vision Mamba (ViM) and
VideoMamba, which introduce bidirectional scanning, selective scanning
mechanisms, and spatiotemporal processing to enhance image and video
understanding. Architectural innovations like position embeddings, cross-scan
modules, and hierarchical designs further optimize the Mamba framework for
global and local feature extraction. These advancements position Mamba as a
promising architecture in computer vision research and applications.","F Ibrahim,G Liu,G Wang- arXiv preprint arXiv:2502.07161, 2025 - arxiv.org",https://arxiv.org/abs/2502.07161,
2024년 2월 16일 오전 2:59 (GMT+8),Hierarchical state space models for continuous sequence-to-sequence modeling,Concrete-RecSys-Related,,Mamba + Time Series Analysis,,"Reasoning from sequences of raw sensory data is a ubiquitous problem across
fields ranging from medical devices to robotics. These problems often involve
using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to
predict sequences of desirable physical quantities (e.g. force, inertial
measurements). While classical approaches are powerful for locally-linear
prediction problems, they often fall short when using real-world sensors. These
sensors are typically non-linear, are affected by extraneous variables (e.g.
vibration), and exhibit data-dependent drift. For many problems, the prediction
task is exacerbated by small labeled datasets since obtaining ground-truth
labels requires expensive equipment. In this work, we present Hierarchical
State-Space Models (HiSS), a conceptually simple, new technique for continuous
sequential prediction. HiSS stacks structured state-space models on top of each
other to create a temporal hierarchy. Across six real-world sensor datasets,
from tactile-based state prediction to accelerometer-based inertial
measurement, HiSS outperforms state-of-the-art sequence models such as causal
Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments
further indicate that HiSS demonstrates efficient scaling to smaller datasets
and is compatible with existing data-filtering techniques. Code, datasets and
videos can be found on https://hiss-csp.github.io.","R Bhirangi, C Wang,V Pattabiraman,C Majidi… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2402.10211,
,Graph-Mamba 기반 POI 추천 시스템,Concrete-RecSys,,Korean only,,,"최민규， 임성수 - 한국통신학회 인공지능 학술대회 논문집, 2024 - dbpia.co.kr",https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11949238,Related articles
,Hypergraph Neural Network with State Space Models for Node Classification,Concrete-RecSys-Related,,Mamba + GNN,,,"A Quadir,M Tanveer- Available at SSRN 5279537 - papers.ssrn.com",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5279537,
2024년 9월 18일 오전 3:36 (GMT+8),Mamba fusion: Learning actions through questioning,Concrete-Non-RecSys,,"Mamba + Vision + Language ",,"Video Language Models (VLMs) are crucial for generalizing across diverse
tasks and using language cues to enhance learning. While transformer-based
architectures have been the de facto in vision-language training, they face
challenges like quadratic computational complexity, high GPU memory usage, and
difficulty with long-term dependencies. To address these limitations, we
introduce MambaVL, a novel model that leverages recent advancements in
selective state space modality fusion to efficiently capture long-range
dependencies and learn joint representations for vision and language data.
MambaVL utilizes a shared state transition matrix across both modalities,
allowing the model to capture information about actions from multiple
perspectives within the scene. Furthermore, we propose a question-answering
task that helps guide the model toward relevant cues. These questions provide
critical information about actions, objects, and environmental context, leading
to enhanced performance. As a result, MambaVL achieves state-of-the-art
performance in action recognition on the Epic-Kitchens-100 dataset and
outperforms baseline methods in action anticipation.","Z Dong,A Beedu, J Sheinkopf,I Essa- arXiv preprint arXiv:2409.11513, 2024 - arxiv.org",https://arxiv.org/abs/2409.11513,
2024년 11월 23일 오후 1:13 (GMT+8),MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction,Concrete-RecSys,,Mamba + Micro-video Recommendation,,"The surge in micro-videos is transforming the concept of popularity. As
researchers delve into vast multi-modal datasets, there is a growing interest
in understanding the origins of this popularity and the forces driving its
rapid expansion. Recent studies suggest that the virality of short videos is
not only tied to their inherent multi-modal content but is also heavily
influenced by the strength of platform recommendations driven by audience
feedback. In this paper, we introduce a framework for capturing long-term
dependencies in user feedback and dynamic event interactions, based on the
Mamba Hawkes process. Our experiments on the large-scale open-source
multi-modal dataset show that our model significantly outperforms
state-of-the-art approaches across various metrics by 23.2%. We believe our
model's capability to map the relationships within user feedback behavior
sequences will not only contribute to the evolution of next-generation
recommendation algorithms and platform applications but also enhance our
understanding of micro video dissemination and its broader societal impact.","J Lu,M Xiao,W Wang,Y Du,Y Cui, J Zhao… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2411.15455,
,Enhancing Long-range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-based Sentiment Analysis,Concrete-Non-RecSys,,Mamba + Graph + Sentiment Analysis,,,"A Lawan, J Pu,H Yunusa,A Umar… - Proceedings of the 31st …, 2025 - aclanthology.org",https://aclanthology.org/2025.coling-main.148/,
,A hybrid model based on transformer and Mamba for enhanced sequence modeling,Concrete-RecSys-Related,,Mamba + Sequence modeling,,,"X Zhu, Q Ruan, S Qian, M Zhang - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-87574-8,
,On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems,Non-Mamba,,,,,"S Wang, X Chen,L Yao- Proceedings of the 33rd ACM International …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3627673.3679674?casa_token=9DOHC-Xo3lkAAAAA:Y-Pw66nqE4BA5qYwcYEsQDxF8rtj91fsrBi5qKVIzr8Jm5irti2A3IGWCoWRBb-Ndxs0hKSEvMbE8w,
2024년 11월 6일 오전 12:59 (GMT+8),ShadowMamba: State-Space Model with Boundary-Region Selective Scan for Shadow Removal,Concrete-Non-RecSys,,Mamba + Vision,,"Image shadow removal is a common low-level vision problem. Shadows cause
sudden brightness changes in some areas, which can affect the accuracy of
downstream tasks. Currently, Transformer-based shadow removal methods improve
computational efficiency by using a window mechanism. However, this approach
reduces the effective receptive field and weakens the ability to model
long-range dependencies in shadow images. Recently, Mamba has achieved
significant success in computer vision by modeling long-sequence information
globally with linear complexity. However, when applied to shadow removal, its
original scanning mechanism overlooks the semantic continuity along shadow
boundaries, and the coherence within each region. To solve this issue, we
propose a new boundary-region selective scanning mechanism that scans shadow,
boundary, and non-shadow regions separately, making pixels of the same type
closer in the sequence. This increases semantic continuity and helps the model
understand local details better. Incorporating this idea, we design the first
Mamba-based lightweight shadow removal model, called ShadowMamba. It uses a
hierarchical combination U-Net structure, which effectively reduces the number
of parameters and computational complexity. Shallow layers rely on our
boundary-region selective scanning to capture local details, while deeper
layers use global cross-scanning to learn global brightness features. Extensive
experiments show that ShadowMamba outperforms current state-of-the-art models
on ISTD+, ISTD, and SRD datasets, and it also requires fewer parameters and
less computational cost. (Code will be made available upon paper acceptance.)","X Zhu,CO Chow,JH Chuah- arXiv preprint arXiv:2411.03260, 2024 - arxiv.org",https://arxiv.org/abs/2411.03260,Related articles
2024년 2월 23일 오후 8:36 (GMT+8),EfficientState Space Model viaFast Tensor Convolutionand Block Diagonalization,Concrete-Non-RecSys,,Mamba Enhanced,,"Existing models encounter bottlenecks in balancing performance and
computational efficiency when modeling long sequences. Although the state space
model (SSM) has achieved remarkable success in handling long sequence tasks, it
still faces the problem of large number of parameters. In order to further
improve the efficiency of SSM, we propose a new state space layer based on
multiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is
built on the convolutional representation of multi-input and multi-input (MIMO)
SSM. We propose a variety of effective strategies to improve the computational
efficiency. The diagonalization of the system matrix first decouples the
original system. Then a fast tensor convolution is proposed based on the fast
Fourier transform. In addition, the block diagonalization of the SSM further
reduces the model parameters and improves the model flexibility. Extensive
experimental results show that the performance of the proposed model on
multiple databases matches the performance of state-of-the-art models, such as
S4, and is significantly better than Transformers and LSTM. In the model
efficiency benchmark, the parameters of eSSM are only 12.89\% of LSTM and
13.24\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and
1.35 times faster than Mamba. Code is available at:
\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.","T Liang, HX Li - arXiv preprint arXiv:2402.15290, 2024 - arxiv.org",https://arxiv.org/abs/2402.15290,
2024년 4월 5일 오전 1:58 (GMT+8),Locating and editing factual associations in mamba,Concrete-Non-RecSys,,Mamba + Language Modeling,,"We investigate the mechanisms of factual recall in the Mamba state space
model. Our work is inspired by previous findings in autoregressive transformer
language models suggesting that their knowledge recall is localized to
particular modules at specific token locations; we therefore ask whether
factual recall in Mamba can be similarly localized. To investigate this, we
conduct four lines of experiments on Mamba. First, we apply causal tracing or
interchange interventions to localize key components inside Mamba that are
responsible for recalling facts, revealing that specific components within
middle layers show strong causal effects at the last token of the subject,
while the causal effect of intervening on later layers is most pronounced at
the last token of the prompt, matching previous findings on autoregressive
transformers. Second, we show that rank-one model editing methods can
successfully insert facts at specific locations, again resembling findings on
transformer LMs. Third, we examine the linearity of Mamba's representations of
factual relations. Finally we adapt attention-knockout techniques to Mamba in
order to dissect information flow during factual recall. We compare Mamba
directly to a similar-sized autoregressive transformer LM and conclude that
despite significant differences in architectural approach, when it comes to
factual recall, the two architectures share many similarities.","AS Sharma,D Atkinson,D Bau- arXiv preprint arXiv:2404.03646, 2024 - arxiv.org",https://arxiv.org/abs/2404.03646,
,Hamba: Single-view 3d hand reconstruction with graph-guided bi-scanning mamba,Concrete-Non-RecSys,,Mamba + 3D Vision,,,"H Dong, A Chharia,W Gou… - Advances in …, 2024 - proceedings.neurips.cc",https://proceedings.neurips.cc/paper_files/paper/2024/hash/03e9a69e5b686c316a07d73f0cf5e225-Abstract-Conference.html,
,MambaHealth: A Lightweight Foundation Model for Efficient Drug Recommendation,Concrete-RecSys-Related,,Mamba + Recommendation via LLM,,,"Y Wang,X He,S Zhu- Advancements In Medical Foundation Models … - openreview.net",https://openreview.net/forum?id=LWXi4leVdV,
,Hyperbolic Variational Graph Auto-Encoder for Next POI Recommendation,Concrete-RecSys,,Mamba + GNN + Recommendation,,,"Y Liu,L Qi,X Mao, W Liu,F Wang,X Xu… - Proceedings of the …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3696410.3714804?casa_token=alj8T_biMHsAAAAA:DfHGrE15x3TgZ9PjV4xiEJ-QEUOZZ8hD3hRIQDep8ctMZGzh-AlkwhWW_H6ynKFq1ofjJD5QQ6YY-A,
,Image Representation Learning with Masked Image Modeling Pre-training in Vision Mamba State Space Models,Concrete-Non-RecSys,,Mamba + Vision,,,A Duyum - 2024 - essay.utwente.nl,http://essay.utwente.nl/100979/,
,An efficient fire detection algorithm based on Mamba space state linear attention,Concrete-Non-RecSys,,Mamba + Vision,,,"Y Li, Y Wang, X Shao, A Zheng - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-95162-z,
2024년 5월 26일 오후 8:26 (GMT+8),Mamba4KT: An Efficient and Effective Mamba-based Knowledge Tracing Model,Concrete-Non-RecSys,,Mamba + Knowledge Tracing,,"Knowledge tracing (KT) enhances student learning by leveraging past
performance to predict future performance. Current research utilizes models
based on attention mechanisms and recurrent neural network structures to
capture long-term dependencies and correlations between exercises, aiming to
improve model accuracy. Due to the growing amount of data in smart education
scenarios, this poses a challenge in terms of time and space consumption for
knowledge tracing models. However, existing research often overlooks the
efficiency of model training and inference and the constraints of training
resources. Recognizing the significance of prioritizing model efficiency and
resource usage in knowledge tracing, we introduce Mamba4KT. This novel model is
the first to explore enhanced efficiency and resource utilization in knowledge
tracing. We also examine the interpretability of the Mamba structure both
sequence-level and exercise-level to enhance model interpretability.
Experimental findings across three public datasets demonstrate that Mamba4KT
achieves comparable prediction accuracy to state-of-the-art models while
significantly improving training and inference efficiency and resource
utilization. As educational data continues to grow, our work suggests a
promising research direction for knowledge tracing that improves model
prediction accuracy, model efficiency, resource utilization, and
interpretability simultaneously.","Y Cao,W Zhang- arXiv preprint arXiv:2405.16542, 2024 - arxiv.org",https://arxiv.org/abs/2405.16542,
2024년 6월 24일 오후 11:27 (GMT+8),Venturing into uncharted waters: The navigation compass from transformer to mamba,Survey-Non-RecSys,,Mamba (Survey),,"Transformer, a deep neural network architecture, has long dominated the field
of natural language processing and beyond. Nevertheless, the recent
introduction of Mamba challenges its supremacy, sparks considerable interest
among researchers, and gives rise to a series of Mamba-based models that have
exhibited notable potential. This survey paper orchestrates a comprehensive
discussion, diving into essential research dimensions, covering: (i) the
functioning of the Mamba mechanism and its foundation on the principles of
structured state space models; (ii) the proposed improvements and the
integration of Mamba with various networks, exploring its potential as a
substitute for Transformers; (iii) the combination of Transformers and Mamba to
compensate for each other's shortcomings. We have also made efforts to
interpret Mamba and Transformer in the framework of kernel functions, allowing
for a comparison of their mathematical nature within a unified context. Our
paper encompasses the vast majority of improvements related to Mamba to date.","Y Zou, Y Chen,Z Li, L Zhang, H Zhao - arXiv preprint arXiv:2406.16722, 2024 - arxiv.org",https://arxiv.org/abs/2406.16722,
,Frequency-Augmented Mixture-of-Heterogeneous-Experts Framework for Sequential Recommendation,Non-Mamba,,MOE based recommendation (Mamba is only used for comparison),,,"J Zhang,R Xie, H Lu, W Sun,WX Zhao… - Proceedings of the …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3696410.3714663?casa_token=QcnIkwhRzzsAAAAA:R0HGq5B71iac1r-lMpoMRyX1pqKe1ocAGO1w-CZwqFUfpsOES1ja740mm7g3KeBJE9LSxHw41c2PMQ,
2024년 7월 7일 오후 4:37 (GMT+8),Mamba hawkes process,Concrete-RecSys-Related,,Mamba + Hawkes Process (Sequential Modelling),,"Irregular and asynchronous event sequences are prevalent in many domains,
such as social media, finance, and healthcare. Traditional temporal point
processes (TPPs), like Hawkes processes, often struggle to model mutual
inhibition and nonlinearity effectively. While recent neural network models,
including RNNs and Transformers, address some of these issues, they still face
challenges with long-term dependencies and computational efficiency. In this
paper, we introduce the Mamba Hawkes Process (MHP), which leverages the Mamba
state space architecture to capture long-range dependencies and dynamic event
interactions. Our results show that MHP outperforms existing models across
various datasets. Additionally, we propose the Mamba Hawkes Process Extension
(MHP-E), which combines Mamba and Transformer models to enhance predictive
capabilities. We present the novel application of the Mamba architecture to
Hawkes processes, a flexible and extensible model structure, and a theoretical
analysis of the synergy between state space models and Hawkes processes.
Experimental results demonstrate the superior performance of both MHP and
MHP-E, advancing the field of temporal point process modeling.","A Gao, S Dai,Y Hu- arXiv preprint arXiv:2407.05302, 2024 - arxiv.org",https://arxiv.org/abs/2407.05302,
,Toward secure industrial internet of behaviours: a federated learning-based lightweight human behaviour recognition method with selective state space models,Concrete-Non-RecSys,,Mamba + Federated Learning,,,"B Hu,R Zhong, Y Feng, J Yang, P Li… - International Journal of …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/00207543.2024.2448604?casa_token=soP0AdahfZoAAAAA:L9_m4RKdud3WzVPgZYK8zJ3CU6nrFr-jGpkBM1Xieqzy7qM5Bymk7kp8U_f1XRHhwIaojTIIQMsy1tG8-A,
2024년 5월 29일 오전 2:38 (GMT+8),"Modeling Long Sequences in Bladder Cancer Recurrence: A Comparative Evaluation of LSTM, Transformer, and Mamba",Concrete-Non-RecSys,,Mamba + Survival Analysis,,"Traditional survival analysis methods often struggle with complex
time-dependent data,failing to capture and interpret dynamic characteristics
adequately.This study aims to evaluate the performance of three long-sequence
models,LSTM,Transformer,and Mamba,in analyzing recurrence event data and
integrating them with the Cox proportional hazards model.This study integrates
the advantages of deep learning models for handling long-sequence data with the
Cox proportional hazards model to enhance the performance in analyzing
recurrent events with dynamic time information.Additionally,this study compares
the ability of different models to extract and utilize features from
time-dependent clinical recurrence data.The LSTM-Cox model outperformed both
the Transformer-Cox and Mamba-Cox models in prediction accuracy and model
fit,achieving a Concordance index of up to 0.90 on the test set.Significant
predictors of bladder cancer recurrence,such as treatment stop time,maximum
tumor size at recurrence and recurrence frequency,were identified.The LSTM-Cox
model aligned well with clinical outcomes,effectively distinguishing between
high-risk and low-risk patient groups.This study demonstrates that the LSTM-Cox
model is a robust and efficient method for recurrent data analysis and feature
extraction,surpassing newer models like Transformer and Mamba.It offers a
practical approach for integrating deep learning technologies into clinical
risk prediction systems,thereby improving patient management and treatment
outcomes.","R Zhang, J Jiang, X Shi - arXiv preprint arXiv:2405.18518, 2024 - arxiv.org",https://arxiv.org/abs/2405.18518,
2025년 2월 11일 오전 1:37 (GMT+8),FinMamba: Market-Aware Graph Enhanced Multi-Level Mamba for Stock Movement Prediction,Concrete-Non-RecSys,,Mamba + Stock Movement Prediction,,"Recently, combining stock features with inter-stock correlations has become a
common and effective approach for stock movement prediction. However, financial
data presents significant challenges due to its low signal-to-noise ratio and
the dynamic complexity of the market, which give rise to two key limitations in
existing methods. First, the relationships between stocks are highly influenced
by multifaceted factors including macroeconomic market dynamics, and current
models fail to adaptively capture these evolving interactions under specific
market conditions. Second, for the accuracy and timeliness required by
real-world trading, existing financial data mining methods struggle to extract
beneficial pattern-oriented dependencies from long historical data while
maintaining high efficiency and low memory consumption. To address the
limitations, we propose FinMamba, a Mamba-GNN-based framework for market-aware
and multi-level hybrid stock movement prediction. Specifically, we devise a
dynamic graph to learn the changing representations of inter-stock
relationships by integrating a pruning module that adapts to market trends.
Afterward, with a selective mechanism, the multi-level Mamba discards
irrelevant information and resets states to skillfully recall historical
patterns across multiple time scales with linear time costs, which are then
jointly optimized for reliable prediction. Extensive experiments on U.S. and
Chinese stock markets demonstrate the effectiveness of our proposed FinMamba,
achieving state-of-the-art prediction accuracy and trading profitability, while
maintaining low computational complexity. The code is available at
https://github.com/TROUBADOUR000/FinMamba.","Y Hu,P Liu, Y Li,D Cheng,N Li,T Dai, J Bao… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2502.06707,
,Mamba: A Competitive Alternative to Lstm for Soil Moisture Retrieval,Concrete-Non-RecSys,,Mamba + Soil Moisture Retrieval (??),,,"S Du,Y Zha,L Shi- Available at SSRN 4962468 - papers.ssrn.com",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4962468,
2024년 9월 28일 오전 5:14 (GMT+8),TTT4Rec: A Test-Time Training Approach for Rapid Adaption in Sequential Recommendation,Concrete-RecSys,,"Mamba + Recsys, Transformer backbone outperforms Mamback backbone.",,"Sequential recommendation tasks, which aim to predict the next item a user
will interact with, typically rely on models trained solely on historical data.
However, in real-world scenarios, user behavior can fluctuate in the long
interaction sequences, and training data may be limited to model this dynamics.
To address this, Test-Time Training (TTT) offers a novel approach by using
self-supervised learning during inference to dynamically update model
parameters. This allows the model to adapt to new user interactions in
real-time, leading to more accurate recommendations. In this paper, we propose
TTT4Rec, a sequential recommendation framework that integrates TTT to better
capture dynamic user behavior. By continuously updating model parameters during
inference, TTT4Rec is particularly effective in scenarios where user
interaction sequences are long, training data is limited, or user behavior is
highly variable. We evaluate TTT4Rec on three widely-used recommendation
datasets, demonstrating that it achieves performance on par with or exceeding
state-of-the-art models. The codes are available at
https://github.com/ZhaoqiZachYang/TTT4Rec.","Z Yang,Y Wang,Y Ge- arXiv preprint arXiv:2409.19142, 2024 - arxiv.org",https://arxiv.org/abs/2409.19142,
2024년 3월 26일 오후 8:08 (GMT+8),Retentive decision transformer with adaptive masking for reinforcement learning based recommendation systems,Non-Mamba,,,,"Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.","S Wang,X Chen,L Yao- arXiv preprint arXiv:2403.17634, 2024 - arxiv.org",https://arxiv.org/abs/2403.17634,
,An empirical study on normalization in mamba,Concrete-Non-RecSys,,Mamba Enhanced,,,"P Feng, Y Wang, Y Ni, Z Li, W Wu, L Huang - 2025 - openreview.net",https://openreview.net/forum?id=YK8eO7BEkJ,
,Covariance Attention Guidance Mamba Hashing for cross-modal retrieval,Concrete-RecSys-Related,,Mamba + Multimodal Retrieval,,,"G Wang,S Cheng, A Du,Q Zou- Engineering Applications of Artificial …, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0952197625007778,
,Transformers to ssms: Distilling quadratic knowledge to subquadratic models,Concrete-Non-RecSys,,Mamba + Language Modeling,,,"A Bick,K Li,E Xing,JZ Kolter… - Advances in Neural …, 2024 - proceedings.neurips.cc",https://proceedings.neurips.cc/paper_files/paper/2024/hash/3848fef259495bfd04d60cdc5c1b4db7-Abstract-Conference.html,
2024년 10월 9일 오전 2:31 (GMT+8),Monocular Visual Place Recognition in LiDAR Maps via Cross-Modal State Space Model and Multi-View Matching,Concrete-Non-RecSys,,Mamba + Vision,,"Achieving monocular camera localization within pre-built LiDAR maps can
bypass the simultaneous mapping process of visual SLAM systems, potentially
reducing the computational overhead of autonomous localization. To this end,
one of the key challenges is cross-modal place recognition, which involves
retrieving 3D scenes (point clouds) from a LiDAR map according to online RGB
images. In this paper, we introduce an efficient framework to learn descriptors
for both RGB images and point clouds. It takes visual state space model
(VMamba) as the backbone and employs a pixel-view-scene joint training strategy
for cross-modal contrastive learning. To address the field-of-view differences,
independent descriptors are generated from multiple evenly distributed
viewpoints for point clouds. A visible 3D points overlap strategy is then
designed to quantify the similarity between point cloud views and RGB images
for multi-view supervision. Additionally, when generating descriptors from
pixel-level features using NetVLAD, we compensate for the loss of geometric
information, and introduce an efficient scheme for multi-view generation.
Experimental results on the KITTI and KITTI-360 datasets demonstrate the
effectiveness and generalization of our method. The code will be released upon
acceptance.","G Yao,X Li, L Fu,Y Pan- arXiv preprint arXiv:2410.06285, 2024 - arxiv.org",https://arxiv.org/abs/2410.06285,
2025년 4월 27일 오후 8:51 (GMT+8),Alphafuse: Learn id embeddings for sequential recommendation in null space of language embeddings,Non-Mamba,,,,"Recent advancements in sequential recommendation have underscored the
potential of Large Language Models (LLMs) for enhancing item embeddings.
However, existing approaches face three key limitations: 1) the degradation of
the semantic space when high-dimensional language embeddings are mapped to
lower-dimensional ID embeddings, 2) the underutilization of language
embeddings, and 3) the reliance on additional trainable parameters, such as an
adapter, to bridge the gap between the semantic and behavior spaces. In this
paper, we introduce AlphaFuse, a simple but effective language-guided learning
strategy that addresses these challenges by learning ID embeddings within the
null space of language embeddings. Specifically, we decompose the semantic
space of language embeddings via Singular Value Decomposition (SVD),
distinguishing it into a semantic-rich row space and a semantic-sparse null
space. Collaborative signals are then injected into the null space, while
preserving the rich semantics of the row space. AlphaFuse prevents degradation
of the semantic space, integrates the retained language embeddings into the
final item embeddings, and eliminates the need for auxiliary trainable modules,
enabling seamless adaptation to any sequential recommendation framework. We
validate the effectiveness and flexibility of AlphaFuse through extensive
experiments on three benchmark datasets, including cold-start user and
long-tail settings, showcasing significant improvements in both discriminative
and diffusion-based generative sequential recommenders. Our codes and datasets
are available at https://github.com/Hugo-Chinn/AlphaFuse.","G Hu,A Zhang,S Liu, Z Cai,X Yang… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2504.19218,
2024년 9월 18일 오전 11:34 (GMT+8),An Enhanced-State Reinforcement Learning Algorithm for Multi-Task Fusion in Large-Scale Recommender Systems,Non-Mamba,,,,"As the last key stage of Recommender Systems (RSs), Multi-Task Fusion (MTF)
is in charge of combining multiple scores predicted by Multi-Task Learning
(MTL) into a final score to maximize user satisfaction, which decides the
ultimate recommendation results. In recent years, to maximize long-term user
satisfaction within a recommendation session, Reinforcement Learning (RL) is
widely used for MTF in large-scale RSs. However, limited by their modeling
pattern, all the current RL-MTF methods can only utilize user features as the
state to generate actions for each user, but unable to make use of item
features and other valuable features, which leads to suboptimal results.
Addressing this problem is a challenge that requires breaking through the
current modeling pattern of RL-MTF. To solve this problem, we propose a novel
method called Enhanced-State RL for MTF in RSs. Unlike the existing methods
mentioned above, our method first defines user features, item features, and
other valuable features collectively as the enhanced state; then proposes a
novel actor and critic learning process to utilize the enhanced state to make
much better action for each user-item pair. To the best of our knowledge, this
novel modeling pattern is being proposed for the first time in the field of
RL-MTF. We conduct extensive offline and online experiments in a large-scale
RS. The results demonstrate that our model outperforms other models
significantly. Enhanced-State RL has been fully deployed in our RS more than
half a year, improving +3.84% user valid consumption and +0.58% user duration
time compared to baseline.","P Liu, J Zhu, C Xu, M Zhao, B Wang - arXiv preprint arXiv:2409.11678, 2024 - arxiv.org",https://arxiv.org/abs/2409.11678,
2024년 4월 30일 오후 5:40 (GMT+8),Clip-mamba: Clip pretrained mamba models with ood and hessian evaluation,Concrete-Non-RecSys,,Mamba + Vision Language Modeling,,"State space models and Mamba-based models have been increasingly applied
across various domains, achieving state-of-the-art performance. This technical
report introduces the first attempt to train a transferable Mamba model
utilizing contrastive language-image pretraining (CLIP). We have trained Mamba
models of varying sizes and undertaken comprehensive evaluations of these
models on 26 zero-shot classification datasets and 16 out-of-distribution (OOD)
datasets. Our findings reveal that a Mamba model with 67 million parameters is
on par with a 307 million-parameter Vision Transformer (ViT) model in zero-shot
classification tasks, highlighting the parameter efficiency of Mamba models. In
tests of OOD generalization, Mamba-based models exhibit exceptional performance
in conditions of OOD image contrast or when subjected to high-pass filtering.
However, a Hessian analysis indicates that Mamba models feature a sharper and
more non-convex landscape compared to ViT-based models, making them more
challenging to train. The source code is available at
https://github.com/raytrun/mamba-clip.","W Huang, Y Shen,Y Yang- arXiv preprint arXiv:2404.19394, 2024 - arxiv.org",https://arxiv.org/abs/2404.19394,
,Enhanced Mamba model with multi-head attention mechanism and learnable scaling parameters for remaining useful life prediction,Concrete-Non-RecSys,,Mamba + Remaining Useful Life Prediction,,,"F Liu, S Liu, Y Chai, Y Zhu - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-91815-1,
2025년 4월 12일 오후 7:57 (GMT+8),Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction,Concrete-Non-RecSys,,Mamba Enhanced (via Repetitive Contrastive Learning),,"Long sequence prediction is a key challenge in time series forecasting. While
Mamba-based models have shown strong performance due to their sequence
selection capabilities, they still struggle with insufficient focus on critical
time steps and incomplete noise suppression, caused by limited selective
abilities. To address this, we introduce Repetitive Contrastive Learning (RCL),
a token-level contrastive pretraining framework aimed at enhancing Mamba's
selective capabilities. RCL pretrains a single Mamba block to strengthen its
selective abilities and then transfers these pretrained parameters to
initialize Mamba blocks in various backbone models, improving their temporal
prediction performance. RCL uses sequence augmentation with Gaussian noise and
applies inter-sequence and intra-sequence contrastive learning to help the
Mamba module prioritize information-rich time steps while ignoring noisy ones.
Extensive experiments show that RCL consistently boosts the performance of
backbone models, surpassing existing methods and achieving state-of-the-art
results. Additionally, we propose two metrics to quantify Mamba's selective
capabilities, providing theoretical, qualitative, and quantitative evidence for
the improvements brought by RCL.","W Yan, H Cao, Y Tan - arXiv preprint arXiv:2504.09185, 2025 - arxiv.org",https://arxiv.org/abs/2504.09185,
2024년 5월 21일 오후 5:04 (GMT+8),Mamba in speech: Towards an alternative to self-attention,Concrete-Non-RecSys,,Mamba + Speech Tasks,,"Transformer and its derivatives have achieved success in diverse tasks across
computer vision, natural language processing, and speech processing. To reduce
the complexity of computations within the multi-head self-attention mechanism
in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an
alternative. Mamba exhibited its effectiveness in natural language processing
and computer vision tasks, but its superiority has rarely been investigated in
speech signal processing. This paper explores solutions for applying Mamba to
speech processing by discussing two typical speech processing tasks: speech
recognition, which requires semantic and sequential information, and speech
enhancement, which focuses primarily on sequential patterns. The experimental
results confirm that bidirectional Mamba (BiMamba) consistently outperforms
vanilla Mamba, highlighting the advantages of a bidirectional design for speech
processing. Moreover, experiments demonstrate the effectiveness of BiMamba as
an alternative to the self-attention module in the Transformer model and its
derivates, particularly for the semantic-aware task. The crucial technologies
for transferring Mamba to speech are then summarized in ablation studies and
the discussion section, offering insights for extending this research to a
broader scope of tasks.","X Zhang,Q Zhang,H Liu, T Xiao,X Qian… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2405.12609,
2024년 5월 13일 오후 10:21 (GMT+8),GMSR: gradient-guided mamba for spectral reconstruction from RGB images,Concrete-Non-RecSys,,Mamba + Vision (Spectral Reconstruction),,"Mainstream approaches to spectral reconstruction (SR) primarily focus on
designing Convolution- and Transformer-based architectures. However, CNN
methods often face challenges in handling long-range dependencies, whereas
Transformers are constrained by computational efficiency limitations. Recent
breakthroughs in state-space model (e.g., Mamba) has attracted significant
attention due to its near-linear computational efficiency and superior
performance, prompting our investigation into its potential for SR problem. To
this end, we propose the Gradient-guided Mamba for Spectral Reconstruction from
RGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by a
global receptive field and linear computational complexity. Its core comprises
multiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch
structure. In addition to benefiting from efficient global feature
representation by Mamba block, we further innovatively introduce spatial
gradient attention and spectral gradient attention to guide the reconstruction
of spatial and spectral cues. GMSR-Net demonstrates a significant
accuracy-efficiency trade-off, achieving state-of-the-art performance while
markedly reducing the number of parameters and computational burdens. Compared
to existing approaches, GMSR-Net slashes parameters and FLOPS by substantial
margins of 10 times and 20 times, respectively. Code is available at
https://github.com/wxy11-27/GMSR.","X Wang,Z Huang, S Zhang,J Zhu,P Gamba… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2405.07777,Cited by 8
,A Comparative Analysis of Modeling Paradigms and Techniques in Sequential Recommendation,Survey-RecSys,,Mamba + Sequence Recommendation (Survey),,,"Y Jia - The 4th International Conference on Economic …, 2024 - books.google.com",https://books.google.com/books?hl=en&lr=&id=-3olEQAAQBAJ&oi=fnd&pg=PA329&dq=(mamba+OR+%22state+space%22+OR+%22state-space%22+OR+%22state+spaces%22+OR+%22state-spaces%22)+AND+(recommend+OR+recommender+OR+recommendation)&ots=Pn8ZofnJ1J&sig=kbGfHUb9f70AZjqSY2oV2ulICNA,
2025년 4월 3일 오후 8:44 (GMT+8),Audio-visual controlled video diffusion with masked selective state spaces modeling for natural talking head generation,Concrete-Non-RecSys,,Mamba + Video Generation,,"Talking head synthesis is vital for virtual avatars and human-computer
interaction. However, most existing methods are typically limited to accepting
control from a single primary modality, restricting their practical utility. To
this end, we introduce \textbf{ACTalker}, an end-to-end video diffusion
framework that supports both multi-signals control and single-signal control
for talking head video generation. For multiple control, we design a parallel
mamba structure with multiple branches, each utilizing a separate driving
signal to control specific facial regions. A gate mechanism is applied across
all branches, providing flexible control over video generation. To ensure
natural coordination of the controlled video both temporally and spatially, we
employ the mamba structure, which enables driving signals to manipulate feature
tokens across both dimensions in each branch. Additionally, we introduce a
mask-drop strategy that allows each driving signal to independently control its
corresponding facial region within the mamba structure, preventing control
conflicts. Experimental results demonstrate that our method produces
natural-looking facial videos driven by diverse signals and that the mamba
layer seamlessly integrates multiple driving modalities without conflict. The
project website can be found at
https://harlanhong.github.io/publications/actalker/index.html.","FT Hong, Z Xu,Z Zhou, J Zhou, X Li, Q Lin… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2504.02542,
,MiM-UNet: An efficient building image segmentation network integrating state space models,Concrete-Non-RecSys,,Mamba + U-net + Vision (Image Segmentation),,,"D Liu, Z Wang,A Liang- Alexandria Engineering Journal, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S1110016825002029,
,Enhancing Heart Rate Prediction with Mamba and Simple-Attention Modules for Early Warning in Exercise,Concrete-Non-RecSys,,Mamba + Heart Rate Prediction,,,"H Zhang, Q Zhan, Z Wang… - 2024 5th International …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10762452/,Related articles
,Selective state models are what you need for animal action recognition,Concrete-Non-RecSys,,Mamba + Vision (Recognition),,,"E Fazzari,D Romano,F Falchi,C Stefanini- Ecological Informatics, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S1574954124004977,
2024년 9월 9일 오후 9:16 (GMT+8),DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification,Concrete-Non-RecSys,,Mamba + Driver Distraction Identification (Vision + Health Stats),,"Driver distraction remains a leading cause of traffic accidents, posing a
critical threat to road safety globally. As intelligent transportation systems
evolve, accurate and real-time identification of driver distraction has become
essential. However, existing methods struggle to capture both global contextual
and fine-grained local features while contending with noisy labels in training
datasets. To address these challenges, we propose DSDFormer, a novel framework
that integrates the strengths of Transformer and Mamba architectures through a
Dual State Domain Attention (DSDA) mechanism, enabling a balance between
long-range dependencies and detailed feature extraction for robust driver
behavior recognition. Additionally, we introduce Temporal Reasoning Confident
Learning (TRCL), an unsupervised approach that refines noisy labels by
leveraging spatiotemporal correlations in video sequences. Our model achieves
state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and
demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin
platform. Extensive experimental results confirm that DSDFormer and TRCL
significantly improve both the accuracy and robustness of driver distraction
detection, offering a scalable solution to enhance road safety.","J Chen, Z Zhang, J Yu, H Huang, R Zhang, X Xu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.05587,
2024년 7월 30일 오후 11:12 (GMT+8),MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network,Concrete-Non-RecSys,,Mamba + Heart Rhythm Sequence Classification,,"Cardiac arrhythmia, a condition characterized by irregular heartbeats, often
serves as an early indication of various heart ailments. With the advent of
deep learning, numerous innovative models have been introduced for diagnosing
arrhythmias using Electrocardiogram (ECG) signals. However, recent studies
solely focus on the performance of models, neglecting the interpretation of
their results. This leads to a considerable lack of transparency, posing a
significant risk in the actual diagnostic process. To solve this problem, this
paper introduces MambaCapsule, a deep neural networks for ECG arrhythmias
classification, which increases the explainability of the model while enhancing
the accuracy.Our model utilizes Mamba for feature extraction and Capsule
networks for prediction, providing not only a confidence score but also signal
features. Akin to the processing mechanism of human brain, the model learns
signal features and their relationship between them by reconstructing ECG
signals in the predicted selection. The model evaluation was conducted on
MIT-BIH and PTB dataset, following the AAMI standard. MambaCapsule has achieved
a total accuracy of 99.54% and 99.59% on the test sets respectively. These
results demonstrate the promising performance of under the standard test
protocol.","Y Xu, X Liu, Z Kong,Y Wu, Y Wang,Y Lu,H Gao… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2407.20893,
2024년 10월 3일 오후 5:38 (GMT+8),TRIS-HAR: Transmissive Reconfigurable Intelligent Surfaces-assisted Cognitive Wireless Human Activity Recognition Using State Space Models,Concrete-Non-RecSys,,Mamba + Human Activity Recognition,,"Human activity recognition (HAR) using radio frequency (RF) signals has
garnered considerable attention for its applications in smart environments.
However, traditional systems often struggle with limited independent channels
between transmitters and receivers, multipath fading, and environmental noise,
which particularly degrades performance in through-the-wall scenarios. In this
paper, we present a transmissive reconfigurable intelligent surface
(TRIS)-assisted through-the-wall human activity recognition (TRIS-HAR) system.
The system employs TRIS technology to actively reshape wireless signal
propagation, creating multiple independent paths to enhance signal clarity and
improve recognition accuracy in complex indoor settings. Additionally, we
propose the Human intelligence Mamba (HiMamba), an advanced state space model
that captures temporal and frequency-based information for precise activity
recognition. HiMamba achieves state-of-the-art performance on two public
datasets, demonstrating superior accuracy. Extensive experiments indicate that
the TRIS-HAR system improves recognition performance from 85.00% to 98.06% in
laboratory conditions and maintains high performance across various
environments. This approach offers a robust solution for enhancing RF-based
HAR, with promising applications in smart home and elderly care systems.","J Liu,Y Huang,X Shi,R Xiong, J Zhang,T Mi… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2410.02334,
,HiSS-Csp: Hierarchical State Space Models for Continuous Sequence Prediction,Concrete-RecSys-Related,,"Mamba Enhanced + Sequence Modeling
CMU + FAIR(Meta) + NYU ",,,V Pattabiraman- 2024 - search.proquest.com,https://search.proquest.com/openview/ec31c56b57eaa922eaf25b64bde0e88f/1?pq-origsite=gscholar&cbl=18750&diss=y,Related articles
2024년 9월 25일 오후 1:12 (GMT+8),"Train once, deploy anywhere: Matryoshka representation learning for multimodal recommendation",Concrete-RecSys,,"Not exactly Mamba, but Inspired by linear recurrence
UC Berkerly",,"Despite recent advancements in language and vision modeling, integrating rich
multimodal knowledge into recommender systems continues to pose significant
challenges. This is primarily due to the need for efficient recommendation,
which requires adaptive and interactive responses. In this study, we focus on
sequential recommendation and introduce a lightweight framework called
full-scale Matryoshka representation learning for multimodal recommendation
(fMRLRec). Our fMRLRec captures item features at different granularities,
learning informative representations for efficient recommendation across
multiple dimensions. To integrate item features from diverse modalities,
fMRLRec employs a simple mapping to project multimodal item features into an
aligned feature space. Additionally, we design an efficient linear
transformation that embeds smaller features into larger ones, substantially
reducing memory requirements for large-scale training on recommendation data.
Combined with improved state space modeling techniques, fMRLRec scales to
different dimensions and only requires one-time training to produce multiple
models tailored to various granularities. We demonstrate the effectiveness and
efficiency of fMRLRec on multiple benchmark datasets, which consistently
achieves superior performance over state-of-the-art baseline methods. We make
our code and data publicly available at https://github.com/yueqirex/fMRLRec.","Y Wang,Z Yue,H Zeng,D Wang,J McAuley- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.16627,
,CS660-Final Report Applying Mamba to GNNs,,,What is this?,,,J Shivottam,,Related articles
2024년 9월 30일 오후 12:28 (GMT+8),Maskmamba: A hybrid mamba-transformer model for masked image generation,Concrete-Non-RecSys,,Mamba + Vision (Image Generation),,"Image generation models have encountered challenges related to scalability
and quadratic complexity, primarily due to the reliance on Transformer-based
backbones. In this study, we introduce MaskMamba, a novel hybrid model that
combines Mamba and Transformer architectures, utilizing Masked Image Modeling
for non-autoregressive image synthesis. We meticulously redesign the
bidirectional Mamba architecture by implementing two key modifications: (1)
replacing causal convolutions with standard convolutions to better capture
global context, and (2) utilizing concatenation instead of multiplication,
which significantly boosts performance while accelerating inference speed.
Additionally, we explore various hybrid schemes of MaskMamba, including both
serial and grouped parallel arrangements. Furthermore, we incorporate an
in-context condition that allows our model to perform both class-to-image and
text-to-image generation tasks. Our MaskMamba outperforms Mamba-based and
Transformer-based models in generation quality. Notably, it achieves a
remarkable $54.44\%$ improvement in inference speed at a resolution of
$2048\times 2048$ over Transformer.","W Chen,L Niu,Z Lu,F Meng, J Zhou - arXiv preprint arXiv:2409.19937, 2024 - arxiv.org",https://arxiv.org/abs/2409.19937,
,Simulation of association rule mining based on sensor networks in Chinese language learning recommendation system for college students,Non-Mamba,,RecSys + RL,,,"L Songyang - Measurement: Sensors, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S2665917424001843,
,Visuomotor Policy Learning for Task Automation of Surgical Robot,Concrete-Non-RecSys,,Mamba + RL,,,"J Huang,Q Shi,D Xie, Y Ma, X Liu… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10685114/,Cited by 2
,Multi-head spatial-spectral mamba for hyperspectral image classification,Concrete-Non-RecSys,,Mamba + Vision (Image Classification),,,"M Ahmad,MHF Butt,M Usama,HA Altuwaijri… - Remote Sensing …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/2150704X.2025.2461330?casa_token=-3G7KgFBx7YAAAAA:9ACyhmD31C8Ehc5RjhgiD6baf1FEOEXHGYiEsCDsF_4eBsz7VLsxSvkDw1xWI1_9afR9K1EK9bHUqVC2mg,
,Reinforcement learning recommendation algorithm based on environment information exploration,Non-Mamba,,RecSys + RL,,,"J Jia, H Wang, F Wang - Proceedings of the 2024 2nd International …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3724504.3724562?casa_token=BSequl-6SqYAAAAA:fvwTX1ay68hb1nKOJ48XHlZ6xNR1V0Kh3iqRJQ2VISaBxk64ROZdVuZ2ZDSWpNIt7e_bhaMf5dha1Q&casa_token=ju_gpdlQ-fEAAAAA:8QKkIoXk0d8L2zR7JkGQEXwNsNXOFC3cQxam0GasRi4h_4WiXHimgpovbBvxkRYGuVYIlKDmGPkZXw,
,Seasonal forecasting of Pan-Arctic sea ice with state space model,Concrete-Non-RecSys,,Mamba + Weather Forecasting,,,"W Wang,W Yang, L Wang, G Wang… - npj Climate and …, 2025 - nature.com",https://www.nature.com/articles/s41612-025-01058-0,
,ReMamba: a hybrid CNN-Mamba aggregation network for visible-infrared person re-identification,Concrete-Non-RecSys,,Mamba + Vision,,,"H Geng, J Peng, W Yang, D Chen, H Lv, G Li… - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-80766-8,
,A Reinforcement Learning Based Recommender System Framework for Web Apps: Radio and Game Aggregators Scenarios,Non-Mamba,,RecSys + RL,,,"A Batista,JM Torres,P Sobral,RS Moreira… - EPIA Conference on …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-73497-7_34,Cited by 1
,Session-aware recommender system using double deep reinforcement learning,Non-Mamba,,RecSys + RL,,,"P Khurana, B Gupta,R Sharma,P Bedi- Journal of Intelligent Information …, 2024 - Springer",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10844-023-00824-x&casa_token=QifoqjG9T4gAAAAA:KCu2mAeZLwFGP1koxfww-OhCXq_81m-eYde5qqIkv2wfKn_SUC1MJLtCPcD8uzyjtMzvDdeD1TpCtfjptA,
,Personalized Recommendation Using Deep Reinforcement Learning for Educational Content,Non-Mamba,,RecSys + RL,,,"Y Su - International Journal of Image and Graphics, 2025 - World Scientific",https://www.worldscientific.com/doi/abs/10.1142/S021946782750029X,Related articles
,On the opportunities and challenges of offline reinforcement learning for recommender systems,Non-Mamba,,RecSys + RL,,,"X Chen,S Wang,J McAuley,D Jannach… - ACM Transactions on …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3661996?casa_token=138LFUmWIJ0AAAAA:UjS_ntR3x8sC9J1lY-bjrMskPWWU6ba3Cyue0X4zp6s4sEu2DD3FOeZ0mcl6MlPTOcmoD7PVu1_uFw,
,Behavior-Dependent Linear Recurrent Units for Efficient Sequential Recommendation,Concrete-RecSys,,Mamba + RecSys,,,"C Liu,J Lin,H Liu,J Wang,J Caverlee- Proceedings of the 33rd ACM …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3627673.3679717,
,Mastering seismic time series response predictions using an attention-Mamba transformer model for bridge bearings and piers across varied testing conditions,Concrete-Non-RecSys,,Mamba + Time Series Analysis,,,"O Yazdanpanah,M Park,M Chang,Y Chae- Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-79195-4,
2024년 10월 15일 오전 4:18 (GMT+8),CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning,Concrete-Non-RecSys,,Mamba + Speech Denoising,,"This paper presents CleanUMamba, a time-domain neural network architecture
designed for real-time causal audio denoising directly applied to raw
waveforms. CleanUMamba leverages a U-Net encoder-decoder structure,
incorporating the Mamba state-space model in the bottleneck layer. By replacing
conventional self-attention and LSTM mechanisms with Mamba, our architecture
offers superior denoising performance while maintaining a constant memory
footprint, enabling streaming operation. To enhance efficiency, we applied
structured channel pruning, achieving an 8X reduction in model size without
compromising audio quality. Our model demonstrates strong results in the
Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba
achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and
468M MACs, matching or outperforming larger models in real-time performance.
Code will be available at: https://github.com/lab-emi/CleanUMamba","S Groot,Q Chen,JC van Gemert,C Gao- arXiv preprint arXiv:2410.11062, 2024 - arxiv.org",https://arxiv.org/abs/2410.11062,
,[HTML][HTML]Competency and skill-based educational recommendation system,Non-Mamba,,Classic RecSys,,,"RGF Feitosa,GAL Campos,IS Santos… - International Journal of …, 2025 - Springer",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s40593-024-00423-z&casa_token=21r3OLtPHL0AAAAA:7YJw7SNfovhSNwWIoOm-1wapCh44zaHml-qmdurBswsHe8wFSv08HfRqMdE6DC_bBYsTF8DWO8-Gp0x1cw,
,Design of dynamic career path recommendation system based on Markov decision process,Non-Mamba,,RecSys + RL,,,"Y Jiang - Journal of Computational Methods in Sciences and …, 2025 - journals.sagepub.com",https://journals.sagepub.com/doi/abs/10.1177/14727978241313261,Related articles
,Using meta-learning to recommend an appropriate time-series forecasting model,Non-Mamba,,RecSys + Meta-Learning,,,"N Talkhi, N Akhavan Fatemi,M Jabbari Nooghabi… - BMC Public Health, 2024 - Springer",https://link.springer.com/article/10.1186/s12889-023-17627-y,
2025년 5월 28일 오전 9:58 (GMT+8),HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis,Concrete-Non-RecSys,,Non-RecSys,,"In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge, this is the first work to explore and effectively model momentum
across multiple granularities in professional tennis tournaments.","R Li,X Zhao,Q Ning, S Guo - arXiv preprint arXiv:2505.21882, 2025 - arxiv.org",https://arxiv.org/abs/2505.21882,Related articles
,Towards knowledge-aware and deep reinforced cross-domain recommendation over collaborative knowledge graph,Non-Mamba,,RecSys + RL,,,"Y Li,L Hou,J Li- IEEE Transactions on Knowledge and Data …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10505847/,Cited by 4
,A reinforcement learning recommender system using bi-clustering and Markov Decision Process,Non-Mamba,,RecSys + RL,,,"A Iftikhar,MA Ghazanfar,M Ayub,SA Alahmari… - Expert Systems with …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417423020432,
,Mamba? catch the hype or rethink what really helps for image registration,Concrete-Non-RecSys,,Mamba + Vision (Image Restoration),,,"B Jian,J Pan,M Ghahremani,D Rueckert… - … on Biomedical Image …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-73480-9_7,
,Strategies for mitigating challenges associated with trace organic compound removal by high-retention membrane bioreactors (HR-MBRs),Non-Mamba,,"Completely irrelevant, Author’s last name is Mamba",,,"OT Mahlangu,TI Nkambule,BB Mamba,FI Hai- npj Clean Water, 2024 - nature.com",https://www.nature.com/articles/s41545-024-00313-w,
,Conditional Information Bottleneck Approach for Out-of-Distribution Sequential Recommendation,Non-Mamba,,Sequential RecSys,,,"B Wu, W Shen, W Xie,J Zheng, Y Cheng, Q Ma - openreview.net",https://openreview.net/forum?id=h9dnHqrkfa,
2024년 4월 26일 오전 4:49 (GMT+8),Learning Actionable Counterfactual Explanations in Large State Spaces,Completely Irrelevant,,Completely irrelevant,,"Recourse generators provide actionable insights, often through feature-based
counterfactual explanations (CFEs), to help negatively classified individuals
understand how to adjust their input features to achieve a positive
classification. These feature-based CFEs, which we refer to as \emph{low-level}
CFEs, are overly specific (e.g., coding experience: \(4 \to 5+\) years) and
often recommended in a feature space that doesn't straightforwardly align with
real-world actions. To bridge this gap, we introduce three novel recourse types
grounded in real-world actions: high-level continuous (\emph{hl-continuous}),
high-level discrete (\emph{hl-discrete}), and high-level ID (\emph{hl-id})
CFEs.
  We formulate single-agent CFE generation methods, where we model the
hl-discrete CFE as a solution to a weighted set cover problem and the
hl-continuous CFE as a solution to an integer linear program. Since these
methods require costly optimization per agent, we propose data-driven CFE
generation approaches that, given instances of agents and their optimal CFEs,
learn a CFE generator that quickly provides optimal CFEs for new agents. This
approach, also viewed as one of learning an optimal policy in a family of large
but deterministic MDPs, considers several problem formulations, including
formulations in which the actions and their effects are unknown, and therefore
addresses informational and computational challenges.
  We conduct extensive empirical evaluations using healthcare datasets (BRFSS,
Foods, and NHANES) and fully-synthetic data. For negatively classified agents
identified by linear or threshold-based classifiers, we compare the high-level
CFE to low-level CFEs and assess the effectiveness of our network-based,
data-driven approaches. Results show that the data-driven CFE generators are
accurate, and resource-efficient, and high-level CFEs offer key advantages over
low-level CFEs.","K Naggita,MR Walter,A Blum- arXiv preprint arXiv:2404.17034, 2024 - arxiv.org",https://arxiv.org/abs/2404.17034,
,A Survey on Efficient Solutions of Large Language Models for Recommendation,Survey-RecSys,,RecSys + LLM (Survey),,,"H Wu,Y Du,Z Sun,T Wei, J Zhang, OY Soon - Authorea Preprints, 2024 - techrxiv.org",https://www.techrxiv.org/doi/full/10.36227/techrxiv.173272687.73188733,
,Feasibility and acceptability of point of care testing for sexually transmitted infections in outpatient clinics offering integrated services in Eswatini,Non-Mamba,,Completely irrelevant,,,"…, N Dube, S Mamba, M Mamba… - Sexually Transmitted …, 2024 - journals.lww.com",https://journals.lww.com/stdjournal/_layouts/15/oaks.journals/downloadpdf.aspx?an=00007435-990000000-00370,
,Reinforcing Long-Term Performance in Recommender Systems with User-Oriented Exploration Policy,Non-Mamba,,RecSys + RL,,,"C Zhang,S Chen,X Zhang,S Dai,W Yu… - Proceedings of the 47th …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3626772.3657714?casa_token=Ev0m8EvZGSwAAAAA:1tZgokwubqAVp6_DAAqRM_T1b51LA4XFFYptVG3xwTAdsPHMQI4ftpxcBIkWSPkMVGugm-dVoqd2OA,
,Using state space grids to quantify and examine dynamics of dyadic conversation,Completely Irrelevant,,Completely irrelevant,,,"M Brinberg,DH Solomon,GD Bodie… - Communication …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/19312458.2024.2413973,
,Reducing black-box nonlinear state-space models: a real-life case study,Completely Irrelevant,,Completely irrelevant,,,"PZ Csurcsia,J Decuyper,B Renczes… - … Systems and Signal …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0888327024001286,
2024년 3월 26일 오후 9:40 (GMT+8),Rotate to scan: Unet-like mamba with triplet ssm module for medical image segmentation,Concrete-Non-RecSys,,Mamba + Vision (Image Segmentation),,"Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.","H Tang, L Cheng, G Huang, Z Tan, J Lu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2403.17701,Cited by 18
2025년 1월 25일 오후 1:30 (GMT+8),PatchRec: Multi-Grained Patching for Efficient LLM-based Sequential Recommendation,Non-Mamba,,"RecSys, Mamba4Rec is used for comparison",,"Large Language Models (LLMs) have emerged as a new paradigm for
recommendation by converting interacted item history into language modeling.
However, constrained by the limited context length of LLMs, existing approaches
have to truncate item history in the prompt, focusing only on recent
interactions and sacrificing the ability to model long-term history. To enable
LLMs to model long histories, we pursue a concise embedding representation for
items and sessions. In the LLM embedding space, we construct an item's
embedding by aggregating its textual token embeddings; similarly, we construct
a session's embedding by aggregating its item embeddings. While efficient, this
way poses two challenges since it ignores the temporal significance of user
interactions and LLMs do not natively interpret our custom embeddings. To
overcome these, we propose PatchRec, a multi-grained patch training method
consisting of two stages: (1) Patch Pre-training, which familiarizes LLMs with
aggregated embeddings -- patches, and (2) Patch Fine-tuning, which enables LLMs
to capture time-aware significance in interaction history. Extensive
experiments show that PatchRec effectively models longer behavior histories
with improved efficiency. This work facilitates the practical use of LLMs for
modeling long behavior histories. Codes are available at
https://github.com/ljy0ustc/PatchRec.","J Liao,R Xie,S Li,X Wang,X Sun,Z Kang… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2501.15087,
,"Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges",Survey-RecSys-Related,,Mamba + Time Series Analysis (Survey),,,"B Narayana Patro, V Srinivas Agneeswaran - arXiv e-prints, 2024 - ui.adsabs.harvard.edu",https://ui.adsabs.harvard.edu/abs/2024arXiv240416112N/abstract,Related articles
2025년 3월 14일 오전 7:58 (GMT+8),OuroMamba: A Data-Free Quantization Framework for Vision Mamba Models,Concrete-Non-RecSys,,Mamba + Vision (Quantization),,"We present OuroMamba, the first data-free post-training quantization (DFQ)
method for vision Mamba-based models (VMMs). We identify two key challenges in
enabling DFQ for VMMs, (1) VMM's recurrent state transitions restricts
capturing of long-range interactions and leads to semantically weak synthetic
data, (2) VMM activations exhibit dynamic outlier variations across time-steps,
rendering existing static PTQ techniques ineffective. To address these
challenges, OuroMamba presents a two-stage framework: (1) OuroMamba-Gen to
generate semantically rich and meaningful synthetic data. It applies
contrastive learning on patch level VMM features generated through neighborhood
interactions in the latent state space, (2) OuroMamba-Quant to employ
mixed-precision quantization with lightweight dynamic outlier detection during
inference. In specific, we present a thresholding based outlier channel
selection strategy for activations that gets updated every time-step. Extensive
experiments across vision and generative tasks show that our data-free
OuroMamba surpasses existing data-driven PTQ techniques, achieving
state-of-the-art performance across diverse quantization settings.
Additionally, we implement efficient GPU kernels to achieve practical latency
speedup of up to 2.36x. Code will be released soon.","A Ramachandran,M Lee,H Xu,S Kundu… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2503.10959,Related articles
2024년 11월 1일 오전 2:58 (GMT+8),NIMBA: Towards Robust and Principled Processing of Point Clouds With SSMs,Concrete-Non-RecSys,,"Max Planck
Mamba + 3d point data + sequence modeling",,"Transformers have become dominant in large-scale deep learning tasks across
various domains, including text, 2D and 3D vision. However, the quadratic
complexity of their attention mechanism limits their efficiency as the sequence
length increases, particularly in high-resolution 3D data such as point clouds.
Recently, state space models (SSMs) like Mamba have emerged as promising
alternatives, offering linear complexity, scalability, and high performance in
long-sequence tasks. The key challenge in the application of SSMs in this
domain lies in reconciling the non-sequential structure of point clouds with
the inherently directional (or bi-directional) order-dependent processing of
recurrent models like Mamba. To achieve this, previous research proposed
reorganizing point clouds along multiple directions or predetermined paths in
3D space, concatenating the results to produce a single 1D sequence capturing
different views. In our work, we introduce a method to convert point clouds
into 1D sequences that maintain 3D spatial structure with no need for data
replication, allowing Mamba sequential processing to be applied effectively in
an almost permutation-invariant manner. In contrast to other works, we found
that our method does not require positional embeddings and allows for shorter
sequence lengths while still achieving state-of-the-art results in ModelNet40
and ScanObjectNN datasets and surpassing Transformer-based models in both
accuracy and efficiency.","N Köprücü,D Okpekpe,A Orvieto- arXiv preprint arXiv:2411.00151, 2024 - arxiv.org",https://arxiv.org/abs/2411.00151,
,Perceptions on circumcision for HIV prevention: an application of the health belief model in a qualitative enquiry among young men in Eswatini,Completely Irrelevant,,,,,"S Mamba, V Holton, SL Huang - AIDS care, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/09540121.2024.2445193,Related articles
,"A comprehensive survey on reinforcement learning-based recommender systems: State-of-the-art, challenges, and future perspectives",Survey-RecSys,,RecSys + RL (Survey),,,"OD Rossiiev,NN Shapovalova… - CEUR Workshop …, 2025 - ceur-ws.org",https://ceur-ws.org/Vol-3917/paper62.pdf,
,Learning the Minimal Representation of a Continuous State-Space Markov Decision Process from Transition Data,Completely Irrelevant,,RL,,,"A Bennouna,D Pachamanova,G Perakis… - Management …, 2024 - pubsonline.informs.org",https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2022.01652,Related articles
,Contrasting Impact of Start State on Performance of A Reinforcement Learning Recommender System,Completely Irrelevant,,RL,,,"S Hassan,M Ayub,M Waqar,T Khan- researchgate.net",https://www.researchgate.net/profile/Ijist-Jr/publication/382084848_Contrasting_Impact_of_Start_State_on_Performance_of_A_Reinforcement_Learning_Recommender_System/links/668ce808af9e615a15d74b3d/Contrasting-Impact-of-Start-State-on-Performance-of-A-Reinforcement-Learning-Recommender-System.pdf,
,Towards healthy and economically sustainable communities through clean water and resource recovery,Completely Irrelevant,,Author’s name is Mamba,,,"BB Mamba- npj Clean Water, 2024 - nature.com",https://www.nature.com/articles/s41545-024-00422-6,
,Combining graph neural network and mamba to capture local and global tissue spatial relationships in whole slide images,Concrete-Non-RecSys,,Mamba + Vision + GNN,,,"R Ding,KD Luong, E Rodriguez, ACAL Da Silva… - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-99042-4,
2024년 9월 25일 오후 12:05 (GMT+8),Semi-LLIE: Semi-supervised Contrastive Learning with Mamba-based Low-light Image Enhancement,Concrete-Non-RecSys,,Mamba + Vision (Image Enhancement),,"Despite the impressive advancements made in recent low-light image
enhancement techniques, the scarcity of paired data has emerged as a
significant obstacle to further advancements. This work proposes a
mean-teacher-based semi-supervised low-light enhancement (Semi-LLIE) framework
that integrates the unpaired data into model training. The mean-teacher
technique is a prominent semi-supervised learning method, successfully adopted
for addressing high-level and low-level vision tasks. However, two primary
issues hinder the naive mean-teacher method from attaining optimal performance
in low-light image enhancement. Firstly, pixel-wise consistency loss is
insufficient for transferring realistic illumination distribution from the
teacher to the student model, which results in color cast in the enhanced
images. Secondly, cutting-edge image enhancement approaches fail to effectively
cooperate with the mean-teacher framework to restore detailed information in
dark areas due to their tendency to overlook modeling structured information
within local regions. To mitigate the above issues, we first introduce a
semantic-aware contrastive loss to faithfully transfer the illumination
distribution, contributing to enhancing images with natural colors. Then, we
design a Mamba-based low-light image enhancement backbone to effectively
enhance Mamba's local region pixel relationship representation ability with a
multi-scale feature learning scheme, facilitating the generation of images with
rich textural details. Further, we propose novel perceptive loss based on the
large-scale vision-language Recognize Anything Model (RAM) to help generate
enhanced images with richer textual details. The experimental results indicate
that our Semi-LLIE surpasses existing methods in both quantitative and
qualitative metrics.","G Li, K Zhang, T Wang, M Li,B Zhao, X Li - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.16604,Cited by 2
,Chemical language modeling with structured state space sequence models,Concrete-Non-RecSys,,Mamba + Chemical Modeling,,,"R Özçelik,S de Ruiter,E Criscuolo,F Grisoni- Nature Communications, 2024 - nature.com",https://www.nature.com/articles/s41467-024-50469-9,
,Personalized Sports Health Recommendation System Assisted by Q-Learning Algorithm,Non-Mamba,,RecSys + RL,,,"Y Yang, Y Zhao - International Journal of Human–Computer …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2295693?casa_token=KauPg7dBfe4AAAAA:O__Z3DoZFU15uGoI22hza34W8TzZowa4Co5P1cWKqQpA1wVP85z-_m47vyG0T00KMNoq4SO6KWFoNDoHgg,
,Toward identity preserving in face sketch-photo synthesis using a hybrid CNN-Mamba framework,Concrete-Non-RecSys,,Mamba + Vision,,,"D Tang,X Jiang, K Wang, W Guo, J Zhang, Y Lin… - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-72066-y,
2024년 10월 17일 오후 9:20 (GMT+8),RemoteDet-Mamba: A Hybrid Mamba-CNN Network for Multi-modal Object Detection in Remote Sensing Images,Concrete-Non-RecSys,,Mamba + Vision,,"Unmanned aerial vehicle (UAV) remote sensing is widely applied in fields such
as emergency response, owing to its advantages of rapid information acquisition
and low cost. However, due to the effects of shooting distance and imaging
mechanisms, the objects in the images present challenges such as small size,
dense distribution, and low inter-class differentiation. To this end, we
propose a multimodal remote sensing detection network that employs a
quad-directional selective scanning fusion strategy called RemoteDet-Mamba.
RemoteDet-Mamba simultaneously facilitates the learning of single-modal local
features and the integration of patch-level global features across modalities,
enhancing the distinguishability for small objects and utilizing local
information to improve discrimination between different classes. Additionally,
the use of Mamba's serial processing significantly increases detection speed.
Experimental results on the DroneVehicle dataset demonstrate the effectiveness
of RemoteDet-Mamba, which achieves superior detection accuracy compared to
state-of-the-art methods while maintaining computational efficiency and
parameter count.","K Ren, X Wu, L Xu, L Wang - arXiv preprint arXiv:2410.13532, 2024 - arxiv.org",https://arxiv.org/abs/2410.13532,Cited by 5
,A sentiment-guided session-aware recommender system,Non-Mamba,,RecSys + RL,,,"P Khurana, B Gupta,R Sharma,P Bedi- The Journal of Supercomputing, 2024 - Springer",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11227-024-06456-4&casa_token=tz5DgJ_NiuoAAAAA:qnnWx-KiUt6JpdU2oIYxLZXDSHZhJniupNAlZZc2Z82JDLFn4PnF6YnIkAi4Gcu-kbPykBT7ZkyJNZgCTA,
,Vision Mamba and xLSTM-UNet for medical image segmentation,Concrete-Non-RecSys,,Mamba + Vision (Image Segmentation),,,"X Zhong, G Lu, H Li - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-88967-5,
,Multi-Grained Patch Training for Efficient LLM-based Recommendation,Non-Mamba,,"RecSys + LLM, Mamba is used as a comparison model",,,"J Liao,R Xie, S Li, X Wang,X Sun,Z Kang,X He- 2025 - hexiangnan.github.io",https://hexiangnan.github.io/papers/sigir25-PatchRec.pdf,
,Reinforcement learning-based recommender systems with large language models for state reward and action modeling,Non-Mamba,,RecSys + RL,,,"J Wang,A Karatzoglou,I Arapakis… - Proceedings of the 47th …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3626772.3657767,
2025년 2월 19일 오후 4:21 (GMT+8),MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis,Concrete-Non-RecSys,,Mamba + Vision,,"Efficient evaluation of three-dimensional (3D) medical images is crucial for
diagnostic and therapeutic practices in healthcare. Recent years have seen a
substantial uptake in applying deep learning and computer vision to analyse and
interpret medical images. Traditional approaches, such as convolutional neural
networks (CNNs) and vision transformers (ViTs), face significant computational
challenges, prompting the need for architectural advancements. Recent efforts
have led to the introduction of novel architectures like the ``Mamba'' model as
alternative solutions to traditional CNNs or ViTs. The Mamba model excels in
the linear processing of one-dimensional data with low computational demands.
However, Mamba's potential for 3D medical image analysis remains underexplored
and could face significant computational challenges as the dimension increases.
This manuscript presents MobileViM, a streamlined architecture for efficient
segmentation of 3D medical images. In the MobileViM network, we invent a new
dimension-independent mechanism and a dual-direction traversing approach to
incorporate with a vision-Mamba-based framework. MobileViM also features a
cross-scale bridging technique to improve efficiency and accuracy across
various medical imaging modalities. With these enhancements, MobileViM achieves
segmentation speeds exceeding 90 frames per second (FPS) on a single graphics
processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster
than the state-of-the-art deep learning models for processing 3D images with
the same computational resources. In addition, experimental evaluations
demonstrate that MobileViM delivers superior performance, with Dice similarity
scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,
ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses
existing models.","W Dai, J Liu - arXiv preprint arXiv:2502.13524, 2025 - arxiv.org",https://arxiv.org/abs/2502.13524,Related articles
2024년 8월 16일 오후 3:33 (GMT+8),Grassnet: State space model meets graph neural network,Concrete-RecSys-Related,,Mamba + GNN,,"Designing spectral convolutional networks is a formidable task in graph
learning. In traditional spectral graph neural networks (GNNs),
polynomial-based methods are commonly used to design filters via the Laplacian
matrix. In practical applications, however, these polynomial methods encounter
inherent limitations, which primarily arise from the the low-order truncation
of polynomial filters and the lack of overall modeling of the graph spectrum.
This leads to poor performance of existing spectral approaches on real-world
graph data, especially when the spectrum is highly concentrated or contains
many numerically identical values, as they tend to apply the exact same
modulation to signals with the same frequencies. To overcome these issues, in
this paper, we propose Graph State Space Network (GrassNet), a novel graph
neural network with theoretical support that provides a simple yet effective
scheme for designing and learning arbitrary graph spectral filters. In
particular, our GrassNet introduces structured state space models (SSMs) to
model the correlations of graph signals at different frequencies and derives a
unique rectification for each frequency in the graph spectrum. To the best of
our knowledge, our work is the first to employ SSMs for the design of GNN
spectral filters, and it theoretically offers greater expressive power compared
with polynomial filters. Extensive experiments on nine public benchmarks reveal
that GrassNet achieves superior performance in real-world graph modeling tasks.","G Zhao,T Wang,Y Jin,C Lang, Y Li,H Ling- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.08583,
,Proper account of long-term correlations in the observations improves state-space models' performances,Non-Mamba,,Classic MDP,,,"N Bez,P Gloaguen,MP Etienne, R Joo,S Lanco… - 2024 - hal.science",https://hal.science/hal-04547315/,
2024년 3월 7일 오전 12:49 (GMT+8),Medmamba: Vision mamba for medical image classification,Concrete-Non-RecSys,,Mamba + Vision (Medical Image Segmentation),,"Since the era of deep learning, convolutional neural networks (CNNs) and
vision transformers (ViTs) have been extensively studied and widely used in
medical image classification tasks. Unfortunately, CNN's limitations in
modeling long-range dependencies result in poor classification performances. In
contrast, ViTs are hampered by the quadratic computational complexity of their
self-attention mechanism, making them difficult to deploy in real-world
settings with limited computational resources. Recent studies have shown that
state space models (SSMs) represented by Mamba can effectively model long-range
dependencies while maintaining linear computational complexity. Inspired by it,
we proposed MedMamba, the first Vision Mamba for generalized medical image
classification. Concretely, we introduced a novel hybrid basic block named
SS-Conv-SSM, which purely integrates the convolutional layers for extracting
local features with the abilities of SSM to capture long-range dependencies,
aiming to model medical images from different image modalities efficiently. By
employing the grouped convolution strategy and channel-shuffle operation,
MedMamba successfully provides fewer model parameters and a lower computational
burden for efficient applications without sacrificing accuracy. We thoroughly
evaluated MedMamba using 16 datasets containing ten imaging modalities and
411,007 images. Experimental results show that MedMamba demonstrates
competitive performance on most tasks compared with the state-of-the-art
methods. This work aims to explore the potential of Vision Mamba and establish
a new baseline for medical image classification, thereby providing valuable
insights for developing more powerful Mamba-based artificial intelligence
algorithms and applications in medicine. The source codes and all pre-trained
weights of MedMamba are available at https://github.com/YubiaoYue/MedMamba.","Y Yue,Z Li- arXiv preprint arXiv:2403.03849, 2024 - arxiv.org",https://arxiv.org/abs/2403.03849,
2025년 5월 26일 오후 4:24 (GMT+8),Burst Image Super-Resolution via Multi-Cross Attention Encoding and Multi-Scan State-Space Decoding,Concrete-Non-RecSys,,Mamba + Vision (SR),,"Multi-image super-resolution (MISR) can achieve higher image quality than
single-image super-resolution (SISR) by aggregating sub-pixel information from
multiple spatially shifted frames. Among MISR tasks, burst super-resolution
(BurstSR) has gained significant attention due to its wide range of
applications. Recent methods have increasingly adopted Transformers over
convolutional neural networks (CNNs) in super-resolution tasks, due to their
superior ability to capture both local and global context. However, most
existing approaches still rely on fixed and narrow attention windows that
restrict the perception of features beyond the local field. This limitation
hampers alignment and feature aggregation, both of which are crucial for
high-quality super-resolution. To address these limitations, we propose a novel
feature extractor that incorporates two newly designed attention mechanisms:
overlapping cross-window attention and cross-frame attention, enabling more
precise and efficient extraction of sub-pixel information across multiple
frames. Furthermore, we introduce a Multi-scan State-Space Module with the
cross-frame attention mechanism to enhance feature aggregation. Extensive
experiments on both synthetic and real-world benchmarks demonstrate the
superiority of our approach. Additional evaluations on ISO 12233 resolution
test charts further confirm its enhanced super-resolution performance.","T Huang, Y Zhang, T Li, Y Qu, F Liu, Z Wei - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2505.19668,Related articles
2024년 7월 16일 오전 1:23 (GMT+8),OPa-Ma: Text Guided Mamba for 360-degree Image Out-painting,Concrete-Non-RecSys,,Mamba + Vision,,"In this paper, we tackle the recently popular topic of generating 360-degree
images given the conventional narrow field of view (NFoV) images that could be
taken from a single camera or cellphone. This task aims to predict the
reasonable and consistent surroundings from the NFoV images. Existing methods
for feature extraction and fusion, often built with transformer-based
architectures, incur substantial memory usage and computational expense. They
also have limitations in maintaining visual continuity across the entire
360-degree images, which could cause inconsistent texture and style generation.
To solve the aforementioned issues, we propose a novel text-guided out-painting
framework equipped with a State-Space Model called Mamba to utilize its
long-sequence modelling and spatial continuity. Furthermore, incorporating
textual information is an effective strategy for guiding image generation,
enriching the process with detailed context and increasing diversity.
Efficiently extracting textual features and integrating them with image
attributes presents a significant challenge for 360-degree image out-painting.
To address this, we develop two modules, Visual-textual Consistency Refiner
(VCR) and Global-local Mamba Adapter (GMA). VCR enhances contextual richness by
fusing the modified text features with the image features, while GMA provides
adaptive state-selective conditions by capturing the information flow from
global to local representations. Our proposed method achieves state-of-the-art
performance with extensive experiments on two broadly used 360-degree image
datasets, including indoor and outdoor settings.","P Gao,K Yao,T Ye, S Wang, Y Yao,X Wang- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2407.10923,Cited by 4
,Detection and classification of Shiitake mushroom fruiting bodies based on Mamba YOLO,Concrete-Non-RecSys,,Mamba + Vision,,,"K Qi, Z Yang, Y Fan, H Song, Z Liang, S Wang… - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-00133-z,
,VMKLA-UNet: vision Mamba with KAN linear attention U-Net,Concrete-Non-RecSys,,Mamba + Vision,,,"C Su, X Luo, S Li, L Chen, J Wang - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-97397-2,
,"Recovery of values from low‐grade and complex minerals: development of sustainable processes: by Elvis Fosso-Kankeu, Bhekie B. Mamba, and Antoine F. Mulaba …",Completely Irrelevant,,Author’s name is Mamba,,,W Firmansyah- 2024 - Taylor & Francis,https://www.tandfonline.com/doi/full/10.1080/00206814.2024.2384075?casa_token=BK7GTBbjKW4AAAAA:YRVMRkG1WjcJgpoPHWuZNbVKQ8KyvZXmnTX9as4_nttAQ8TZ1qx-ztki0wupuavwTzkWFbSYLQRqKnzOrg&casa_token=AnosRED53qYAAAAA:xCnXQouZOKC4EcIiy6u-79igMfJHXFW5ZAlMyFfOTrjdqaWriWFLDh4s7DCYkj8TMCOQa_xCx5YAWH_KIw,
,The removal of pathogenic bacteria and dissolved organic matter from freshwater using microporous membranes: insights into biofilm formation and fouling …,Completely Irrelevant,,Author’s name is Mamba,,,"PP Mamba,TAM Msagati,BB Mamba, MM Motsa… - Biofouling, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/08927014.2024.2339438,
,Estimation and inference in games of incomplete information with unobserved heterogeneity and large state space,Completely Irrelevant,,Research paper in economics,,,"Y Fan, S Jiang,X Shi- Quantitative Economics, 2024 - Wiley Online Library",https://onlinelibrary.wiley.com/doi/abs/10.3982/QE2169,
,A mixed Mamba U-net for prostate segmentation in MR images,Concrete-Non-RecSys,,Mamba + Vision (Medical Image Segmentation),,,"Q Du, L Wang, H Chen - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-71045-7,
,DRL-HIFA: a dynamic recommendation system with deep reinforcement learning based Hidden Markov Weight Updation and factor analysis,Non-Mamba,,RecSys + RL,,,"GK Shyam- Multimedia Tools and Applications, 2024 - Springer",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11042-024-18296-8&casa_token=VwRUb6aBVqoAAAAA:SNPKw_CIAbSijjotI_eo6ccnD83y5LoOq1Ec1yeUNeQ7YO535I8X7wEC_nhJ2oI6PfxByty1DwQwYvoUpQ,
,Spatio-temporal degradation model with graph neural network and structured state space model for remaining useful life prediction,Concrete-Non-RecSys,,Mamba + Remaining Useful Life Prediction,,,"X Wu, Z Liu,L Wang- Reliability Engineering & System Safety, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S095183202400841X,Cited by 5
,Koopman-Constrained Hierarchical Deep State Space Model for Industrial Quality Prediction via Cloud–Edge Collaborative Framework,Completely Irrelevant,,Cloud Computing + RL,,,"Q Sui, Y Wang,C Liu,M Han… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10765125/,Cited by 7
,Chelation-directed interface engineering of in-place self-cleaning membranes,Completely Irrelevant,,Research paper in biology,,,"…, L Yan, J Guan, Y Wen, Y Bai,BB Mamba… - Proceedings of the …, 2024 - pnas.org",https://www.pnas.org/doi/abs/10.1073/pnas.2319390121,
,Reinforcement Learning Architecture for Facial Skin Treatment Recommender,Non-Mamba,,RecSys + RL,,,"JK Jin, K Dajani, M Kim,SD Kim… - 2024 IEEE/ACIS 22nd …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10685645/,Related articles
,"Precarious Modernities: Assembling State, Space and Society on the Urban Margins in Morocco: by Cristiana Strava, London, Zed Books, 2022, xi+ 203 pp.",Completely Irrelevant,,"“State, space” is in title",,,CN Silva- 2024 - Taylor & Francis,https://www.tandfonline.com/doi/full/10.1080/00083968.2024.2408977?casa_token=HFMov8mQZdYAAAAA:QWbs0x_zkIuT9i78kEkZp0xSTqzU6uFyeTdvR_mWJ5vUgZf5LEfbaYZW8AgPWhj6-Khxn_Ctswk9nBdOsg&casa_token=rTuPR17JpvoAAAAA:Bqhe0BfKFGCpdp1isHT9kcfsqpWXMX39NyKzs6jlY23WJj2tQeel_OHVXy-mqAzxv1orjG_oDyi3_OnK-g,
,(Un)rooted spaces of belonging: migrant kinship structures in Nadifa Mohamed'sBlack Mamba Boy,Completely Irrelevant,,Mentions literature named “black mamba boy”,,,"C Nwachukwu - Journal of the African Literature Association, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/21674736.2024.2379630?casa_token=z3FtnvPbelUAAAAA:7FCUo7OBmgUmCiZ0NS8a2QL1M_j7ID4UkFMMgTldzgzCPd8rcWtHX_9lYnwjoYA1CgvzQlC4HBh6Ldcm3w,
,UVMSR: a novel approach to hyperspectral image super-resolution by fusing U-Net and Mamba,Concrete-Non-RecSys,,Mamba + Vision (SR),,,"T Tang, W Yan, G Bai, X Pan, J Liu - International Journal of …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/01431161.2024.2443619?casa_token=_Qoy9e5N3AsAAAAA:_8RPvW73MF0I6N7fsWgQHTHkZg7TsrTvrGMa8HLbvkQqHVL3FvLPyVrmKUb-kVw-N36AGXWYa95sIOvtBg,
,"Joint Content Caching, Recommendation and Transmission for Layered Scalable Videos Over Dynamic Cellular Networks: A Dueling Deep Q-Learning Approach",Non-Mamba,,RecSys + RL,,,"J Xie, Q Jia, X Mu, F Lu - IEEE Access, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10463054/,
,Notation for mass parallel algorithms: computing Petri net state space on GPU case study,Completely Irrelevant,,“petri net state space” is mentioned in context of algorithm theory,,,"DA Zaitsev, Z Zhang, D Liu… - International Journal of …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/17445760.2024.2431545,
,Optimising TinyML with quantization and distillation of transformer and mamba models for indoor localisation on edge devices,General Mamba,,Edge Intelligence + Mamba,,,"T Suwannaphong,F Jovan,I Craddock… - Scientific Reports, 2025 - nature.com",https://www.nature.com/articles/s41598-025-94205-9,
,Deep Reinforcement Learning Recommendation System Algorithm Based on Multi-Level Attention Mechanisms,Non-Mamba,,RecSys + RL,,,"G Wang, J Ding, F Hu - Electronics, 2024 - search.proquest.com",https://search.proquest.com/openview/f70242c793538fa3acf60562f913bac1/1?pq-origsite=gscholar&cbl=2032404&casa_token=7AmkW6EvgIEAAAAA:cus4dtCr-gPkNopjYl6iq4-dNIol564d5jlTIGPXDGhWycvdF7mm1uP4jA69aosDtuyrVQNOErU,
,State-space modelling using wastewater virus and epidemiological data to estimate reported COVID-19 cases and the potential infection numbers,Completely Irrelevant,,,,,"S Kadoya, Y Li, Y Wang… - Journal of the Royal …, 2025 - royalsocietypublishing.org",https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2024.0456?casa_token=jksLu-UmWwcAAAAA:FRKkySNQDAX03Dcokqi3b73pIuPi6XyKmRNqmQ5HwC5P2jLH4snfF3KwvIJTOCLINFrhhscFXRPetwo,
,Using state space grids for analysing teacher–student interaction in an intervention: The complex dynamics of teaching for musical creativity,Completely Irrelevant,,,,,"L Hendriks,E Kupers,H Steenbeek… - Classroom …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/19463014.2024.2360421,
2024년 3월 25일 오전 7:50 (GMT+8),Modeling analog dynamic range compressors using deep learning and state-space models,Concrete-Non-RecSys,,Mamba + Virtual Analog Modeling(Audio Modeling),,"We describe a novel approach for developing realistic digital models of
dynamic range compressors for digital audio production by analyzing their
analog prototypes. While realistic digital dynamic compressors are potentially
useful for many applications, the design process is challenging because the
compressors operate nonlinearly over long time scales. Our approach is based on
the structured state space sequence model (S4), as implementing the state-space
model (SSM) has proven to be efficient at learning long-range dependencies and
is promising for modeling dynamic range compressors. We present in this paper a
deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic
range compressor. The model is causal, executes efficiently in real time, and
achieves roughly the same quality as previous deep-learning models but with
fewer parameters.","H Yin, G Cheng,CJ Steinmetz,R Yuan… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2403.16331,
,A barking emotion recognition method based on Mamba and Synchrosqueezing Short-Time Fourier Transform,Concrete-Non-RecSys,,Mamba + Audio Modeling,,,"C Yang, S Hu, L Tang, R Deng,G Zhou, J Yi… - Expert Systems with …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417424020803,Cited by 6
,EGCM-UNet: Edge Guided Hybrid CNN-Mamba UNet for farmland remote sensing image semantic segmentation,Concrete-Non-RecSys,,Mamba + Vision (Image Segmentation),,,"J Zheng, Y Fu, X Chen, R Zhao, J Lu, H Zhao… - Geocarto …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/10106049.2024.2440407,
,Data and Knowledge-Driven Dual-State Augmented Reinforcement Learning for Sequential Recommendation,Non-Mamba,,RecSys + RL,,,"Z Meng, X Tong, Q Zhang, X Cheng - Available at SSRN 4987653 - papers.ssrn.com",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4987653,
,Adsorption of lead and chromium ions from electroplating wastewater using plantain stalk modified by amorphous alumina developed from waste cans,Completely Irrelevant,,Author’s name is Mamba,,,"EO Ajala, MO Aliyu,MA Ajala,G Mamba, AM Ndana… - Scientific reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-56183-2,
,Enhancing parcel singulation efficiency through transformer-based position attention and state space augmentation,Non-Mamba,,RL,,,"J Shen, H Lu, S Lyu,Y Lu- Expert Systems with Applications, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417424002586,
,Deep Q-Learning with Whittle Index for Contextual Restless Bandits: Application to Email Recommender Systems,Non-Mamba,,RecSys + RL,,,"I El Mimouni,K Avrachenkov- Northern Lights Deep Learning …, 2025 - openreview.net",https://openreview.net/forum?id=alnaQJdBNs,
,UU-Mamba: uncertainty-aware u-mamba for cardiac image segmentation,Concrete-Non-RecSys,,Mamba + Vision (Image Segmentation),,,"TY Tsai,L Lin,S Hu,MC Chang… - 2024 IEEE 7th …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10708025/,
2024년 5월 24일 오후 10:04 (GMT+8),Meteor: Mamba-based traversal of rationale for large language and vision models,Concrete-Non-RecSys,,Mamba + LLVM (Large Language and Vision Model),,"The rapid development of large language and vision models (LLVMs) has been
driven by advances in visual instruction tuning. Recently, open-source LLVMs
have curated high-quality visual instruction tuning datasets and utilized
additional vision encoders or multiple computer vision models in order to
narrow the performance gap with powerful closed-source LLVMs. These
advancements are attributed to multifaceted information required for diverse
capabilities, including fundamental image understanding, real-world knowledge
about common-sense and non-object concepts (e.g., charts, diagrams, symbols,
signs, and math problems), and step-by-step procedures for solving complex
questions. Drawing from the multifaceted information, we present a new
efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages
multifaceted rationale to enhance understanding and answering capabilities. To
embed lengthy rationales containing abundant information, we employ the Mamba
architecture, capable of processing sequential data with linear time
complexity. We introduce a new concept of traversal of rationale that
facilitates efficient embedding of rationale. Subsequently, the backbone
multimodal language model (MLM) is trained to generate answers with the aid of
rationale. Through these steps, Meteor achieves significant improvements in
vision language performances across multiple evaluation benchmarks requiring
diverse capabilities, without scaling up the model size or employing additional
vision encoders and computer vision models.","BK Lee,CW Kim,B Park,YM Ro- arXiv preprint arXiv:2405.15574, 2024 - arxiv.org",https://arxiv.org/abs/2405.15574,
,Exploring temperature and precipitation changes under future climate change scenarios for black and white rhinoceros populations in Southern Africa,Completely Irrelevant,,Author’s name is Mamba,,,"HS Mamba,TO Randhir- Biodiversity, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/14888386.2023.2291133?casa_token=19jw8-Sb9LcAAAAA:e5rqm_cDelQtUSogYpktrTwijtxTub2MeJxeO_YA76L84UhcJ9Dahv7pGFofIOToN7cYnqxGP2SuQIOQBg,
2024년 9월 4일 오후 11:35 (GMT+8),Obsidian: Cooperative state-space exploration for performant inference on secure ml accelerators,Completely Irrelevant,,,,"Trusted execution environments (TEEs) for machine learning accelerators are
indispensable in secure and efficient ML inference. Optimizing workloads
through state-space exploration for the accelerator architectures improves
performance and energy consumption. However, such explorations are expensive
and slow due to the large search space. Current research has to use fast
analytical models that forego critical hardware details and cross-layer
opportunities unique to the hardware security primitives. While cycle-accurate
models can theoretically reach better designs, their high runtime cost
restricts them to a smaller state space.
  We present Obsidian, an optimization framework for finding the optimal
mapping from ML kernels to a secure ML accelerator. Obsidian addresses the
above challenge by exploring the state space using analytical and
cycle-accurate models cooperatively. The two main exploration components
include: (1) A secure accelerator analytical model, that includes the effect of
secure hardware while traversing the large mapping state space and produce the
best m model mappings; (2) A compiler profiling step on a cycle-accurate model,
that captures runtime bottlenecks to further improve execution runtime, energy
and resource utilization and find the optimal model mapping.
  We compare our results to a baseline secure accelerator, comprising of the
state-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11].
The analytical model reduces the inference latency by 20.5% for a cloud and
8.4% for an edge deployment with an energy improvement of 24% and 19%
respectively. The cycle-accurate model, further reduces the latency by 9.1% for
a cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%.","S Banerjee,S Wei,P Ramrakhyani,M Tiwari- arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.02817,
,Quantum Representation based Preference Evolution Network for E-commerce recommendation,Non-Mamba,,"RecSys + Quantum Mechanics
와 지린다",,,"P Wang, H Cao, P Li, Y Wang, Y Chu,T Liao… - Physica A: Statistical …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0378437124006642,Cited by 2
,Adaptive estimation of the Gutenberg–Richterbvalue using a state space model and particle filtering,Completely Irrelevant,,RL + Sequence Modeling (Earthquake Prediction),,,"D Iwata,KZ Nanjo- Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-54576-x,
,Stability analysis of ND discrete state–space systems,Completely Irrelevant,,Classic SSM,,,"A Kanellakis,A Tawfik,P Agathoklis- Systems Science & Control …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/21642583.2024.2413554,
2025년 2월 7일 오후 10:22 (GMT+8),SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model,Completely Irrelevant,,SSM + LoRA,,"Fine-tuning is a key approach for adapting language models to specific
downstream tasks, but updating all model parameters becomes impractical as
model sizes increase. Parameter-Efficient Fine-Tuning (PEFT) methods, such as
Low-Rank Adaptation (LoRA), address this challenge by introducing additional
adaptation parameters into pre-trained weight matrices. However, LoRA's
performance varies across different insertion points within the model,
highlighting potential parameter inefficiency due to unnecessary insertions. To
this end, we propose SSMLoRA (State Space Model Low-Rank Adaptation), an
extension of LoRA that incorporates a State Space Model (SSM) to interconnect
low-rank matrices. SSMLoRA ensures that performance is maintained even with
sparser insertions. SSMLoRA allows the model to not only map inputs to a
low-rank space for better feature extraction but also leverage the computations
from the previous low-rank space. Our method achieves comparable performance to
LoRA on the General Language Understanding Evaluation (GLUE) benchmark while
using only half the parameters. Additionally, due to its structure, SSMLoRA
shows promise in handling tasks with longer input sequences. .You can find our
code here:https://github.com/yuhkalhic/SSMLoRA.","J Yu, Y Zhang, B Wang,P Lin,Y Liu,S Feng- arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2502.04958,
,"Integrating state-space modeling, parameter estimation, deep learning, and docking techniques in drug repurposing: A case study on COVID-19 cytokine storm",Completely Irrelevant,,SSM + DL + Chemical Modeling,,,"A Bakshi,K Gangopadhyay,S Basak… - Journal of the …, 2025 - academic.oup.com",https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocaf035/8020798,
,Stability robust for fractional generalized multi-dimensional state–space models,Completely Irrelevant,,SSM,,,"S Salmi, D Bouagada - International Journal of Computer …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/00207160.2024.2339243?casa_token=6LjDkZ3YCrMAAAAA:wjFlHtF2XBTLzITw2L7gz7Ji-12004JS-q1p0DfkIeS6gkVxHlvweZW3IoGLk-jASpvGIE7jBcHfIhrMmA&casa_token=mYZPR6ll5i0AAAAA:nH52OakuOYPecLZ32FpbCzG_LrDoONDp70j1TJYVFqex-x6D-KbJIeDoICJ35g9amIOZjuBtD0W7e4FxWQ,
,Fine Tuning Out-of-Vocabulary Item Recommendation with User Sequence Imagination,Non-Mamba,,RecSys,,,"R Liu,H Chen, Y Bei,Q Shen… - Advances in …, 2024 - proceedings.neurips.cc",https://proceedings.neurips.cc/paper_files/paper/2024/hash/10d52f5d2ef0f69ac10da7c962fb6db9-Abstract-Conference.html,
,Efficient Importance Variational Approximations for State Space Models,Completely Irrelevant,,SSM,,,"R Loaiza-Maya,D Nibbering- Journal of Business & Economic …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/07350015.2024.2429468,
,State space model identification using model reference adaptive approach: software and hardware-in-the-loop simulation,Completely Irrelevant,,SSM,,,"VT Duong,CT Truong, TT Nguyen, HH Nguyen… - Cogent …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/23311916.2024.2434938,
,State space mixture modeling: Finding people with similar patterns of change,Completely Irrelevant,,SSM,,,"MD Hunter- Multivariate Behavioral Research, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/00273171.2023.2261224?casa_token=WBac70obP40AAAAA:L9gOmiZP2ms1pBK93pGa6PwBE0JuKOazQuLJVJwL82XOAomSdF82iKklkBicRq4WSlfuI8eSyZJhgmDRgA,
,Application of Power System Intelligent Recommendation Algorithm in Improving Human-computer Interactive Experience Model,Non-Mamba,,RecSys,,,"K Xu, J Zhang, H Huang, J Ni, B Cui… - … on Electronics and …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10834905/,Related articles
,Quantifying spontaneous infant movements using state-space models,Completely Irrelevant,,SSM,,,"E Passmore, AKL Kwong, JE Olsen, AL Eeles… - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-80202-x,
,Curriculum Generation for Learning Guiding Functions in State-Space Search Algorithms,Completely Irrelevant,,SSM,,,"S Pendurkar,LHS Lelis,NR Sturtevant… - Proceedings of the …, 2024 - ojs.aaai.org",https://ojs.aaai.org/index.php/SOCS/article/view/31546,
,A deep clustering-based state-space model for improved disease risk prediction in personalized healthcare,Completely Irrelevant,,SSM,,,"S Niu,J Ma,Q Yin,L Bai, C Li,X Yang- Annals of Operations Research, 2024 - Springer",https://link.springer.com/article/10.1007/s10479-023-05817-1,
,Efficient TinyML architectures for on-device small language models: Privacy-preserving inference at the edge,Completely Irrelevant,,Edge Intelligence + Language Modeling,,,"M Pujari,A Goel, AK Pakina - International Journal Science and …, 2024 - journal.admi.or.id",https://journal.admi.or.id/index.php/IJST/article/view/1958,
,Relational Stock Selection via Probabilistic State Space Learning,Completely Irrelevant,,SSM + Stock Selection,,,"Q Gao, Z Liu, L Huang, K Zhang… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10771680/,
2024년 1월 17일 오후 4:01 (GMT+8),UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems,Non-Mamba,,RecSys + RL,,"Reinforcement learning (RL) has gained traction for enhancing user long-term
experiences in recommender systems by effectively exploring users' interests.
However, modern recommender systems exhibit distinct user behavioral patterns
among tens of millions of items, which increases the difficulty of exploration.
For example, user behaviors with different activity levels require varying
intensity of exploration, while previous studies often overlook this aspect and
apply a uniform exploration strategy to all users, which ultimately hurts user
experiences in the long run. To address these challenges, we propose
User-Oriented Exploration Policy (UOEP), a novel approach facilitating
fine-grained exploration among user groups. We first construct a distributional
critic which allows policy optimization under varying quantile levels of
cumulative reward feedbacks from users, representing user groups with varying
activity levels. Guided by this critic, we devise a population of distinct
actors aimed at effective and fine-grained exploration within its respective
user group. To simultaneously enhance diversity and stability during the
exploration process, we further introduce a population-level diversity
regularization term and a supervision module. Experimental results on public
recommendation datasets demonstrate that our approach outperforms all other
baselines in terms of long-term performance, validating its user-oriented
exploration effectiveness. Meanwhile, further analyses reveal our approach's
benefits of improved performance for low-activity users as well as increased
fairness among users.","C Zhang,S Chen,X Zhang,S Dai,W Yu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2401.09034,
,Novel Device Placement Approach with Neighbor Effect Aware Graph Mamba Networks,Concrete-RecSys-Related,,Mamba + GNN,,,"H Shu, W Hao, M Han, F Li - International Conference on Brain Inspired …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-981-96-2882-7_27,Related articles
,Reinforcement learning for addressing the cold-user problem in recommender systems,Non-Mamba,,RecSys + RL,,,"S Giannikis,F Frasincar,D Boekestijn- Knowledge-Based Systems, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0950705124003873,
,Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation,Non-Mamba,,RecSys,,,"X Chen,S Wang,L Yao- Proceedings of the 30th ACM SIGKDD …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3637528.3671750,
,Interpreting Shift Encoders as State Space models for Stationary Time Series,Completely Irrelevant,,SSM,,,P Donkoh - 2024 - search.proquest.com,https://search.proquest.com/openview/b301b26b22e695d0a62c08023a9c9c91/1?pq-origsite=gscholar&cbl=18750&diss=y,
,Adaptive centre-frequency bandpass state-space digital filters and theirl2-sensitivity analysis,Completely Irrelevant,,SSM,,,"Y Hinamoto, A Doi - SICE Journal of Control, Measurement, and …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/18824889.2024.2445373,
,Scalable Personalised Treatment Recommendation in Critical Care,Completely Irrelevant,,RL + Medical,,,"CP Utomo,K Ichikawa, N Insani… - 2024 8th …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10730747/,Related articles
2024년 9월 22일 오전 11:22 (GMT+8),UU-Mamba: Uncertainty-aware U-Mamba for Cardiovascular Segmentation,Concrete-Non-RecSys,,Mamba + Vision (Medical Image Segmentation),,"Building on the success of deep learning models in cardiovascular structure
segmentation, increasing attention has been focused on improving generalization
and robustness, particularly in small, annotated datasets. Despite recent
advancements, current approaches often face challenges such as overfitting and
accuracy limitations, largely due to their reliance on large datasets and
narrow optimization techniques. This paper introduces the UU-Mamba model, an
extension of the U-Mamba architecture, designed to address these challenges in
both cardiac and vascular segmentation. By incorporating Sharpness-Aware
Minimization (SAM), the model enhances generalization by targeting flatter
minima in the loss landscape. Additionally, we propose an uncertainty-aware
loss function that combines region-based, distribution-based, and pixel-based
components to improve segmentation accuracy by capturing both local and global
features. While the UU-Mamba model has already demonstrated great performance,
further testing is required to fully assess its generalization and robustness.
We expand our evaluation by conducting new trials on the ImageCAS (coronary
artery) and Aorta (aortic branches and zones) datasets, which present more
complex segmentation challenges than the ACDC dataset (left and right
ventricles) used in our previous work, showcasing the model's adaptability and
resilience. We confirm UU-Mamba's superior performance over leading models such
as TransUNet, Swin-Unet, nnUNet, and nnFormer. Moreover, we provide a more
comprehensive evaluation of the model's robustness and segmentation accuracy,
as demonstrated by extensive experiments.","TY Tsai,L Lin,S Hu, CW Tsao, X Li,MC Chang… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.14305,
,Heterogeneous Multivariate Functional Time Series Modeling: A State Space Approach,Completely Irrelevant,,SSM + Sequence Modeling,,,"P Liu, J Lin,C Zhang- IEEE Transactions on Knowledge and …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10713887/,Cited by 1
,A Hyperheuristic and Reinforcement Learning Guided Meta-heuristic Algorithm Recommendation,Completely Irrelevant,,RL,,,"N Zhu, F Zhao, J Cao - 2024 27th International Conference on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10580058/,Cited by 2
,Selective blockade of Cav1.2 (α1C) versus Cav1.3 (α1D) L-type calcium channels by the black mamba toxin calciseptine,Completely Irrelevant,,biology,,,"P Mesirca,J Chemin, C Barrère, E Torre… - Nature …, 2024 - nature.com",https://www.nature.com/articles/s41467-023-43502-w,
,Test-time adaptation with state-space models,Completely Irrelevant,,SSM,,,"M Schirmer,D Zhang,E Nalisnick- ICML 2024 Workshop on …, 2024 - openreview.net",https://openreview.net/forum?id=aNitTRBw70,
,Individualized survival predictions using state space model with longitudinal and survival data,Completely Irrelevant,,SSM,,,"M Cauchi,AR Mills,A Lawrie… - Journal of the …, 2024 - royalsocietypublishing.org",https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2023.0682,
,Stock Recommendation Model with Investor Risk Acceptance.,Completely Irrelevant,,RL + Finance,,,"HEIC WANG, YIH CHENG… - Journal of Information …, 2024 - search.ebscohost.com",https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=10162364&AN=178433503&h=MWoTwDghdCcsPywR%2BrML%2BkIeDC14p%2FMZgGu7JbA%2BclvIP6ZO9p7torhTb10KxV7kx8E%2FOx0rkfMercpLbDVF4g%3D%3D&crl=c,Cited by 2
,Operating minimally intelligent agent-based manufacturing systems across the Average demand Interval–coefficient of variation (ADI-CV) demand state space,Completely Irrelevant,,,,,"Z Neu,B Hicks,J Gopsill- Production & Manufacturing Research, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/21693277.2024.2323479,
,Application of Poisson state-space models and shared frailty models for multistage processes surveillance,Completely Irrelevant,,SSM,,,"F Ebrahimi,F Sogandi- Quality Technology & Quantitative …, 2025 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/16843703.2024.2314819?casa_token=pxL1k1LkIi8AAAAA:pKrVOmJnEi37U9Cm6Ky1y6sBN0eJq_S3GmQ_6zKgixYLNeWl9b9vNUkRf5wMZuVtTYh8FF6WS-d04cKRbg,
,Computational and mathematical biophysics: Markov modeling on dynamic state space for genetic disorders and infectious diseases with mutations: Probabilistic …,Completely Irrelevant,,SSM,,,"MD Baranon,PGO Weke,J Alladatin,BM Ale- 2024 - digital.zlb.de",https://digital.zlb.de/viewer/metadata/1330616685/,Related articles
,Fall-Mamba: A Multimodal Fusion and Masked Mamba-based Approach for Fall Detection,Concrete-Non-RecSys,,Mamba + Fall detection,,,"X Zhang, Q Xu, F Feng, X Lu… - IEEE Internet of Things …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10833684/,Cited by 1
,Content Update via Deep Reinforcement Learning in Recommendation-Aware Edge Caching Network,Non-Mamba,,RecSys + RL,,,"Z Qiao, Q Yu - 2024 9th International Conference on Intelligent …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10743589/,Related articles
,The effect of process variability and data quality on performance of a state-space stock assessment model,Non-Mamba,,SSM + Stock Prediction,,,"EM Liljestrand,JR Bence, JJ Deroba - Fisheries Research, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0165783624000870,
,GraphDRL: GNN-based deep reinforcement learning for interactive recommendation with sparse data,Non-Mamba,,RecSys + RL + GNN,,,"W Li, X Song,Y Tu- Expert Systems with Applications, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417425004543,Cited by 2
2024년 7월 17일 오후 7:18 (GMT+8),Temporal Test-Time Adaptation with State-Space Models,Completely Irrelevant,,,,"Distribution shifts between training and test data are inevitable over the
lifecycle of a deployed model, leading to performance decay. Adapting a model
on test samples can help mitigate this drop in performance. However, most
test-time adaptation methods have focused on synthetic corruption shifts,
leaving a variety of distribution shifts underexplored. In this paper, we focus
on distribution shifts that evolve gradually over time, which are common in the
wild but challenging for existing methods, as we show. To address this, we
propose STAD, a probabilistic state-space model that adapts a deployed model to
temporal distribution shifts by learning the time-varying dynamics in the last
set of hidden features. Without requiring labels, our model infers
time-evolving class prototypes that act as a dynamic classification head.
Through experiments on real-world temporal distribution shifts, we show that
our method excels in handling small batch sizes and label shift.","M Schirmer,D Zhang,E Nalisnick- arXiv preprint arXiv:2407.12492, 2024 - arxiv.org",https://arxiv.org/abs/2407.12492,
,Parameter-coupled state space models based on quasi-Gaussian fuzzy approximation,Completely Irrelevant,,,,,"Y Wang, F Ma, X Tian, W Chen, Y Zhang, S Ge - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-77731-w,
,Estimating mixed-effects state-space models via particle filters and theEMalgorithm,Completely Irrelevant,,,,,"F Hamdi,C Lellou- Journal of Statistical Computation and …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/00949655.2024.2337339?casa_token=sfdettJt4oUAAAAA:ISDc4FEJ_iumI34pVGJj_udWSJOSBOaRfk55-Vb-1Ntspwylgy6_LV3Sv5Swu9Jn9m0JKO32sCYMuNoj6g,
,Offline reinforcement learning in large state spaces: Algorithms and guarantees,Completely Irrelevant,,RL,,,"N Jiang,T Xie- Statistical Science, 2024 - nanjiang.cs.illinois.edu",https://nanjiang.cs.illinois.edu/files/STS_Special_Issue_Offline_RL.pdf,
2024년 10월 6일 오후 11:21 (GMT+8),A reinforcement learning engine with reduced action and state space for scalable cyber-physical optimal response,Completely Irrelevant,,RL,,"Numerous research studies have been conducted to enhance the resilience of
cyber-physical systems (CPSs) by detecting potential cyber or physical
disturbances. However, the development of scalable and optimal response
measures under power system contingency based on fusing cyber-physical data is
still in an early stage. To address this research gap, this paper introduces a
power system response engine based on reinforcement learning (RL) and role and
interaction discovery (RID) techniques. RL-RID-GridResponder is designed to
automatically detect the contingency and assist with the decision-making
process to ensure optimal power system operation. The RL-RID-GridResponder
learns via an RL-based structure and achieves enhanced scalability by
integrating an RID module with reduced action and state spaces. The
applicability of RL-RID-GridResponder in providing scalable and optimal
responses for CPSs is demonstrated on power systems in the context of Denial of
Service (DoS) attacks. Moreover, simulations are conducted on a Volt-Var
regulation problem using the augmented WSCC 9-bus and augmented IEEE 24-bus
systems based on fused cyber and physical data sets. The results show that the
proposed RL-RID-GridResponder can provide fast and accurate responses to ensure
optimal power system operation under DoS and can extend to other system
contingencies such as line outages and loss of loads.","S Sun,KA Haque,X Huo,LA Homoud… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2410.04518,
,An Assessment of the Challenges in the Adoption of Agric-Tech in Kwazulu-Natal North Coast and Midlands,Completely Irrelevant,,Author’s name is mamba,,,"SG Mamba, A Beharry-Ramraj - Gender and Behaviour, 2024 - journals.co.za",https://journals.co.za/doi/abs/10.10520/ejc-genbeh_v22_n1_a9,
,Linear recurrent units for sequential recommendation,Concrete-RecSys,,RecSys + LRU(Mamba’s predecessor),,,"Z Yue,Y Wang,Z He,H Zeng,J McAuley… - Proceedings of the 17th …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3616855.3635760,
,Time varying M with starvation mortality in a state-space stock assessment model: part 2: atlantic cod (Gadus morhua) on the southern Grand Bank of …,Completely Irrelevant,,,,,"NG Cadigan,S Weerasekera,PM Regular… - Fisheries …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0165783624002388,
,Fractional-order state space reconstruction: a new frontier in multivariate complex time series,Completely Irrelevant,,,,,"J Xie, G Xu, X Chen, X Zhang, R Chen, Z Yang… - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-68693-0,
,Real-Time Distributed Charging Station Recommendation for Electric Vehicles: A Federated Meta-RL Approach,Non-Mamba,,,,,"Y Zhang,J Hu,G Min, J Gao… - IEEE Transactions on …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10876774/,
,Turning Pages with Reinforcement: A Markov Decision Process Approach to Book Recommendations,Non-Mamba,,RecSys + MDP,,,"O Spector, P Zhao - orispector.com",https://www.orispector.com/assets/papers/cs238_final_paper.pdf,
2024년 8월 29일 오후 12:46 (GMT+8),Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep State Space Models,Completely Irrelevant,,,,"This study investigates a method to evaluate time-series datasets in terms of
the performance of deep neural networks (DNNs) with state space models (deep
SSMs) trained on the dataset. SSMs have attracted attention as components
inside DNNs to address time-series data. Since deep SSMs have powerful
representation capacities, training datasets play a crucial role in solving a
new task. However, the effectiveness of training datasets cannot be known until
deep SSMs are actually trained on them. This can increase the cost of data
collection for new tasks, as a trial-and-error process of data collection and
time-consuming training are needed to achieve the necessary performance. To
advance the practical use of deep SSMs, the metric of datasets to estimate the
performance early in the training can be one key element. To this end, we
introduce the concept of data evaluation methods used in system identification.
In system identification of linear dynamical systems, the effectiveness of
datasets is evaluated by using the spectrum of input signals. We introduce this
concept to deep SSMs, which are nonlinear dynamical systems. We propose the
K-spectral metric, which is the sum of the top-K spectra of signals inside deep
SSMs, by focusing on the fact that each layer of a deep SSM can be regarded as
a linear dynamical system. Our experiments show that the K-spectral metric has
a large absolute value of the correlation coefficient with the performance and
can be used to evaluate the quality of training datasets.","S Kanai,Y Ida,K Adachi, M Uchida, T Yoshida… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.16261,
,BrYOLO-Mamba: A Approach to Efficient Tracheal Lesion Detection in Bronchoscopy,Concrete-Non-RecSys,,Mamba + Vision (Medical Image Detection),,,"Y Cao, J Zhang, R Zhuo, J Zhao, Y Dong, T Liu… - IEEE …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10758630/,
,A Review on Reinforcement Learning based News Recommendation Systems and its challenges,Survey-RecSys,,RecSys + RL (Survey),,,"DSN Sindhuja Bangari, RKT Ladly Patel - academia.edu",https://www.academia.edu/download/110605473/1_ICAIS_Reco_Systems_March_2021_.pdf,
,Analytic discrete state-space model of LLC resonant converter,Completely Irrelevant,,,,,"EY Rezaii,A Carvalho- e-Prime-Advances in Electrical Engineering …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S2772671124002055,
,Multi-Agent Decision S4: Leveraging State Space Models for Offline Multi-Agent Reinforcement Learning,Concrete-Non-RecSys,,Mamba + RL,,,"A Bhattacharya,M Bal- Multi-Agent reinforcement Learning for … - openreview.net",https://openreview.net/forum?id=haXIrdvM57,
,Biogas Technology: A review of current practices and the potential for sustainable implementation in Eswatini,Completely Irrelevant,,,,,"AT Tiruneh,AF Murye, S Nxumalo, R Mamba… - researchgate.net",https://www.researchgate.net/profile/Ababu-Teklemariam-Tiruneh/publication/391840046_Biogas_Technology_A_review_of_current_practices_and_the_potential_for_sustainable_implementation_in_Eswatini/links/682924e1d1054b0207ef9cc3/Biogas-Technology-A-review-of-current-practices-and-the-potential-for-sustainable-implementation-in-Eswatini.pdf,
,MFB: A 3D point cloud segmentation model based on Mamba,Concrete-Non-RecSys,,Mamba + point cloud segmentation,,,"X Wang, J Zhang, X Zhang, L Liu… - … on Computer, Vision …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10872599/,Cited by 1
,"Cybernetic states: Communication, control, and state-space in the advanced research projects agency",Completely Irrelevant,,,,,"BJ Jefferson- Political Geography, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0962629824001094,
,S5Utis: Structured State-Space Sequence SegNeXt UNet-like Tongue Image Segmentation in Traditional Chinese Medicine,Concrete-Non-RecSys,,Mamba + Vision (Medical Image Segmentation),,,"D Song, H Zhang,L Shi,H Xu,Y Xu- Sensors, 2024 - mdpi.com",https://www.mdpi.com/1424-8220/24/13/4046,
,Multi-behavior Session-based Recommendation via Graph Reinforcement Learning,Non-Mamba,,RecSys + GNN + RL,,,"S Qin, F Lin, L Xu, B Deng, S Li… - Asian Conference on …, 2024 - proceedings.mlr.press",https://proceedings.mlr.press/v222/qin24a.html,
2025년 5월 23일 오후 2:47 (GMT+8),MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation,Concrete-Non-RecSys,,Mamba + Dance Generation,,"Music-driven 3D dance generation has attracted increasing attention in recent
years, with promising applications in choreography, virtual reality, and
creative content creation. Previous research has generated promising realistic
dance movement from audio signals. However, traditional methods underutilize
genre conditioning, often treating it as auxiliary modifiers rather than core
semantic drivers. This oversight compromises music-motion synchronization and
disrupts dance genre continuity, particularly during complex rhythmic
transitions, thereby leading to visually unsatisfactory effects. To address the
challenge, we propose MEGADance, a novel architecture for music-driven 3D dance
generation. By decoupling choreographic consistency into dance generality and
genre specificity, MEGADance demonstrates significant dance quality and strong
genre controllability. It consists of two stages: (1) High-Fidelity Dance
Quantization Stage (HFDQ), which encodes dance motions into a latent
representation by Finite Scalar Quantization (FSQ) and reconstructs them with
kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage
(GADG), which maps music into the latent representation by synergistic
utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid
backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate
the state-of-the-art performance of MEGADance both qualitatively and
quantitatively. Code will be released upon acceptance.","K Yang,X Tang,Z Peng, Y Hu, J He, H Liu - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2505.17543,
,Knowledge Graph-enhanced Hierarchical Reinforcement Learning for Interactive and Explainable Recommendation,Non-Mamba,,RecSys + RL,,,"M Zhang, Y Li, S Li, Y Wang, J Yan - IEEE Access, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10684710/,
,Rethinking Offline Reinforcement Learning for Sequential Recommendation from A Pair-Wise Q-Learning Perspective,Non-Mamba,,RecSys + RL,,,"R Yang, L Yu, Z Li,S Li,L Wu- 2024 International Joint …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10650400/,Related articles
,Research on library resource optimization algorithm based on reinforcement learning,Non-Mamba,,RL,,,"S Zhang, Y Yang - 2024 International Conference on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10835035/,Related articles
,"The Benefits of Hierarchical Ecosystem Models: Demonstration Using EcoState, a New State‐Space Mass‐Balance Model",Completely Irrelevant,,,,,"JT Thorson,K Kristensen, KY Aydin… - Fish and …, 2025 - Wiley Online Library",https://onlinelibrary.wiley.com/doi/abs/10.1111/faf.12874,
,Balancing therapeutic effect and safety in ventilator parameter recommendation: An offline reinforcement learning approach,Completely Irrelevant,,RL + Medical,,,"B Zhang,X Qiu,X Tan- Engineering Applications of Artificial Intelligence, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0952197623019681,Cited by 2
,Model predictive inferential control of neural state-space models for autonomous vehicle motion planning,Completely Irrelevant,,SSM + Autonomous Vehicles,,,"I Askari,A Vaziri,X Tu,S Zeng… - IEEE Transactions on …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10981628/,
,Repulsive Markovian models for opinion dynamics,Completely Irrelevant,,,,,"CJ Heiker,E Gaetan,L Giarré,P Falcone- Systems & Control Letters, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0167691124000082,
,Team stress: Grasping physiological stress dynamics in small teams through state space grids.,Completely Irrelevant,,,,,"S Sassenus,P Van den Bossche… - International Journal of …, 2024 - psycnet.apa.org",https://psycnet.apa.org/record/2024-83454-001,Cited by 1
,Explainability of Digital Wallets' Fraud Detection Algorithms: Comparative Analysis of SHAP and Permutation Feature Importance,Completely Irrelevant,,,,,"S Mamba, FRK Djomou, OO Awe - … Methods: Case Studies from LISA 2020 …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-72215-8_27,Related articles
,Reinforced prompt personalization for recommendation with large language models,Non-Mamba,,RecSys + Prompt Engineering,,,"W Mao,J Wu,W Chen,C Gao,X Wang… - ACM Transactions on …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3716320?casa_token=G4PEcRbFpG0AAAAA:dEbnkFvobEaACmAAQJVNJc7lypsQIfH_hdUKVdLKZ-xqDUlLpRSQ3GjHXFNvQMuSg2cKOaqr2f7qzw,
,PDQN: User Preference-Based Charging Station Recommendation,Non-Mamba,,RecSys + RL,,,"H Lin, X Li,Y Cao,H Labiod… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10555418/,Cited by 4
,nnu-net revisited: A call for rigorous validation in 3d medical image segmentation,Concrete-Non-RecSys,,Mamba + Vision,,,"F Isensee,T Wald,C Ulrich,M Baumgartner… - … Conference on Medical …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-72114-4_47,
,The introduction of video-enabled directly observed therapy (video-DOT) for patients with drug-resistant TB disease in Eswatini amid the COVID-19 pandemic–a …,Completely Irrelevant,,Author’s name is Mamba,,,"…,T Dlamini, S Ngwenya, C Danbakli, B Mamba… - BMC Health Services …, 2024 - Springer",https://link.springer.com/article/10.1186/s12913-024-11151-4,
,Deep Reinforcement Learning for Boosting Individual and Aggregate Diversity in Product Recommendation Systems,Non-Mamba,,RecSys + RL,,,"J Yu, S Lyu, P Chen - Proceeding of the 2024 5th International …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3708036.3708043?casa_token=S1tLNqCWFRIAAAAA:VAX-2r0pegKPkNIZr3C-wfLX77f3Bx33wujtYTcUwkGj6Ykm4Ng_GESdYE3Sq-sMz9nHFCI3sHC5BA&casa_token=Dkwg8RIS7L0AAAAA:d8aArd9PEyTZaNch6JEVRCi0Bv2gfwF_H51wWe85OObbQLGb7HfcYKH2u6iaa9EY5uuImAG1u4Mkng,
,MCMC aided Bayesian period-height formulation for RC buildings with solid brick infills using system identification,Completely Irrelevant,,,,,"D Gautam,R Adhikari, R Baruwal,D Thapa,L Bhatt… - Engineering …, 2025 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0141029624016663,Cited by 3
2025년 1월 21일 오전 1:23 (GMT+8),Rigidity and nonexistence of complete spacelike hypersurfaces in the steady state space,Completely Irrelevant,,,,"We study complete spacelike hypersurfaces immersed in an open region of the
de Sitter space $\mathbb{S}^{n+1}_{1}$ which is known as the steady state space
$\mathcal{H}^{n+1}$. In this setting, under suitable constraints on the
behavior of the higher order mean curvatures of these hypersurfaces, we prove
that they must be spacelike hyperplanes of $\mathcal{H}^{n+1}$. Nonexistence
results concerning these spacelike hypersurfaces are also given. Our approach
is based on a suitable extension of the Omori-Yau's generalized maximum
principle due to Al\'{\i}as, Impera and Rigoli in [5].","WFC Barboza, HF de Lima, MAL Velásquez - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2501.11614,
,Recommendation-Enabled Edge Caching and D2D Offloading via Incentive-Driven Deep Reinforcement Learning,Completely Irrelevant,,,,,"T Wu,D Yu,C Liu,D Wang… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10439661/,
,Large Language Model driven Policy Exploration for Recommender Systems,Non-Mamba,,RecSys + RL,,,"J Wang,A Karatzoglou,I Arapakis… - Proceedings of the …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3701551.3703496,
,Multi-Preview Recommendation via Reinforcement Learning,Non-Mamba,,RecSys + RL,,,"Y Xu, KT Lai, P Xiong, Z Wu - … 18th ACM Conference on Recommender …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3640457.3691698?casa_token=de8eDK9AwSsAAAAA:qk71DFYs8SzBXNzKqkqU4QX4QD9RR692KIwBM6oCS2-RlP8y0ynl2asCJ6MTIWgdNBhpvtI8pirG2A,
,Genderbased violence committed by women against men in the Kingdom of Eswatini,Completely Irrelevant,,Author’s name is Mamba,,,"P Mpofu, LG Mamba - wiredspace.wits.ac.za",https://wiredspace.wits.ac.za/bitstreams/f03cec89-cf89-4fdc-afad-31293f4bcab3/download,
,Efficient Directed Controller Synthesis Using Action Priority,Completely Irrelevant,,,,,"H Takeuchi, T Hirano, T Yamauchi… - 2024 IEEE International …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10773937/,Related articles
2024년 10월 7일 오후 10:07 (GMT+8),A Metric on the Polycrystalline Microstructure State Space,Completely Irrelevant,,,,"Material microstructures are traditionally compared using sets of statistical
measures that are incomplete, e.g., two visually distinct microstructures can
have identical grain size distributions and phase fractions. While this is not
a severe concern for materials fabricated by traditional means, the
microstructures produced by advanced manufacturing methods can depend
sensitively and unpredictably on the processing conditions. Moreover, the
advent of computational materials design has increased the frequency of
synthetic microstructure generation, and there is not yet a standard approach
in the literature to validate the generated microstructures with experimental
ones. This work proposes an idealized distance on the space of single-phase
polycrystalline microstructures such that two microstructures that are close
with respect to the distance exhibit statistically similar grain geometries in
all respects below a user-specified length scale. Given a pair of micrographs,
the distance is approximated by sampling windows from the micrographs, defining
a distance between pairs of windows, and finding a window matching that
minimizes the sum of pairwise window distances. The approach is used to compare
a variety of synthetic microstructures and to develop a procedure to query a
proof-of-concept database suitable for general single-phase polycrystalline
microstructures.","D Miley, E Suwandi,B Schweinhart… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2410.18981,
2024년 11월 22일 오후 11:13 (GMT+8),Event USKT: U-State Space Model in Knowledge Transfer for Event Cameras,Completely Irrelevant,,,,"Event cameras, as an emerging imaging technology, offer distinct advantages
over traditional RGB cameras, including reduced energy consumption and higher
frame rates. However, the limited quantity of available event data presents a
significant challenge, hindering their broader development. To alleviate this
issue, we introduce a tailored U-shaped State Space Model Knowledge Transfer
(USKT) framework for Event-to-RGB knowledge transfer. This framework generates
inputs compatible with RGB frames, enabling event data to effectively reuse
pre-trained RGB models and achieve competitive performance with minimal
parameter tuning. Within the USKT architecture, we also propose a bidirectional
reverse state space model. Unlike conventional bidirectional scanning
mechanisms, the proposed Bidirectional Reverse State Space Model (BiR-SSM)
leverages a shared weight strategy, which facilitates efficient modeling while
conserving computational resources. In terms of effectiveness, integrating USKT
with ResNet50 as the backbone improves model performance by 0.95%, 3.57%, and
2.9% on DVS128 Gesture, N-Caltech101, and CIFAR-10-DVS datasets, respectively,
underscoring USKT's adaptability and effectiveness. The code will be made
available upon acceptance.","Y Lin,J Zhang,S Li, J Xiao,D Xu, W Wu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2411.15276,
,Understanding customer loyalty-aware recommender systems in E-commerce: an analytical perspective,Non-Mamba,,"RecSys ",,,"R Esmeli,AS Can, A Awad,M Bader-El-Den- Electronic Commerce …, 2025 - Springer",https://link.springer.com/article/10.1007/s10660-025-09954-6,
2024년 5월 22일 오전 12:33 (GMT+8),The implications of state aggregation in deteriorating Markov Decision Processes with optimal threshold policies,Completely Irrelevant,,,,"Markov Decision Processes (MDPs) are mathematical models of sequential
decision-making under uncertainty that have found applications in healthcare,
manufacturing, logistics, and others. In these models, a decision-maker
observes the state of a stochastic process and determines which action to take
with the goal of maximizing the expected total discounted rewards received. In
many applications, the state space of the true system is large and there may be
limited observations out of certain states to estimate the transition
probability matrix. To overcome this, modelers will aggregate the true states
into ``superstates"" resulting in a smaller state space. This aggregation
process improves computational tractability and increases the number of
observations among superstates. Thus, the modeler's choice of state space leads
to a trade-off in transition probability estimates. While coarser
discretization of the state space gives more observations in each state to
estimate the transition probability matrix, this comes at the cost of precision
in the state characterization and resulting policy recommendations. In this
paper, we consider the implications of this modeling decision on the resulting
policies from MDPs for which the true model is expected to have a threshold
policy that is optimal. We analyze these MDPs and provide conditions under
which the aggregated MDP will also have an optimal threshold policy. Using a
simulation study, we explore the trade-offs between more fine and more coarse
aggregation. We explore the the show that there is the highest potential for
policy improvement on larger state spaces, but that aggregated MDPs are
preferable under limited data. We discuss how these findings the implications
of our findings for modelers who must select which state space design to use.","M Pollack,LN Steimle- arXiv preprint arXiv:2405.12912, 2024 - arxiv.org",https://arxiv.org/abs/2405.12912,
2024년 9월 26일 오후 6:27 (GMT+8),Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation,Non-Mamba,,RecSys,,"News recommendation is a challenging task that involves personalization based
on the interaction history and preferences of each user. Recent works have
leveraged the power of pretrained language models (PLMs) to directly rank news
items by using inference approaches that predominately fall into three
categories: pointwise, pairwise, and listwise learning-to-rank. While pointwise
methods offer linear inference complexity, they fail to capture crucial
comparative information between items that is more effective for ranking tasks.
Conversely, pairwise and listwise approaches excel at incorporating these
comparisons but suffer from practical limitations: pairwise approaches are
either computationally expensive or lack theoretical guarantees, and listwise
methods often perform poorly in practice. In this paper, we propose a novel
framework for PLM-based news recommendation that integrates both pointwise
relevance prediction and pairwise comparisons in a scalable manner. We present
a rigorous theoretical analysis of our framework, establishing conditions under
which our approach guarantees improved performance. Extensive experiments show
that our approach outperforms the state-of-the-art methods on the MIND and
Adressa news recommendation datasets.","N Kannen,Y Ma,GJJ van den Burg… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2409.17711,
,Spatializing social reproduction theory: integrating state space and the urban fabric,Completely Irrelevant,,,,,"W Conroy - Review of inteRnational Political economy, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/09692290.2023.2269415?casa_token=rOif8DWTe2gAAAAA:DBgSUjHuX-1ouvgYZqNLiOvpr9MDZABZxz0bzeHhFbW1yCU3Bv8uaoxYkzfO0wnMGjJuYBA8h7w_bera7g,
,Adaptive Recommendation System Strategies: An Exploration of Online Machine Learning Algorithms,Non-Mamba,,RecSys + RL,,,F Lu - scitepress.org,https://www.scitepress.org/Papers/2024/129492/129492.pdf,
,State-space perturbation analytical solution for the dynamics of launch-vehicle boost phase,Completely Irrelevant,,,,,"Z Deng, L Liu - Journal of Aerospace Engineering, 2024 - ascelibrary.org",https://ascelibrary.org/doi/abs/10.1061/JAEEEZ.ASENG-5523,Cited by 2
,Upgrading the GRAVITY fringe tracker for GRAVITY+-Tracking the white-light fringe in the non-observable optical path length state-space,Completely Irrelevant,,,,,"M Nowak, S Lacour, R Abuter, J Woillez… - Astronomy & …, 2024 - aanda.org",https://www.aanda.org/articles/aa/abs/2024/04/aa48771-23/aa48771-23.html,
,Combine temporal information in session-based recommendation with graph neural networks,Non-Mamba,,RecSys + GNN,,,"Q Chen, F Jiang, X Guo, J Chen, K Sha… - Expert Systems with …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417423024715,Cited by 22
,Hybrid session-aware recommendation with feature-based models,Non-Mamba,,Classic RecSys,,,"J Bauer,D Jannach- User Modeling and User-Adapted Interaction, 2024 - Springer",https://link.springer.com/article/10.1007/s11257-023-09379-6,
,Wearable sensors-based human locomotion and indoor localization with smartphone,Completely Irrelevant,,,,,"M Rafiq, A Jalal - ICET24, 2024 - researchgate.net",https://www.researchgate.net/profile/Ahmad-Jalal-9/publication/385074366_Wearable_Sensors-based_Human_Locomotion_And_Indoor_Localization_with_Smartphone/links/6713d7e1069cb92a811d00b2/Wearable-Sensors-based-Human-Locomotion-And-Indoor-Localization-with-Smartphone.pdf,
,Federated distributed deep reinforcement learning for recommendation-enabled edge caching,Completely Irrelevant,,Federated RL,,,"H Zhou, H Wang,Z Yu,G Bin,M Xiao… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10609540/,Cited by 17
,User Response Modeling in Reinforcement Learning for Ads Allocation,Completely Irrelevant,,RL,,,"Z Zhang,Q Zhang, X Wu, X Shi,G Liao… - … Proceedings of the …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3589335.3648310,
,Artificial Intelligence and Blockchain Technology Drive Leadership Decision-Making Research Group Recommendation Algorithm,Completely Irrelevant,,,,,"C Chen, Y Liu, X Wang, Y Xia - International Journal of …, 2024 - World Scientific",https://www.worldscientific.com/doi/abs/10.1142/S1469026824410013,Related articles
,Multi-dimensional fusion attention mechanism with vim-like structure for mobile network design,Concrete-Non-RecSys,,Mamba + Vision,,,"J Shi,R Zhou,P Ren, Z Long - Applied Sciences, 2024 - mdpi.com",https://www.mdpi.com/2076-3417/14/15/6670,
,Missing Data Substitution for Enhanced Robust Filtering and Forecasting in Linear State-Space Models,Non-Mamba,,SSM,,,"D Dobrev, P Szerszen - 2025 - papers.ssrn.com",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5094783,
2024년 4월 8일 오후 7:00 (GMT+8),Continuous-time state-space methods for delta-O-18 and delta-C-13,Completely Irrelevant,,,,"Time series analysis of delta-O-18 and delta-C-13 measurements from benthic
foraminifera for purposes of paleoclimatology is challenging. The time series
reach back tens of millions of years, they are relatively sparse in the early
record and relatively dense in the later, the time stamps of the observations
are not evenly spaced, and there are instances of multiple different
observations at the same time stamp. The time series appear non-stationary over
most of the historical record with clearly visible temporary trends of varying
directions. In this paper, we propose a continuous-time state-space framework
to analyze the time series. State space models are uniquely suited for this
purpose, since they can accommodate all the challenging features mentioned
above. We specify univariate models and joint bivariate models for the two time
series of delta-O-18 and delta-C-13. The models are estimated using maximum
likelihood by way of the Kalman filter recursions. The suite of models we
consider has an interpretation as an application of the Butterworth filter. We
propose model specifications that take the origin of the data from different
studies into account and that allow for a partition of the total period into
sub-periods reflecting different climate states. The models can be used, for
example, to impute evenly time-stamped values by way of Kalman filtering. They
can also be used, in future work, to analyze the relation to proxies for CO2
concentrations.","M Bennedsen,E Hillebrand,SJ Koopman… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2404.05401,
,Recent Developments in Titania–Carbon Nanotube Nanohybrids: Towards Enhanced Photocatalytic Efficiency,Completely Irrelevant,,,,,"…,NN Gumbi,AT Kuvarega,BB Mamba… - … for Energy and Water …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-55329-5_11,Cited by 1
,"Advancements in Modern Recommender Systems: Industrial Applications in Social Media, E-commerce, Entertainment, and Beyond",Non-Mamba,,RecSys (Survey),,,"KJ Sankalp, SN BV, CCS Balne,VK Sunkara… - 2024 - hal.science",https://hal.science/hal-04711099/,
,CIPPO: Contrastive Imitation Proximal Policy Optimization for Recommendation Based on Reinforcement Learning,Non-Mamba,,Recsys + RL,,,"W Chen, S Zhang,R Xie,F Xia, L Lin… - … on Knowledge and …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10534824/,Related articles
2025년 3월 21일 오전 6:37 (GMT+8),Towards agentic recommender systems in the era of multimodal large language models,Non-Mamba,,RecSys + LLM (Survey),,"Recent breakthroughs in Large Language Models (LLMs) have led to the
emergence of agentic AI systems that extend beyond the capabilities of
standalone models. By empowering LLMs to perceive external environments,
integrate multimodal information, and interact with various tools, these
agentic systems exhibit greater autonomy and adaptability across complex tasks.
This evolution brings new opportunities to recommender systems (RS): LLM-based
Agentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive
recommendations, potentially reshaping the user experience and broadening the
application scope of RS. Despite promising early results, fundamental
challenges remain, including how to effectively incorporate external knowledge,
balance autonomy with controllability, and evaluate performance in dynamic,
multimodal settings. In this perspective paper, we first present a systematic
analysis of LLM-ARS: (1) clarifying core concepts and architectures; (2)
highlighting how agentic capabilities -- such as planning, memory, and
multimodal reasoning -- can enhance recommendation quality; and (3) outlining
key research questions in areas such as safety, efficiency, and lifelong
personalization. We also discuss open problems and future directions, arguing
that LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee
a paradigm shift toward intelligent, autonomous, and collaborative
recommendation experiences that more closely align with users' evolving needs
and complex decision-making processes.","C Huang,J Wu,Y Xia,Z Yu,R Wang, T Yu… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2503.16734,
,1–2–3–Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization,Completely Irrelevant,,RL,,,"M Azeem,D Chakraborty,S Kanav,J Křetínský… - … on Verification, Model …, 2025 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-82703-7_5,
,A state-space model-based temperature control system for laser remanufacturing molten pool,Completely Irrelevant,,,,,"GZ Yang, TM Liu, BX Song, XY Jiang… - … Journal of Computer …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/0951192X.2023.2228276?casa_token=O_uFQQwDVo0AAAAA:s6MGVO9kuf_Dvfw--H0UZ4-HLZAadex0oY209Us00FI3HG-wr-pWXVrGMXT2H7D1NqQV0lhhWCtOu-NwzQ,
,A State-Space Estimation and Control of Power System Oscillations,Completely Irrelevant,,,,,WA Khan - 2025 - School of Electrical Engineering and …,,Related articles
,Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning,Non-Mamba,,RecSys + RL + LLM,,,"A Sharma, H Li,X Li,J Jiao- Proceedings of the 30th ACM SIGKDD …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3637528.3671618,
,Faster inference from state space models via GPU computing,Completely Irrelevant,,,,,"C Fagard-Jenkin,L Thomas- Ecological Informatics, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S1574954124000281,
,ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems,Completely Irrelevant,,RecSys + RL,,,"Y Zhang,R Qiu,J Liu,S Wang- Proceedings of the 33rd ACM …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3627673.3679633,
,Scaling representation learning from ubiquitous ecg with state-space models,Completely Irrelevant,,,,,"K Avramidis,D Kunc,B Perz,K Adsul… - IEEE Journal of …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10574308/,
,"IMPLEMENTATION OF EXTRACT, TRANSFORM, LOAD (ETL) ON UNIVERSITY DATABASE USING STATE-SPACE PROBLEM",Completely Irrelevant,,,,,"R Dharayani, KA Laksitowening, AP Yanuarfiani - TEKTRIKA-Jurnal Penelitian dan …, 2024",,Related articles
,Re2llm: Reflective reinforcement large language model for session-based recommendation,Non-Mamba,,RecSys + RL + LLM,,,"Z Wang,Y Du,Z Sun, H Chua,K Feng… - Proceedings of the …, 2025 - ojs.aaai.org",https://ojs.aaai.org/index.php/AAAI/article/view/33399,
,City Bus Reliability Measurement Based on Sparse Field Data Supported by Selected State Space Models,Completely Irrelevant,,,,,"D Vališ, K Hasilová, Z Vintr… - Transportation Research …, 2024 - journals.sagepub.com",https://journals.sagepub.com/doi/abs/10.1177/03611981241263563?casa_token=ss_duOtsuQgAAAAA:CxZYMvMVrH4UzYIe9Elv7gXIlgFLpl1x5a34NluNbH9DL4Q-Xl-NAvOV04tRvW_JwdAnI_FViZSywQ,
,AURO: Reinforcement learning for adaptive user retention optimization in recommender systems,Non-Mamba,,RecSys + RL,,,"Z Xue,Q Cai,B Yang,L Hu,P Jiang,K Gai… - Proceedings of the ACM …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3696410.3714956,
,An Autonomous Vehicle Behavior Decision Method Based on Deep Reinforcement Learning with Hybrid State Space and Driving Risk,Completely Irrelevant,,RL,,,"X Wang, B Qian, J Zhuo,W Liu- Sensors (Basel, Switzerland), 2025 - pmc.ncbi.nlm.nih.gov",https://pmc.ncbi.nlm.nih.gov/articles/PMC11821044/,
,Adaptive English vocabulary recommendation systems: a computational intelligence approach using deep reinforcement learning,Completely Irrelevant,,RecSys + RL,,,"T Zhang, C Li - Second International Conference on Big Data …, 2025 - spiedigitallibrary.org",https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13550/1355040/Adaptive-English-vocabulary-recommendation-systems--a-computational-intelligence-approach/10.1117/12.3059790.short,Cited by 1
,Personalized preference based electric vehicle charging recommendation considering photovoltaic consumption: A transfer reinforcement learning method,,,,,,"W Chen, D Liu,J Cao- IEEE Transactions on Transportation …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10663735/,Cited by 2
,xMTF: A Formula-Free Model for Reinforcement-Learning-Based Multi-Task Fusion in Recommender Systems,,,,,,"Y Cao, C Zhang, X Chen, K Zhan, B Wang - Proceedings of the ACM on …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3696410.3714959?casa_token=B9M7zMOzNOQAAAAA:z2rFVKACcWt6-dTz6PwpgYO5SmHg3No8iMQSS9U7dnndKtlApz34c47rlwcu4hrH6oso5qfp4-WkTA,
,[HTML][HTML]Good practices for surplus production models,,,,,,"A Kokkalis,CW Berg,MS Kapur,H Winker… - Fisheries …, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0165783624000742,
,A dynamic multiobjective recommendation method based on soft actor-critic with discrete actions,,,,,,"J Luo,F Li, J Jiao - Journal of King Saud University Computer and …, 2025 - Springer",https://link.springer.com/article/10.1007/s44443-025-00016-3,
,Adaptive Both homo-and hetero-Feature Integration for Multimodal Emotion Recognition,,,,,,"ZK Wang, ZJ Si - Proceedings of the 6th ACM International Conference …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3696409.3700177?casa_token=RpcCGNrOrjoAAAAA:rkmf31hDp7pmsl_uXFZQYy4lwI09Bk_q7YMPW43izBrYekNZ6uhZTOr_Z-Bf9asi_DDk7xbw-cLhpQ,
2025년 3월 8일 오전 2:20 (GMT+8),A survey of large language model empowered agents for recommendation and search: Towards next-generation information retrieval,,,,,"Information technology has profoundly altered the way humans interact with
information. The vast amount of content created, shared, and disseminated
online has made it increasingly difficult to access relevant information. Over
the past two decades, recommender systems and search (collectively referred to
as information retrieval systems) have evolved significantly to address these
challenges. Recent advances in large language models (LLMs) have demonstrated
capabilities that surpass human performance in various language-related tasks
and exhibit general understanding, reasoning, and decision-making abilities.
This paper explores the transformative potential of LLM agents in enhancing
recommender and search systems. We discuss the motivations and roles of LLM
agents, and establish a classification framework to elaborate on the existing
research. We highlight the immense potential of LLM agents in addressing
current challenges in recommendation and search, providing insights into future
research directions. This paper is the first to systematically review and
classify the research on LLM agents in these domains, offering a novel
perspective on leveraging this advanced AI technology for information
retrieval. To help understand the existing works, we list the existing papers
on LLM agent based recommendation and search at this link:
https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.","Y Zhang, S Qiao,J Zhang,TH Lin,C Gao… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2503.05659,
2024년 2월 8일 오후 8:46 (GMT+8),Noise through an additional variable for mean field games master equation on finite state space,,,,,"This paper provides a mathematical study of the well-posedness of master
equation on finite state space involving terms modelling common noise. In this
setting, the solution of the master equation depends on an additional variable
modelling the value of a stochastic process impacting all players. Using
technique from viscosity solutions, we give sufficient conditions for the
existence of a Lipschitz continuous solution on any time interval. Under some
structural assumptions, we are even able to treat cases in which the dynamics
of this stochastic process depend on the state of the game.","C Bertucci, C Meynard - arXiv preprint arXiv:2402.05635, 2024 - arxiv.org",https://arxiv.org/abs/2402.05635,
,[PDF][PDF]Application of Exercise Recommendation Model Based on Deep Reinforcement Learning,,,,,,"J Dao, L Hong - pdfs.semanticscholar.org",https://pdfs.semanticscholar.org/63fe/c7d71e1a1c731fb5e340e66c94adbadba70a.pdf,
2025년 2월 11일 오후 3:01 (GMT+8),Flow Matching for Collaborative Filtering,,,,,"Generative models have shown great promise in collaborative filtering by
capturing the underlying distribution of user interests and preferences.
However, existing approaches struggle with inaccurate posterior approximations
and misalignment with the discrete nature of recommendation data, limiting
their expressiveness and real-world performance. To address these limitations,
we propose FlowCF, a novel flow-based recommendation system leveraging flow
matching for collaborative filtering. We tailor flow matching to the unique
challenges in recommendation through two key innovations: (1) a behavior-guided
prior that aligns with user behavior patterns to handle the sparse and
heterogeneous user-item interactions, and (2) a discrete flow framework to
preserve the binary nature of implicit feedback while maintaining the benefits
of flow matching, such as stable training and efficient inference. Extensive
experiments demonstrate that FlowCF achieves state-of-the-art recommendation
accuracy across various datasets with the fastest inference speed, making it a
compelling approach for real-world recommender systems.","C Liu,Y Zhang,J Wang,R Ying,J Caverlee- arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2502.07303,
,Deep Recommender System for Optimizing Debt Collection Using Reinforcement Learning,,,,,,"S Keerthana,R Elakkiya… - Cognitive Analytics and …, 2024 - Wiley Online Library",https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394214068.ch3,Related articles
,Model-based recommendations for next-best actions in knowledge-intensive processes,,,,,,"A Seidel,S Haarmann,M Weske- International Conference on Advanced …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-61057-8_12,Cited by 1
,Fine-tuning large language model based explainable recommendation with explainable quality reward,,,,,,"M Yang,M Zhu,Y Wang, L Chen,Y Zhao… - Proceedings of the …, 2024 - ojs.aaai.org",https://ojs.aaai.org/index.php/AAAI/article/view/28777,
,Deep Learning-based U-Mamba Model to Predict Differentiated Gastric Cancer using Radiomics Features from Spleen Segmentation,,,,,,"H Shang, Y Tong, M Li, S Xu, L Xu… - Current Medical …, 2024 - benthamdirect.com",https://www.benthamdirect.com/content/journals/cmir/10.2174/0115734056349216241118115005,
,Judge a Book by its Cover: A Multimodal Approach to Book Genre Prediction,,,,,,"R Toosi,A Hosseini,R Toosi… - 2025 11th International …, 2025 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/11006189/,Related articles
,[PDF][PDF]FaultNVC: Earthquake fault plane extractor through point‐cloud normal vector clustering for hypocenter distributions,,,,,,"Y Sawaki, Y Sato,T Uchide- 2025 - gsj.jp",https://www.gsj.jp/data/openfile/no0759/gsj_openfile_report_759_en.pdf,
,Bayesian state-space modelling of stock markets in G7 countries During the COVID-19 Pandemic,,,,,,"OO Ojo- Heliyon, 2024 - cell.com",https://www.cell.com/heliyon/fulltext/S2405-8440(24)15477-1,
,[HTML][HTML]UET4Rec: U-net encapsulated transformer for sequential recommender,,,,,,"J Wang,MJ Ignacio, S Yu, H Jin,YG Kim- Expert Systems with Applications, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417424016488,
,EasyRL4Rec: An Easy-to-use Library for Reinforcement Learning Based Recommender Systems,,,,,,"Y Yu,C Gao,J Chen, H Tang, Y Sun,Q Chen… - Proceedings of the 47th …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3626772.3657868,
,Adaptive Bayesian Algorithms for Complex State Space and Mathematical Models,,,,,,I Botha- 2024 - eprints.qut.edu.au,https://eprints.qut.edu.au/251487/,
,Coral: Collaborative retrieval-augmented large language models improve long-tail recommendation,,,,,,"J Wu,CC Chang,T Yu,Z He,J Wang,Y Hou… - Proceedings of the 30th …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3637528.3671901,
,Multivariate trace estimation using quantum state space linear algebra,,,,,,"L Mor-Yosef,S Ubaru,L Horesh,H Avron- SIAM Journal on Matrix Analysis …, 2025 - SIAM",https://epubs.siam.org/doi/abs/10.1137/24M1654749,
,[HTML][HTML]A Framework of Recommendation System for Unmanned Aerial Vehicle Autonomous Maneuver Decision,,,,,,"Q Hao, T Jing, Y Sun, Z Yang, J Zhang, J Wang… - Drones, 2024 - mdpi.com",https://www.mdpi.com/2504-446X/9/1/25,
,RPAF: A Reinforcement Prediction-Allocation Framework for Cache Allocation in Large-Scale Recommender Systems,,,,,,"S Su,X Chen, Y Wang, Y Wu, Z Zhang, K Zhan… - … on Recommender …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3640457.3688128?casa_token=IIHXsobdJmQAAAAA:zCZTrdidOZnr2FxTnAYZNod_ZWLDeuYugfBj_ayto8QUHsXVpBxgLSnzY-oxTx2FGqjShcnLewjFEQ,
,On the relative stability of linear time invariant systems,,,,,,"GR Murthy,TJ Swamy- IETE Journal of Research, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/03772063.2023.2297376?casa_token=kqo4yRkDlqkAAAAA:NdsKVBG_dY6eUkf9xEyoDTTrQoBwZTFJTb8uruiWeZwG34eXhQbNZYjrNvKu7W0fcreKtasRlfrCQQP-fA,
,Sentiment in a State-Space Model,,,,,,"DF Iezzi, R Monte - New Frontiers in Textual Data Analysis, 2024 - books.google.com",https://books.google.com/books?hl=en&lr=&id=5JQjEQAAQBAJ&oi=fnd&pg=PA77&dq=(mamba+OR+%22state+space%22+OR+%22state-space%22+OR+%22state+spaces%22+OR+%22state-spaces%22)+AND+(recommend+OR+recommender+OR+recommendation)&ots=haDEx1eaEm&sig=9YUS_gX3PGCL1ZISXJy94ziOtdA,Related articles
,Review Of Approaches Towards Building AI Based Career Recommender & Guidance Systems,,,,,,"A Shah, R Pati, A Pimplikar,S Puthran… - ScienceOpen …, 2024 - scienceopen.com",https://www.scienceopen.com/hosted-document?doi=10.14293/PR2199.000978.v1,
,LQR Discrete Time Control of a Buck Converter Using a Non-Minimal State Space Representation,,,,,,"R Tymerski - Journal of Power and Energy Engineering, 2025 - scirp.org",https://www.scirp.org/journal/paperinformation?paperid=140199,
,Gaussian Sum Filtering for Wiener State-Space Models with a Class of Non-Monotonic Piecewise Nonlinearities,,,,,,"AL Cedeño,RA González,JC Agüero- IFAC-PapersOnLine, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S2405896324012783,
,Distributed Sharing and Personalized Recommendation System of College Preschool Education Resources Under the Intelligent Education Cloud Platform …,,,,,,"Y Su, Z Dong, S Li - International Journal of High Speed Electronics …, 2025 - World Scientific",https://www.worldscientific.com/doi/abs/10.1142/S0129156425404309,Cited by 1
,Improved Control of a Primary Linear Actuator using State-Space,,,,,,A Lagerström - 2024 - diva-portal.org,https://www.diva-portal.org/smash/record.jsf?pid=diva2:1872914,
,The Africa CDC Guidance for Strengthening NCDI-MH Surveillance Systems: The role of HIS assessments,,,,,,"…, B Kabarega, F Moser, D Njobo Mamba… - European Journal of …, 2024 - academic.oup.com",https://academic.oup.com/eurpub/article-abstract/34/Supplement_3/ckae144.092/7842921,Related articles
,Recommendation System in Industrial data analysis using Machine Learning Techniques,,,,,,"T Joshi,B Joshi, K Syed, S Vijayarani… - 2024 IEEE North …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10774728/,Cited by 1
,Digital-twin-based cps anomaly diagnosis and security defense countermeasure recommendation,,,,,,"J Ma, Y Guo, C Fang, Q Zhang - IEEE Internet of Things Journal, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10439988/,Cited by 7
,[HTML][HTML]Adaptive task recommendation based on reinforcement learning in mobile crowd sensing,,,,,,"G Yang, G Xie, J Wang, X He, L Gao, Y Liu - Applied Intelligence, 2024 - Springer",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10489-023-05247-3&casa_token=4bP1-KJbccwAAAAA:ct7CbkYQtVdk6ftJyQphxoLvb4L4bIr7gd7TAM-SOey9VVbibDd9UM952BGRtNCNCBTsIcWuEEWcMXKEeg,
,Td3: Tucker decomposition based dataset distillation method for sequential recommendation,,,,,,"J Zhang,M Yin,H Wang, Y Li,Y Ye, X Lou… - Proceedings of the …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3696410.3714613?casa_token=yhqrzXuBvZgAAAAA:D4T2ky5ql84Mx0vrOAwx8uJnRqXGzTEGj3LMyJVrvWu513tQ9fjVy345uEP8rxPBO4Zkh0U50dJ5Lw,
,"Extrafamilial Communication and Risky Sexual Behavior of Adolescents in Urban Areas, Mont Ngafula Health Zone in Kinshasa",,,,,,"SM Mamba, D Tshilomba, O Kiyonge… - Open Access Library …, 2024 - scirp.org",https://www.scirp.org/journal/paperinformation?paperid=131005,
2024년 4월 19일 오전 7:12 (GMT+8),Enabling stateful behaviors for diffusion-based policy learning,,,,,"While imitation learning provides a simple and effective framework for policy
learning, acquiring consistent actions during robot execution remains a
challenging task. Existing approaches primarily focus on either modifying the
action representation at data curation stage or altering the model itself, both
of which do not fully address the scalability of consistent action generation.
To overcome this limitation, we introduce the Diff-Control policy, which
utilizes a diffusion-based model to learn the action representation from a
state-space modeling viewpoint. We demonstrate that we can reduce
diffusion-based policies' uncertainty by making it stateful through a Bayesian
formulation facilitated by ControlNet, leading to improved robustness and
success rates. Our experimental results demonstrate the significance of
incorporating action statefulness in policy learning, where Diff-Control shows
improved performance across various tasks. Specifically, Diff-Control achieves
an average success rate of 72% and 84% on stateful and dynamic tasks,
respectively. Project page: https://github.com/ir-lab/Diff-Control","X Liu,F Weigend,Y Zhou,HB Amor- arXiv preprint arXiv:2404.12539, 2024 - arxiv.org",https://arxiv.org/abs/2404.12539,Cited by 3
,A social image recommendation system based on deep reinforcement learning,,,,,,"S Ahmadkhani,ME Moghaddam- Plos one, 2024 - journals.plos.org",https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0300059,
,Early Life-Stage and Adult Productivity Dynamics Derived from a State-Space Stock Assessment Model for Data-Limited Thorny Skates (Amblyraja Radiata Donovan …,,,,,,"NG Cadigan,R Steele,M Weerasekera… - Available at SSRN … - papers.ssrn.com",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4905591,
,Deep Reinforcement Learning-Based Recommendation System for Libraries Using Implicit Feedback,,,,,,L Roldan Vilardell - 2025 - upcommons.upc.edu,https://upcommons.upc.edu/handle/2117/427325,
,A Reinforcement Learning Based Recommendation System to Improve Performance of Students in Outcome Based Education Model,,,,,,"MB Tariq, HA Habib - IEEE Access, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10445475/,
,The Unexpected Results of Reinforcement Learning for Sequential Recommendation,,,,,,ÁL Silva- 2024 - search.proquest.com,https://search.proquest.com/openview/8e40fab0c2e91a783b9290613def2099/1?pq-origsite=gscholar&cbl=2026366&diss=y,Related articles
,A state-space perspective on modelling and inference for online skill rating,,,,,,"S Duffield,S Power,L Rimella- Journal of the Royal Statistical …, 2024 - academic.oup.com",https://academic.oup.com/jrsssc/article-abstract/73/5/1262/7734616,
,Policy GRU-RL: Simplified Music Playlist Recommendation U sing Sequential on Reinforcement Learning Concept,,,,,,"C Chanarong, S Maneeroj - 2024 21st International Joint …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10613646/,Related articles
,Learning Personalized Health Recommendations via Offline Reinforcement Learning,,,,,,"LD Preuett - … of the 18th ACM Conference on Recommender …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3640457.3688021?casa_token=eAxlZ5xlMQ4AAAAA:8U75N-WVZVYPwnUXO7oRDvn21R0_1mEOPaYFe3d77Ds_DEUnW4N0D2_r6CGNvjJ3rZiU5AITOz6b9w,
,Exploring team strategy dynamics in tennis doubles matches,,,,,,"Z Liu, C Dong, C Wang, TY Dong,K Jiang- International Sports Analytics …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-3-031-69073-0_9,Cited by 2
,A State-Space Method for Vibration of Double-Beam Systems with Variable Cross Sections,,,,,,"Y Li, H Guo, F Xiong, L Xie, J Gong… - Journal of Engineering …, 2024 - ascelibrary.org",https://ascelibrary.org/doi/abs/10.1061/JENMDT.EMENG-7723,
,Scalable Accelerated Intelligent Charging Strategy Recommendation for Electric Vehicles Based on Deep Q-Networks.,,,,,,"X Shen, Z Wu, Y Zhang, S Niu - International Journal of …, 2024 - search.ebscohost.com",https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=2158107X&AN=175401646&h=lYtvh00rW85BLIId2aKDOOVfmFVTEdQ6VtK5%2FyToUOovCIln4mqLLdPCQi6qpPJsepxW%2FBOCyimTPuCSbY9L3A%3D%3D&crl=c,
,Efficient methods for fitting nonlinear non-Gaussian state space models of wildlife population dynamics,,,,,,F Empacher - 2024 - research-repository.st-andrews.ac …,https://research-repository.st-andrews.ac.uk/handle/10023/29716,
,MARRGM: Learning Framework for Multi-agent Reinforcement Learning via Reinforcement Recommendation and Group Modification,,,,,,"P Wu, L Tian, Q Zhang, B Mao… - IEEE Robotics and …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10502122/,Cited by 4
2025년 2월 5일 오전 12:14 (GMT+8),Sparse Data Generation Using Diffusion Models,,,,,"Sparse data is ubiquitous, appearing in numerous domains, from economics and
recommender systems to astronomy and biomedical sciences. However, efficiently
generating high-fidelity synthetic sparse data remains a significant challenge.
We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse
data. SDD extends continuous state-space diffusion models with an explicit
representation of exact zeros by modeling sparsity through the introduction of
Sparsity Bits. Empirical validation in various domains, including two
scientific applications in physics and biology, demonstrates that SDD achieves
high fidelity in representing data sparsity while preserving the quality of the
generated data.","P Ostheimer,M Nagda,M Kloft,S Fellenz- arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2502.02448,
,Research on Book Information Accurate Retrieval Recommendation System Based on Large Language Model,,,,,,"J Qiao - … Conference on Computers, Information Processing and …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10788139/,Related articles
2024년 7월 3일 오전 12:54 (GMT+8),Reinforcement Learning and Machine ethics: a systematic review,,,,,"Machine ethics is the field that studies how ethical behaviour can be
accomplished by autonomous systems. While there exist some systematic reviews
aiming to consolidate the state of the art in machine ethics prior to 2020,
these tend to not include work that uses reinforcement learning agents as
entities whose ethical behaviour is to be achieved. The reason for this is that
only in the last years we have witnessed an increase in machine ethics studies
within reinforcement learning. We present here a systematic review of
reinforcement learning for machine ethics and machine ethics within
reinforcement learning. Additionally, we highlight trends in terms of ethics
specifications, components and frameworks of reinforcement learning, and
environments used to result in ethical behaviour. Our systematic review aims to
consolidate the work in machine ethics and reinforcement learning thus
completing the gap in the state of the art machine ethics landscape","A Vishwanath,LA Dennis,M Slavkovik- arXiv preprint arXiv:2407.02425, 2024 - arxiv.org",https://arxiv.org/abs/2407.02425,
2024년 10월 16일 오전 7:51 (GMT+8),The Moral Case for Using Language Model Agents for Recommendation,,,,,"Our information and communication environment has fallen short of the ideals
that networked global communication might have served. Identifying all the
causes of its pathologies is difficult, but existing recommender systems very
likely play a contributing role. In this paper, which draws on the normative
tools of philosophy of computing, informed by empirical and technical insights
from natural language processing and recommender systems, we make the moral
case for an alternative approach. We argue that existing recommenders
incentivise mass surveillance, concentrate power, fall prey to narrow
behaviourism, and compromise user agency. Rather than just trying to avoid
algorithms entirely, or to make incremental improvements to the current
paradigm, researchers and engineers should explore an alternative paradigm: the
use of language model (LM) agents to source and curate content that matches
users' preferences and values, expressed in natural language. The use of LM
agents for recommendation poses its own challenges, including those related to
candidate generation, computational efficiency, preference modelling, and
prompt injection. Nonetheless, if implemented successfully LM agents could:
guide us through the digital public sphere without relying on mass
surveillance; shift power away from platforms towards users; optimise for what
matters instead of just for behavioural proxies; and scaffold our agency
instead of undermining it.","S Lazar,L Thorburn,T Jin,L Belli- arXiv preprint arXiv:2410.12123, 2024 - arxiv.org",https://arxiv.org/abs/2410.12123,
,[AML] Project Proposal Guidelines for Advanced Machine Learning Course,,,,,,J Du - Tsinghua University Course: Advanced Machine … - openreview.net,https://openreview.net/forum?id=0CfLQLw5yV&filter=excludedInvitations%3Atsinghua.edu.cn%2FTHU%2F2024%2FFall%2FAML%2FSubmission2%2F-%2FChat&nesting=3&sort=date-desc,
,Interactive Recommender System: Causality-based Popularity Bias and Popularity Drift,,,,,,"R Ye,WKV Chan, Y Ye,K Zhang… - 2024 4th International …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10602873/,Related articles
,"M3C: a Multi-Domain Multi-Objective, Mixed-Modality Framework for Cost-Effective, Industry Scale Recommendation",,,,,,"L Luo,M Hang, Z Zhang, A Gu, B Zhang, B Liu, C Chen… - openreview.net",https://openreview.net/forum?id=VCZ1o8gFny,
,Retention Depolarization in Recommender System,,,,,,"X Zhang,H Wang, Y Liu - Proceedings of the ACM Web Conference …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3589334.3645485?casa_token=EvVDp1yB3LUAAAAA:OA58dB-vfVa4X5kcay8v3JIdE6v_fJSlJbTXLQM4XYpaIGNN8A1uStSbvCgEitZR0ao0252ZYHd7SQ,
,[HTML][HTML]Optimizing electric vehicle charging recommendation in smart cities: A multi-agent reinforcement learning approach,,,,,,"P Suanpang, P Jamjuntr - World Electric Vehicle Journal, 2024 - mdpi.com",https://www.mdpi.com/2032-6653/15/2/67,
,M3Rec: A Context-Aware Offline Meta-Level Model-Based Reinforcement Learning Approach for Cold-Start Recommendation,,,,,,"Y Wang,Y Ge,Z Li, L Li,R Chen- ACM Transactions on Information …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3659947,
,Cutting-Edge Technology in Recommender Systems,,,,,,"L Hu, Y Li, G Cui, K Yi - … Recommender System: Principles, Technologies …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-981-97-2581-6_10,Related articles
,Attention is not the only choice: Counterfactual reasoning for path-based explainable recommendation,,,,,,"Y Li,X Sun,H Chen,S Zhang… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10463173/,
,[HTML][HTML]Analysis of Autonomous Penetration Testing Through Reinforcement Learning and Recommender Systems,,,,,,"AC Moreno,A Hernandez-Suarez,G Sanchez-Perez… - Sensors, 2025 - mdpi.com",https://www.mdpi.com/1424-8220/25/1/211,
2025년 4월 23일 오후 2:43 (GMT+8),Killing Two Birds with One Stone: Unifying Retrieval and Ranking with a Single Generative Recommendation Model,,,,,"In recommendation systems, the traditional multi-stage paradigm, which
includes retrieval and ranking, often suffers from information loss between
stages and diminishes performance. Recent advances in generative models,
inspired by natural language processing, suggest the potential for unifying
these stages to mitigate such loss. This paper presents the Unified Generative
Recommendation Framework (UniGRF), a novel approach that integrates retrieval
and ranking into a single generative model. By treating both stages as sequence
generation tasks, UniGRF enables sufficient information sharing without
additional computational costs, while remaining model-agnostic. To enhance
inter-stage collaboration, UniGRF introduces a ranking-driven enhancer module
that leverages the precision of the ranking stage to refine retrieval
processes, creating an enhancement loop. Besides, a gradient-guided adaptive
weighter is incorporated to dynamically balance the optimization of retrieval
and ranking, ensuring synchronized performance improvements. Extensive
experiments demonstrate that UniGRF significantly outperforms existing models
on benchmark datasets, confirming its effectiveness in facilitating information
transfer. Ablation studies and further experiments reveal that UniGRF not only
promotes efficient collaboration between stages but also achieves synchronized
optimization. UniGRF provides an effective, scalable, and compatible framework
for generative recommendation systems.","L Zhang, K Song, YQ Lee,W Guo,H Wang, Y Li… - arXiv preprint arXiv …, 2025 - arxiv.org",https://arxiv.org/abs/2504.16454,
,Integration of deep reinforcement learning with collaborative filtering for movie recommendation systems,,,,,,"S Peng, S Siet,S Ilkhomjon, DY Kim,DS Park- Applied Sciences, 2024 - mdpi.com",https://www.mdpi.com/2076-3417/14/3/1155,
,FuXi-α: Scaling Recommendation Model with Feature Interaction Enhanced Transformer,,,,,,"Y Ye,W Guo,JY Chin,H Wang, H Zhu, X Lin… - … Proceedings of the …, 2025 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3701716.3715448?casa_token=pkj0x8fUEQwAAAAA:Dv_v4l4G24KxVe-QMTDbeXGT5REYsUCeNXAiDXhw2xtmUXRjP3nlXLPb9ncQgp1XJ1KLWOqDlhFFKA,
,Personalized Route Recommendation Based on User Habits for Vehicle Navigation,,,,,,"Y Huang, X Jin,M Fan, X Yang, F Jiang - Proceedings of the 2024 …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3704657.3704663?casa_token=jSfOrBF5vqcAAAAA:Tvmg2qXTmRKZP-hKkMhDUdMy3S9k_1bdqYyPgfwxx8HE2kJQbyqMOEnxv66tIM22P4nUp9of2rqbmw,
,[PDF][PDF]Exercise Recommendation Algorithm Based on Reinforcement Learning.,,,,,,"S Yu, J Li,T Zhang- Engineering Letters, 2024 - engineeringletters.com",https://www.engineeringletters.com/issues_v32/issue_10/EL_32_10_13.pdf,
,[HTML][HTML]A switching state-space transmission model for tracking epidemics and assessing interventions,,,,,,"J Feng,L Wang- Computational Statistics & Data Analysis, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0167947324000616,
,YOLOv5_mamba: unmanned aerial vehicle object detection based on bidirectional dense feedback network and adaptive gate feature fusion,,,,,,"S Wu,X Lu, C Guo - Scientific Reports, 2024 - nature.com",https://www.nature.com/articles/s41598-024-73241-x,
,CDCM: ChatGPT-aided diversity-aware causal model for interactive recommendation,,,,,,"X Wen,W Nie, J Liu, Y Su, Y Zhang… - IEEE Transactions on …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10387774/,Cited by 12
,[HTML][HTML]Exploring multi-dimensional interests for session-based recommendation,,,,,,"Y Yang, J Sun, G An - Multimedia Systems, 2024 - Springer",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s00530-024-01437-2&casa_token=k1t-XJrGeUMAAAAA:K7vi6KrMSnzQFrmBGTLkNemmiHuh96j5oa9gKRlo_ku_2qWRfwedODolmtIMoUF9e9iK8yweofIeJwUgkA,
,A hierarchical random effects state-space model for modeling brain activities from electroencephalogram data,,,,,,"X Guo,B Yang,JM Loh,Q Wang, Y Wang - Biometrics, 2024 - academic.oup.com",https://academic.oup.com/biometrics/article-abstract/80/4/ujae130/7879547,
,The bii4africa dataset of faunal and floral population intactness estimates across Africa's major land uses,,,,,,"…, ZJK Madikiza,TAM Mahlaba, D Mallon,ML Mamba… - Scientific data, 2024 - nature.com",https://www.nature.com/articles/s41597-023-02832-6,
2024년 12월 24일 오후 1:27 (GMT+8),VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis,,,,,"Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two
dominant models for image analysis. While CNNs excel at extracting multi-scale
features and ViTs effectively capture global dependencies, both suffer from
high computational costs, particularly when processing high-resolution images.
Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have
attracted attention due to their efficiency. However, their performance in
image classification tasks remains limited. To address these challenges, this
paper introduces VisionGRU, a novel RNN-based architecture designed for
efficient image classification. VisionGRU leverages a simplified Gated
Recurrent Unit (minGRU) to process large-scale image features with linear
complexity. It divides images into smaller patches and progressively reduces
the sequence length while increasing the channel depth, thus facilitating
multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional
scanning captures both local and global contexts, improving long-range
dependency modeling, particularly for tasks like semantic segmentation.
Experimental results on the ImageNet and ADE20K datasets demonstrate that
VisionGRU outperforms ViTs, significantly reducing memory usage and
computational costs, especially for high-resolution images. These findings
underscore the potential of RNN-based approaches for developing efficient and
scalable computer vision solutions. Codes will be available at
https://github.com/YangLiu9208/VisionGRU.","S Yin, K Yin,W Chen, E Huang,Y Liu- arXiv preprint arXiv:2412.18178, 2024 - arxiv.org",https://arxiv.org/abs/2412.18178,
,Social domain integrated semantic self-discovery method for recommendation,,,,,,"D Wu, X Fan, P Zhang, M Fu - Pattern Recognition Letters, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0167865524001594,Related articles
,MemoCRS: Memory-enhanced Sequential Conversational Recommender Systems with Large Language Models,,,,,,"Y Xi,W Liu,J Lin,B Chen,R Tang,W Zhang… - Proceedings of the 33rd …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3627673.3679599?casa_token=27W8zrQstMcAAAAA:KNjf06vpljW_YNFo5xzTx24nFZZoRdCdKJ1XJWVWX4mW5s-D9-EHL2URVay_c_a6nGEq5k-gAUZb9w,
2024년 8월 7일 오후 12:20 (GMT+8),Lifelong personalized low-rank adaptation of large language models for recommendation,,,,,"We primarily focus on the field of large language models (LLMs) for
recommendation, which has been actively explored recently and poses a
significant challenge in effectively enhancing recommender systems with logical
reasoning abilities and open-world knowledge. Current mainstream efforts mainly
center around injecting personalized information from recommendation models
into LLMs by customizing input templates or aligning representations between
semantic and recommendation spaces at the prediction layer. However, they face
three significant limitations: (1) LoRA is mostly used as a core component in
existing works, but personalization is not well established in LoRA parameters
as the LoRA matrix shared by every user may not cater to different users'
characteristics, leading to suboptimal performance. (2) Although lifelong
personalized behavior sequences are ideal for personalization, their use raises
effectiveness and efficiency issues since LLMs require escalating training and
inference time to extend text lengths. (3) Existing approaches aren't scalable
for large datasets due to training efficiency constraints. Thus, LLMs only see
a small fraction of the datasets (e.g., less than 10%) instead of the whole
datasets, limiting their exposure to the full training space. To address these
problems, we propose RecLoRA. This model incorporates a Personalized LoRA
module that maintains independent LoRAs for different users and a Long-Short
Modality Retriever that retrieves different history lengths for different
modalities, significantly improving performance while adding minimal time cost.
Furthermore, we design a Few2Many Learning Strategy, using a conventional
recommendation model as a lens to magnify small training spaces to full spaces.
Extensive experiments on public datasets demonstrate the efficacy of our
RecLoRA compared to existing baseline models.","J Zhu,J Lin,X Dai,B Chen, R Shan,J Zhu… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2408.03533,
,Fully actuated system theory and applications: new developments in 2023,,,,,,"G Duan, B Zhou, X Yang - International Journal of Systems …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/full/10.1080/00207721.2024.2373589,
,Research on digital English teaching materials recommendation based on improved machine learning,,,,,,"M Ma - International Journal of Information Technology and …, 2025 - inderscienceonline.com",https://www.inderscienceonline.com/doi/abs/10.1504/IJITM.2025.144113,Cited by 1
2024년 10월 23일 오전 1:13 (GMT+8),Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing,,,,,"Knowledge Editing (KE) algorithms alter models' weights to perform targeted
updates to incorrect, outdated, or otherwise unwanted factual associations.
However, recent work has shown that applying KE can adversely affect models'
broader factual recall accuracy and diminish their reasoning abilities.
Although these studies give insights into the potential harms of KE algorithms,
e.g., performance evaluations on benchmarks, little is understood about why
such destructive failures occur. Motivated by this, we define a novel synthetic
task in which a Transformer is trained from scratch to internalize a
""structured"" knowledge graph. The structure enforces relationships between
entities of the graph, such that editing a factual association has ""trickling
effects"" on other entities (e.g., altering X's parent is Y to Z affects who X's
siblings' parent is). Through evaluations of edited models on this task, we
show that KE inadvertently affects representations of entities beyond the
targeted one, distorting relevant structures that allow a model to infer unseen
knowledge about an entity. We call this phenomenon representation shattering
and demonstrate that it degrades models' factual recall and reasoning
performance. We further corroborate our findings in naturalistic settings with
pre-trained Llama and Mamba models as well. Overall, our work yields a precise
mechanistic hypothesis to explain why KE has adverse effects on model
abilities.","K Nishi,M Okawa,R Ramesh,M Khona… - arXiv preprint arXiv …, 2024 - arxiv.org",https://arxiv.org/abs/2410.17194,Related articles
,"A multi-task deep reinforcement learning-based recommender system for co-optimizing energy, comfort, and air quality in commercial buildings with humans-in-the …",,,,,,"S Xia,P Wei, Y Liu,A Sonta,X Jiang- Data-Centric Engineering, 2024 - cambridge.org",https://www.cambridge.org/core/journals/data-centric-engineering/article/multitask-deep-reinforcement-learningbased-recommender-system-for-cooptimizing-energy-comfort-and-air-quality-in-commercial-buildings-with-humansintheloop/2165D51CABA9B5AF821A103571836F9E,
,[HTML][HTML]Snakes and ladders: Accelerating state space model inference with speculative decoding,,,,,,"Y Wu, Y Dukler,M Trager,A Achille,W Xia,S Soatto- 2024 - amazon.science",https://www.amazon.science/publications/snakes-and-ladders-accelerating-state-space-model-inference-with-speculative-decoding,
,English Learning Knowledge Point Recommendation Algorithm based on Deep Deterministic Policy Gradient,,,,,,"J Yang - 2024 International Conference on Integrated …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10860216/,Related articles
,Smart manufacturing enabled by intelligent technologies,,,,,,"Y Lu,L Wang,A Nassehi,J Wan- International Journal of …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/full/10.1080/0951192X.2023.2296185,
,Innovative Application of Reinforcement Learning in User Growth and Behavior Prediction,,,,,,"Z Ma- European Journal of AI, Computing & Informatics, 2025 - pinnaclepubs.com",http://pinnaclepubs.com/index.php/EJACI/article/view/13,
,Enhancing data-limited assessments with random effects: a case study on Korea chub mackerel (Scomber japonicus),,,,,,"K Kim,N Sibanda,R Arnold… - Canadian Journal of …, 2024 - cdnsciencepub.com",https://cdnsciencepub.com/doi/abs/10.1139/cjfas-2023-0358?casa_token=hZWO5uhymDAAAAAA:7yj010BeecZkCOKdSREHLzk15Z1FUyN6hsEFPtxFS510VHdofpTZoU5wxzheaMiHUTRy_hUfp-RPgw,
,An automatic college library book recommendation system using optimized Hidden Markov based weighted fuzzy ranking model,,,,,,"M Verma, PK Patnaik - Engineering Applications of Artificial Intelligence, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0952197623018481,Cited by 10
,"Environmental risk assessment, principal component analysis, tracking the source of toxic heavy metals of solid gold mine waste tailings, South Africa",,,,,,"LV Tibane, D Mamba - Environmental Forensics, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/15275922.2023.2172478?casa_token=w4QSItZUKC4AAAAA:BcoZrQm4OZFIqSrXFKdTnk18yrUwQD68quJ58B-ekH3wKsKXO2x966TviFWNSJuszTQ_Ocjo2hBNwEv4jA,
,Smart contract code repair recommendation based on reinforcement learning and multi-metric optimization,,,,,,"H Guo, Y Chen,X Chen,Y Huang… - ACM Transactions on …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3637229?casa_token=db1bO7jPMbYAAAAA:NV4cghC_4_T7TdY2qWXEsza_xHJWECC4dDV6qFMNltHIZy9bvOqAVKuS3wqQWbk-rM1IMSeVcuvsfg,
,A Model Predictive Control Approach to Enhance Obstacle Avoidance While Performing Autonomous Docking,,,,,,"A Pinto,BM Ferreira, N Cruz,SP Soares… - OCEANS 2024 …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10753848/,Related articles
,Personalized English Vocabulary Learning Path Recommendation Based on Reinforcement Learning,,,,,,"Y Pan - Proceedings of the 2024 International Conference on …, 2024 - dl.acm.org",https://dl.acm.org/doi/abs/10.1145/3662739.3672181?casa_token=7Ner2a0SDRgAAAAA:PqVsG2FUKxPSGMOn76Pec4SZ7NxYUTsI4JHF0yuo2x93uaMZ-xFV_fLKJNTJBHdMM2U_u4YfAwXvMw,
,Moveformer: Spatial graph periodic injection network for next POI recommendation,,,,,,"Y Li, Z Zhang,Z Huang, C Wang, T He, M Lu… - … on Knowledge Science …, 2024 - Springer",https://link.springer.com/chapter/10.1007/978-981-97-5495-3_4,Cited by 1
,Work-in-Progress: Worst-Case Execution-Time Measurement Techniques for Nonlinear Model Predictive Controllers,,,,,,"R Krishnamurthy,GA Pérez,J Denil… - … Codesign and System …, 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10740782/,
,EPAN-SERec: Expertise preference-aware networks for software expert recommendations with knowledge graph,,,,,,"M Tang, D Wu, S Zhang, W Gao - Expert Systems with Applications, 2024 - Elsevier",https://www.sciencedirect.com/science/article/pii/S0957417423034875,Cited by 7
,Personalized Health Assistant with Reinforcement Learning,,,,,,"J Jin, M Kim,SD Kim- … for Medicine, Health and Care (AIMHC), 2024 - ieeexplore.ieee.org",https://ieeexplore.ieee.org/abstract/document/10504359/,Cited by 1
,Llm-esr: Large language models enhancement for long-tailed sequential recommendation,,,,,,"Q Liu,X Wu,Y Wang,Z Zhang,F Tian… - Advances in …, 2024 - proceedings.neurips.cc",https://proceedings.neurips.cc/paper_files/paper/2024/hash/2f0728449cb3150189d765fc87afc913-Abstract-Conference.html,
,"Labor, democracy, and the postcolonial state: Spaces of union organizing and the duppy state in Britain and Trinidad",,,,,,"B Gowland, D Featherstone… - Annals of the American …, 2024 - Taylor & Francis",https://www.tandfonline.com/doi/abs/10.1080/24694452.2023.2240875,
,Ordinal Outcome State-Space Models for Intensive Longitudinal Data,,,,,,"TR Henry, LR Slipetz, A Falk, J Qiu, M Chen - psychometrika, 2024 - cambridge.org",https://www.cambridge.org/core/journals/psychometrika/article/ordinal-outcome-statespace-models-for-intensive-longitudinal-data/2F37BAE5FB7BF2ECCE89D5A212A87DD5,